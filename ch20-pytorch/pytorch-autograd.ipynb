{"cells":[{"cell_type":"code","execution_count":null,"id":"be1f5a9a-b3ee-424b-ab02-4371f49bd786","metadata":{"id":"be1f5a9a-b3ee-424b-ab02-4371f49bd786"},"outputs":[],"source":["# !conda install numpy pandas matplotlib --yes"]},{"cell_type":"code","execution_count":null,"id":"1ea7b3b8-9092-4b37-8b7f-57362be611ad","metadata":{"id":"1ea7b3b8-9092-4b37-8b7f-57362be611ad"},"outputs":[],"source":["# !pip install torch torchvision torchaudio"]},{"cell_type":"code","source":["import torch\n","import numpy as np\n","import random"],"metadata":{"id":"MDC2HV54TsvD","executionInfo":{"status":"ok","timestamp":1684957789846,"user_tz":-120,"elapsed":5271,"user":{"displayName":"Artem Vachev","userId":"08691019652572164792"}}},"id":"MDC2HV54TsvD","execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["**Loss function**. The logistic regression does not use the predicted class labels during the learning (training) phase. Instead, it uses the predicted probabilities to optimize a   \n","1) surrogate loss - general term that refers to a “proxy” loss that is optimized instead of the target evaluation metric (like classification accuracy). We use surrogate loss if we cannot optimize the target metric directly  \n","2) negative log-likelihood loss - a different term for Binary cross-entropy.  \n","3) binary cross-entropy loss - a different term for negative log-likelihood loss.  "],"metadata":{"id":"U6dCua9jT4Fk"},"id":"U6dCua9jT4Fk"},{"cell_type":"markdown","source":["The logistic regression loss function increases exponentially the farther the predicted probability is from the true target label. There is a steep loss increase for wrong predictions; the loss approaches infinity for wrong predictions."],"metadata":{"id":"I0RTxu0ST6bq"},"id":"I0RTxu0ST6bq"},{"cell_type":"markdown","source":["**Stochastic gradient descent** is based on calculus: we compute the loss function's derivatives (or gradients) with respect to the model weights. Why? The loss measures “how wrong” the predictions are. And the gradient tells us how we have to change the weights to minimize (improve) the loss.  \n","\n","The loss is correlated to the accuracy, but sadly, we cannot optimize the accuracy directly using stochastic gradient descent. That's because accuracy is not a smooth function.\n","\n","Computing the loss gradients is based on the chain rule from calculus. Introduce PyTorch functions that can handle the differentiation (that is, the calculation of the gradients) automatically for us. This is known as automatic differentiation or autograd."],"metadata":{"id":"ICxGNeg6T6XI"},"id":"ICxGNeg6T6XI"},{"cell_type":"markdown","source":["**Derivatives vs Gradients**\n","\n","Partial derivative - function(multiple variables) and we need to compute individual partial derivatives with respect to one variable.\n","\n","Gradient of the function - the way of writing down partial derivatives in a vector form. \n","\n","f(x,y) = x^2 + y  \n","Gradient - slope in the 2 dimensions  \n","\n","\n","We can think of a “gradient” as a fancy term to describe the concept of a derivative in multiple dimensions. If we have a function with multiple inputs, we can compute a gradient to capture the slope in multiple dimensions. E.g., if the function takes 2 inputs, we have a 2D slope."],"metadata":{"id":"gNp6bAaOT6Ra"},"id":"gNp6bAaOT6Ra"},{"cell_type":"markdown","source":["Back propagation - learning algorithm for Deep Neural Networks.  \n","\n","Gradient descent - 1 model update per training epoch  \n","Stochastic gradient descent - N model updates for each training epoch, n - number samples    \n","Minibatch gradient descent - form small groupd of training examples, make 1 update after each batch.  \n","\n","\n","**Stochastic gradient descent** is a flavor of gradient descent that introduces a certain level of randomness into the training process. For each weight update, we compute the loss based on a single training example or a minibatch, which introduces a certain level of noise (or randomness) compared to regular gradient descent, which computes the weight update based on the whole training set. In this sense, the gradient for the weight update in stochastic gradient descent is an approximation of the full-gradient from regular gradient descent."],"metadata":{"id":"-2mPvFGAT6GJ"},"id":"-2mPvFGAT6GJ"},{"cell_type":"markdown","source":["## Autograd"],"metadata":{"id":"arsLdc44U5zT"},"id":"arsLdc44U5zT"},{"cell_type":"code","source":["# Model params\n","w_1 = torch.tensor([0.23], requires_grad=True)\n","b = torch.tensor([0.1], requires_grad=True)\n","\n","# Inputs and targets\n","x_1 = torch.tensor([1.23])\n","y = torch.tensor([1.])"],"metadata":{"id":"1WHerduwT2tR","executionInfo":{"status":"ok","timestamp":1684958052012,"user_tz":-120,"elapsed":1884,"user":{"displayName":"Artem Vachev","userId":"08691019652572164792"}}},"id":"1WHerduwT2tR","execution_count":2,"outputs":[]},{"cell_type":"code","source":["u = x_1.dot(w_1)\n","z = u + b\n","print(z)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4JoX_vglU4Ic","executionInfo":{"status":"ok","timestamp":1684958070409,"user_tz":-120,"elapsed":8,"user":{"displayName":"Artem Vachev","userId":"08691019652572164792"}},"outputId":"38e27933-519c-46b7-e167-0ad285c10b6f"},"id":"4JoX_vglU4Ic","execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([0.3829], grad_fn=<AddBackward0>)\n"]}]},{"cell_type":"code","source":["a = torch.sigmoid(z)\n","print(a)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y30KW14hU4Ea","executionInfo":{"status":"ok","timestamp":1684958077786,"user_tz":-120,"elapsed":8,"user":{"displayName":"Artem Vachev","userId":"08691019652572164792"}},"outputId":"5734dcd1-9ea0-4323-cfd6-ea1deddba0e3"},"id":"y30KW14hU4Ea","execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([0.5946], grad_fn=<SigmoidBackward0>)\n"]}]},{"cell_type":"code","source":["import torch.nn.functional as F\n","\n","l = F.binary_cross_entropy(a, y)\n","print(l)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SNDu8eSQU3_n","executionInfo":{"status":"ok","timestamp":1684958093492,"user_tz":-120,"elapsed":582,"user":{"displayName":"Artem Vachev","userId":"08691019652572164792"}},"outputId":"468d967b-7613-44f8-e979-f056bab7dd89"},"id":"SNDu8eSQU3_n","execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(0.5199, grad_fn=<BinaryCrossEntropyBackward0>)\n"]}]},{"cell_type":"code","source":["# Best practice - use logits - for computation efficiency and numerical stability\n","l = F.binary_cross_entropy_with_logits(z, y)\n","print(l)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hacq0nSZU32m","executionInfo":{"status":"ok","timestamp":1684958110812,"user_tz":-120,"elapsed":6,"user":{"displayName":"Artem Vachev","userId":"08691019652572164792"}},"outputId":"4f9bed9a-e881-4a99-b543-31ff1779ea48"},"id":"hacq0nSZU32m","execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(0.5199, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"]}]},{"cell_type":"code","source":["# Compute gradients\n","from torch.autograd import grad\n","\n","grad_L_w1 = grad(l, w_1, retain_graph=True) # retain_graph - keep the computation graph in memory\n","print(grad_L_w1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gBCzQz-QU3z_","executionInfo":{"status":"ok","timestamp":1684958121434,"user_tz":-120,"elapsed":6,"user":{"displayName":"Artem Vachev","userId":"08691019652572164792"}},"outputId":"9f563e91-3625-4a4b-8c83-3b59380f6761"},"id":"gBCzQz-QU3z_","execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["(tensor([-0.4987]),)\n"]}]},{"cell_type":"code","source":["grad_L_b = grad(l, b, retain_graph=True)\n","print(grad_L_b)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3iv3EC0BU3wQ","executionInfo":{"status":"ok","timestamp":1684958152782,"user_tz":-120,"elapsed":296,"user":{"displayName":"Artem Vachev","userId":"08691019652572164792"}},"outputId":"b89d02fc-a7e3-4370-cc64-b599c36609bb"},"id":"3iv3EC0BU3wQ","execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["(tensor([-0.4054]),)\n"]}]},{"cell_type":"code","source":["# Compute partial derivatives (gradients) of the loss automatically\n","l.backward()"],"metadata":{"id":"-k3HsnQ_U3qd","executionInfo":{"status":"ok","timestamp":1684958166004,"user_tz":-120,"elapsed":314,"user":{"displayName":"Artem Vachev","userId":"08691019652572164792"}}},"id":"-k3HsnQ_U3qd","execution_count":10,"outputs":[]},{"cell_type":"code","source":["w_1.grad"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yjkIh884U3kp","executionInfo":{"status":"ok","timestamp":1684958172679,"user_tz":-120,"elapsed":5,"user":{"displayName":"Artem Vachev","userId":"08691019652572164792"}},"outputId":"8c6a43e6-0405-49e0-b7b3-8447bc003eff"},"id":"yjkIh884U3kp","execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([-0.4987])"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["b.grad"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M4iqHGEtU3fA","executionInfo":{"status":"ok","timestamp":1684958181334,"user_tz":-120,"elapsed":6,"user":{"displayName":"Artem Vachev","userId":"08691019652572164792"}},"outputId":"60ad2b31-d517-47f7-a143-09f64fc5ff3c"},"id":"M4iqHGEtU3fA","execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([-0.4054])"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["x = torch.tensor(3.)\n","y = torch.tensor(2., requires_grad=True)\n","z = torch.tensor(5.)\n","f = 4 * x**3 + 3 * y**2 + 2*z\n","print(grad(f, y))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zp74348oTslG","executionInfo":{"status":"ok","timestamp":1684958191311,"user_tz":-120,"elapsed":311,"user":{"displayName":"Artem Vachev","userId":"08691019652572164792"}},"outputId":"36f5135c-4c7e-45d0-c2a7-495b1110a9bc"},"id":"zp74348oTslG","execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["(tensor(12.),)\n"]}]},{"cell_type":"markdown","source":["\n","\n","---\n","\n","\n","\n","---\n","\n"],"metadata":{"id":"mi9CfWUAVfr8"},"id":"mi9CfWUAVfr8"},{"cell_type":"markdown","source":["## Model training"],"metadata":{"id":"gi_BtUejVg3r"},"id":"gi_BtUejVg3r"},{"cell_type":"markdown","source":["In the PyTorch Module context, it's a unique method of the Module API that will implement a backward method automatically for us (we don't see it because it happens behind the scenes.)\n","\n","Why is this useful? Using the Module class comes with certain benefits. If we use it, we can use the `loss.backward()` call in our training loop together with `optimizer.step()`. The `.backward()` method computes all the gradients for us. Then, using the `.step()` method will use the loss gradients to update the model weights automatically for us.  \n","\n","\n","\n","Python classes typically require an `__init__` if we want to define class attributes upon creating new objects from that class. In this context, we define the model parameters here.\n","\n","\n","In the `forward` method we define how the model computes the outputs. In the case of logistic regression, this could be the computation of the class-membership probabilities."],"metadata":{"id":"2QNwKtppVma2"},"id":"2QNwKtppVma2"},{"cell_type":"code","source":["\"\"\" Define the model \"\"\"\n","\n","class MyClassifier(torch.nn.Module):\n","    def __init__(self, num_features):\n","        # Define model parameters\n","        pass\n","\n","    def forward(self, x):\n","        # Define how the model produces outputs\n","        return outputs"],"metadata":{"id":"OTDJpQ-bVj5n"},"id":"OTDJpQ-bVj5n","execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","model = MyClassifier()\n","optimizer = torch.optim.SGD()   # init optimizer\n","\n","for epoch in range(num_epochs):\n","    for x, y in train_dataloader: # mini-batch\n","\n","        # forward pass\n","        outputs = model(x)\n","        loss = loss_fn(outputs, y)\n","\n","        # backward pass\n","        optimizer.zero_grad() # reset the gradients from previous iteration (do not accumulate)\n","        loss.backward()       # compute gradients\n","\n","        # update model params\n","        optimizer.step()   \n","'''"],"metadata":{"id":"Jsw72Bl6Vj1D"},"id":"Jsw72Bl6Vj1D","execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.manual_seed(123)\n","\n","linear = torch.nn.Linear(in_features=2, out_features=1)"],"metadata":{"id":"KJFf1PRQVjpt","executionInfo":{"status":"ok","timestamp":1684958275426,"user_tz":-120,"elapsed":3,"user":{"displayName":"Artem Vachev","userId":"08691019652572164792"}}},"id":"KJFf1PRQVjpt","execution_count":14,"outputs":[]},{"cell_type":"code","source":["print(linear.weight)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nIgG6JulVjlv","executionInfo":{"status":"ok","timestamp":1684958281895,"user_tz":-120,"elapsed":303,"user":{"displayName":"Artem Vachev","userId":"08691019652572164792"}},"outputId":"48bde4b6-9384-464f-845e-9776ff8b1aa3"},"id":"nIgG6JulVjlv","execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Parameter containing:\n","tensor([[-0.2883,  0.0234]], requires_grad=True)\n"]}]},{"cell_type":"code","source":["print(linear.bias)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EMTywAc1Vjhg","executionInfo":{"status":"ok","timestamp":1684958288399,"user_tz":-120,"elapsed":7,"user":{"displayName":"Artem Vachev","userId":"08691019652572164792"}},"outputId":"1459181b-5e1d-4625-e208-a2551544275e"},"id":"EMTywAc1Vjhg","execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Parameter containing:\n","tensor([-0.3512], requires_grad=True)\n"]}]},{"cell_type":"code","source":["x = torch.tensor([[1.2, 0.5]])\n","x"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xm0jlveOVjcr","executionInfo":{"status":"ok","timestamp":1684958301292,"user_tz":-120,"elapsed":6,"user":{"displayName":"Artem Vachev","userId":"08691019652572164792"}},"outputId":"c5ec3ded-353a-4b37-a916-a0aaa457db5f"},"id":"xm0jlveOVjcr","execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1.2000, 0.5000]])"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["w = linear.weight.detach()   # detach from the computation graph\n","b = linear.bias.detach()\n","z = x.matmul(w.T) + b\n","\n","print(z)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H0MsjVNTVjWz","executionInfo":{"status":"ok","timestamp":1684958307973,"user_tz":-120,"elapsed":6,"user":{"displayName":"Artem Vachev","userId":"08691019652572164792"}},"outputId":"5775d378-9b75-4630-bcf0-9762a479cf61"},"id":"H0MsjVNTVjWz","execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[-0.6855]])\n"]}]},{"cell_type":"code","source":["z = linear(x)\n","print(z)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gNjOpV_KVaij","executionInfo":{"status":"ok","timestamp":1684958314279,"user_tz":-120,"elapsed":5,"user":{"displayName":"Artem Vachev","userId":"08691019652572164792"}},"outputId":"cf13bada-dd93-4e04-f5ec-a0b70ce5d7e1"},"id":"gNjOpV_KVaij","execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[-0.6855]], grad_fn=<AddmmBackward0>)\n"]}]},{"cell_type":"code","source":["# How many trainable parameters does this layer have?\n","list(torch.nn.Linear(in_features=5, out_features=1).parameters())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9jHLOizHVacy","executionInfo":{"status":"ok","timestamp":1684958329892,"user_tz":-120,"elapsed":5,"user":{"displayName":"Artem Vachev","userId":"08691019652572164792"}},"outputId":"5c59ff13-c25d-417a-92cc-d8089be72d39"},"id":"9jHLOizHVacy","execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Parameter containing:\n"," tensor([[ 0.1687, -0.3811,  0.3278, -0.3251, -0.3556]], requires_grad=True),\n"," Parameter containing:\n"," tensor([-0.2826], requires_grad=True)]"]},"metadata":{},"execution_count":20}]},{"cell_type":"markdown","source":["## Logistic Regression"],"metadata":{"id":"FOL2BzozWAvM"},"id":"FOL2BzozWAvM"},{"cell_type":"code","source":["class LogisticRegression(torch.nn.Module):\n","    \n","    def __init__(self, num_features):\n","        super().__init__()     # call the __init__ of the (torch.nn.Module)\n","        self.linear = torch.nn.Linear(num_features, 1)\n","    \n","    def forward(self, x):\n","        logits = self.linear(x)\n","        probas = torch.sigmoid(logits)\n","        return probas"],"metadata":{"id":"ufmUdCNjV_Og","executionInfo":{"status":"ok","timestamp":1684958364993,"user_tz":-120,"elapsed":6,"user":{"displayName":"Artem Vachev","userId":"08691019652572164792"}}},"id":"ufmUdCNjV_Og","execution_count":21,"outputs":[]},{"cell_type":"code","source":["torch.manual_seed(1)\n","\n","model = LogisticRegression(num_features=2)\n","\n","x = torch.tensor([1.1, 2.1])\n","\n","# Best practice to run faster\n","with torch.no_grad():    # within context - disable the construction of computation graph in the background\n","    proba = model(x)\n","    \n","print(proba)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0KQpeagpV_Ks","executionInfo":{"status":"ok","timestamp":1684958375710,"user_tz":-120,"elapsed":313,"user":{"displayName":"Artem Vachev","userId":"08691019652572164792"}},"outputId":"762ddb73-be68-450e-92b5-b8812e14c779"},"id":"0KQpeagpV_Ks","execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([0.4033])\n"]}]},{"cell_type":"markdown","source":["## Define the Dataset and Dataloaders"],"metadata":{"id":"brqVoRrSWIXo"},"id":"brqVoRrSWIXo"},{"cell_type":"code","source":["from torch.utils.data import Dataset, DataLoader"],"metadata":{"id":"1ZEz0wjoV_GL","executionInfo":{"status":"ok","timestamp":1684958395996,"user_tz":-120,"elapsed":3,"user":{"displayName":"Artem Vachev","userId":"08691019652572164792"}}},"id":"1ZEz0wjoV_GL","execution_count":23,"outputs":[]},{"cell_type":"code","source":["class MyDataset(Dataset):\n","    def __init__(self, X, y):\n","        self.features = torch.tensor(X, dtype=torch.float32)\n","        self.labels = torch.tensor(y, dtype=torch.float32)\n","\n","    def __getitem__(self, index):  # Fetch individual training record\n","        x = self.features[index]\n","        y = self.labels[index]        \n","        return x, y\n","\n","    def __len__(self):\n","        return self.labels.shape[0]"],"metadata":{"id":"9OXaEshvV-7s","executionInfo":{"status":"ok","timestamp":1684958402554,"user_tz":-120,"elapsed":4,"user":{"displayName":"Artem Vachev","userId":"08691019652572164792"}}},"id":"9OXaEshvV-7s","execution_count":24,"outputs":[]},{"cell_type":"code","source":["# Init Dataset\n","train_ds = MyDataset(X_train, y_train)\n","\n","# Wrap dataset into DataLoader\n","train_loader = DataLoader(\n","    dataset=train_ds,\n","    batch_size=10,\n","    shuffle=True,\n",")"],"metadata":{"id":"FoXeVcj9V-zk"},"id":"FoXeVcj9V-zk","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Train Logistic Regression"],"metadata":{"id":"ldQJZUxFWVb4"},"id":"ldQJZUxFWVb4"},{"cell_type":"code","source":["# Training Model\n","\n","torch.manual_seed(1)\n","\n","model = LogisticRegression(num_features=2)\n","optimizer = torch.optim.SGD(model.parameters(),    # return all the relevant model parameters to update \n","                            lr=0.05\n","                            )\n","\n","num_epochs = 20\n","\n","for epoch in range(num_epochs):\n","    \n","    model = model.train()    # set the model into the training mode\n","    \n","    for batch_idx, (features, class_labels) in enumerate(train_loader):\n","\n","        probas = model(features)\n","        \n","        loss = F.binary_cross_entropy(probas, class_labels.view(probas.shape))\n","        \n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        \n","        ### LOGGING\n","        print(f'Epoch: {epoch+1:03d}/{num_epochs:03d}'\n","               f' | Batch {batch_idx:03d}/{len(train_loader):03d}'\n","               f' | Loss: {loss:.2f}')"],"metadata":{"id":"LaVYSX7XV-ub"},"id":"LaVYSX7XV-ub","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["When we work with real-world datasets, features often come in different scales. For example, think of the alcohol content of wine (when measured in percent, it's typically a value between 10 and 15). On the other hand, the proline content of wine is measured in mg/L (milligram per Liter) and can be 100 times larger, typically ranging between 300 and 1300.   \n","\n","Working with features that have vastly different numeric scales can often result in suboptimal training. To make it easier to find a good learning rate and get good convergence (that means, successfully minimizing the loss), **feature normalization** can help.  \n","\n","This lecture covered two of the most widely used feature normalization methods: min-max scaling and standardization (also known as z-score standardization). In certain scenarios, one normalization scheme might work slightly better than another. Still, the most important lesson is that we use a normalization scheme to ensure that the features are all roughly on the same scale.   \n","\n","\n","----\n","If features on different scales --> partial derivateives on different scales --> hard to find a good learning rate working well for different weights.\n","\n","**Advantages**: \n","- easier to find a good learning rate for all gradients\n","- numerically stable gradients\n","- faster convergence (fewer epochs)"],"metadata":{"id":"QcnSikAiWg6v"},"id":"QcnSikAiWg6v"},{"cell_type":"markdown","id":"d71bce70-9dc3-448b-9f9a-8896e83b6d09","metadata":{"id":"d71bce70-9dc3-448b-9f9a-8896e83b6d09"},"source":["# Implementing a Logistic Regression Classifies"]},{"cell_type":"markdown","source":["we are applying logistic regression to a banknote authentication dataset to distinguish between genuine and forged bank notes."],"metadata":{"id":"omeOyfeesJ6a"},"id":"omeOyfeesJ6a"},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn.functional as F"],"metadata":{"id":"8pduXY_HWyF7","executionInfo":{"status":"ok","timestamp":1684958558745,"user_tz":-120,"elapsed":1146,"user":{"displayName":"Artem Vachev","userId":"08691019652572164792"}}},"id":"8pduXY_HWyF7","execution_count":27,"outputs":[]},{"cell_type":"markdown","id":"b9549676-2fa5-41a7-bbb9-ce03f5797c34","metadata":{"id":"b9549676-2fa5-41a7-bbb9-ce03f5797c34"},"source":["## Loading the Dataset"]},{"cell_type":"markdown","source":["The dataset consists of 1372 examples and 4 features for binary classification. The features are:\n","\n","- variance of a wavelet-transformed image (continuous)\n","- skewness of a wavelet-transformed image (continuous)\n","- kurtosis of a wavelet-transformed image (continuous)\n","- entropy of the image (continuous)"],"metadata":{"id":"HI91q00rsNT5"},"id":"HI91q00rsNT5"},{"cell_type":"code","execution_count":32,"id":"f609024c-3eae-4ad5-8cb8-b95b403b7606","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"f609024c-3eae-4ad5-8cb8-b95b403b7606","executionInfo":{"status":"ok","timestamp":1684958655892,"user_tz":-120,"elapsed":10,"user":{"displayName":"Artem Vachev","userId":"08691019652572164792"}},"outputId":"b3753c74-2d99-4778-f6fa-e0e21946cbe0"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["         0       1       2        3  4\n","0  3.62160  8.6661 -2.8073 -0.44699  0\n","1  4.54590  8.1674 -2.4586 -1.46210  0\n","2  3.86600 -2.6383  1.9242  0.10645  0\n","3  3.45660  9.5228 -4.0112 -3.59440  0\n","4  0.32924 -4.4552  4.5718 -0.98880  0"],"text/html":["\n","  <div id=\"df-c3acb1e7-4713-4c2d-b482-9bce827ddf08\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>3.62160</td>\n","      <td>8.6661</td>\n","      <td>-2.8073</td>\n","      <td>-0.44699</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>4.54590</td>\n","      <td>8.1674</td>\n","      <td>-2.4586</td>\n","      <td>-1.46210</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3.86600</td>\n","      <td>-2.6383</td>\n","      <td>1.9242</td>\n","      <td>0.10645</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3.45660</td>\n","      <td>9.5228</td>\n","      <td>-4.0112</td>\n","      <td>-3.59440</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.32924</td>\n","      <td>-4.4552</td>\n","      <td>4.5718</td>\n","      <td>-0.98880</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c3acb1e7-4713-4c2d-b482-9bce827ddf08')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-c3acb1e7-4713-4c2d-b482-9bce827ddf08 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-c3acb1e7-4713-4c2d-b482-9bce827ddf08');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":32}],"source":["df = pd.read_csv(\"/content/drive/MyDrive/ds-ml/data_banknote_authentication.txt\", header=None)\n","df.head()"]},{"cell_type":"code","execution_count":33,"id":"319546d0-e9ed-4542-873e-395edc05ef2f","metadata":{"id":"319546d0-e9ed-4542-873e-395edc05ef2f","executionInfo":{"status":"ok","timestamp":1684958667471,"user_tz":-120,"elapsed":313,"user":{"displayName":"Artem Vachev","userId":"08691019652572164792"}}},"outputs":[],"source":["X_features = df[[0, 1, 2, 3]].values\n","y_labels = df[4].values"]},{"cell_type":"code","execution_count":34,"id":"71792068-9926-41bb-81c0-2a46f6e956fc","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"71792068-9926-41bb-81c0-2a46f6e956fc","executionInfo":{"status":"ok","timestamp":1684958678372,"user_tz":-120,"elapsed":5,"user":{"displayName":"Artem Vachev","userId":"08691019652572164792"}},"outputId":"356d0dba-3ccb-4d70-8c8a-17799e041797"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1372, 4)"]},"metadata":{},"execution_count":34}],"source":["X_features.shape"]},{"cell_type":"code","execution_count":35,"id":"3a5e5ffb-1bca-4f1b-b4cf-a78be1b07753","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3a5e5ffb-1bca-4f1b-b4cf-a78be1b07753","executionInfo":{"status":"ok","timestamp":1684958691545,"user_tz":-120,"elapsed":5,"user":{"displayName":"Artem Vachev","userId":"08691019652572164792"}},"outputId":"8f639447-d7b4-4ad0-995a-976bbb2255c0"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1372,)"]},"metadata":{},"execution_count":35}],"source":["y_labels.shape"]},{"cell_type":"code","execution_count":37,"id":"b6800df4-98f6-401e-bb6c-9964c3b6e3cb","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b6800df4-98f6-401e-bb6c-9964c3b6e3cb","executionInfo":{"status":"ok","timestamp":1684958714462,"user_tz":-120,"elapsed":4,"user":{"displayName":"Artem Vachev","userId":"08691019652572164792"}},"outputId":"61f0dbf3-9bb4-47df-d202-77a376dc6a4b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([762, 610])"]},"metadata":{},"execution_count":37}],"source":["# look at the label distribution\n","np.bincount(y_labels)"]},{"cell_type":"code","execution_count":null,"id":"56e353d0-f75d-4267-8b41-68433780d590","metadata":{"id":"56e353d0-f75d-4267-8b41-68433780d590"},"outputs":[],"source":["# X_train = (X_train - X_train.mean(axis=0)) / X_train.std(axis=0)"]},{"cell_type":"code","source":["# standardization"],"metadata":{"id":"6DQ6Suf_o2fH"},"id":"6DQ6Suf_o2fH","execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_features"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LjCG-RHxQAkA","executionInfo":{"status":"ok","timestamp":1684963338644,"user_tz":-120,"elapsed":481,"user":{"displayName":"Artem Vachev","userId":"08691019652572164792"}},"outputId":"17a00661-a27c-43d3-89c5-dd34ed142aaf"},"execution_count":41,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[  3.6216 ,   8.6661 ,  -2.8073 ,  -0.44699],\n","       [  4.5459 ,   8.1674 ,  -2.4586 ,  -1.4621 ],\n","       [  3.866  ,  -2.6383 ,   1.9242 ,   0.10645],\n","       ...,\n","       [ -3.7503 , -13.4586 ,  17.5932 ,  -2.7771 ],\n","       [ -3.5637 ,  -8.3827 ,  12.393  ,  -1.2823 ],\n","       [ -2.5419 ,  -0.65804,   2.6842 ,   1.1952 ]])"]},"metadata":{},"execution_count":41}],"id":"LjCG-RHxQAkA"},{"cell_type":"code","source":["X_features.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t1qJ9nEMRUEq","executionInfo":{"status":"ok","timestamp":1684963340339,"user_tz":-120,"elapsed":8,"user":{"displayName":"Artem Vachev","userId":"08691019652572164792"}},"outputId":"2d04736e-6728-41b1-8ac9-d77c6d709615"},"execution_count":42,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1372, 4)"]},"metadata":{},"execution_count":42}],"id":"t1qJ9nEMRUEq"},{"cell_type":"code","source":["def standardize(X_data, X_mean, X_std):\n","    return (X_data - X_mean) / X_std"],"metadata":{"id":"lsrEuABRQnUG","executionInfo":{"status":"ok","timestamp":1684963342846,"user_tz":-120,"elapsed":310,"user":{"displayName":"Artem Vachev","userId":"08691019652572164792"}}},"execution_count":43,"outputs":[],"id":"lsrEuABRQnUG"},{"cell_type":"code","source":["X_mean, X_str = np.mean(X_features, axis=0), np.std(X_features, axis=0)\n","\n","# mean, std per feature column will be used to standardize any test/live dataset"],"metadata":{"id":"LlcxfE0jQxc0","executionInfo":{"status":"ok","timestamp":1684963348861,"user_tz":-120,"elapsed":868,"user":{"displayName":"Artem Vachev","userId":"08691019652572164792"}}},"execution_count":44,"outputs":[],"id":"LlcxfE0jQxc0"},{"cell_type":"code","source":["X_features_std = standardize(X_features, X_mean, X_str)"],"metadata":{"id":"t5xudUMWR7uz","executionInfo":{"status":"ok","timestamp":1684963353807,"user_tz":-120,"elapsed":295,"user":{"displayName":"Artem Vachev","userId":"08691019652572164792"}}},"execution_count":45,"outputs":[],"id":"t5xudUMWR7uz"},{"cell_type":"code","source":["X_features_std"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZYijdP9qR7o8","executionInfo":{"status":"ok","timestamp":1684963356085,"user_tz":-120,"elapsed":338,"user":{"displayName":"Artem Vachev","userId":"08691019652572164792"}},"outputId":"16deb582-d091-4c33-f88e-19c0289a5cf7"},"execution_count":46,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 1.12180565,  1.14945512, -0.97597007,  0.35456135],\n","       [ 1.44706568,  1.06445293, -0.89503626, -0.12876744],\n","       [ 1.20780971, -0.77735215,  0.12221838,  0.61807317],\n","       ...,\n","       [-1.47235682, -2.62164576,  3.75901744, -0.75488418],\n","       [-1.40669251, -1.75647104,  2.552043  , -0.04315848],\n","       [-1.04712236, -0.43982168,  0.29861555,  1.1364645 ]])"]},"metadata":{},"execution_count":46}],"id":"ZYijdP9qR7o8"},{"cell_type":"code","source":["train_size = int(X_features.shape[0]*0.80)\n","train_size"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ntKPXvw4NdCM","executionInfo":{"status":"ok","timestamp":1684963474312,"user_tz":-120,"elapsed":691,"user":{"displayName":"Artem Vachev","userId":"08691019652572164792"}},"outputId":"53816bf9-b3d5-4583-f776-af7f8feea696"},"execution_count":47,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1097"]},"metadata":{},"execution_count":47}],"id":"ntKPXvw4NdCM"},{"cell_type":"code","source":["val_size = X_features.shape[0] - train_size\n","val_size"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jY_-UQdZNeBy","executionInfo":{"status":"ok","timestamp":1684963474745,"user_tz":-120,"elapsed":3,"user":{"displayName":"Artem Vachev","userId":"08691019652572164792"}},"outputId":"46c48382-23b5-46a6-c23b-204ca69afc3e"},"execution_count":48,"outputs":[{"output_type":"execute_result","data":{"text/plain":["275"]},"metadata":{},"execution_count":48}],"id":"jY_-UQdZNeBy"},{"cell_type":"markdown","source":["## Define Dataloaders"],"metadata":{"id":"xkg8NseepKxd"},"id":"xkg8NseepKxd"},{"cell_type":"code","source":["class BanknoteAuthenDataset(Dataset):\n","    def __init__(self, X, y):\n","        self.features = torch.tensor(X, dtype=torch.float32)\n","        self.labels = torch.tensor(y, dtype=torch.float32)\n","\n","    def __getitem__(self, index):\n","        x = self.features[index]\n","        y = self.labels[index]        \n","        return x, y\n","\n","    def __len__(self):\n","        return self.labels.shape[0]"],"metadata":{"id":"_WN9p7MMo_Dd","executionInfo":{"status":"ok","timestamp":1684963511862,"user_tz":-120,"elapsed":301,"user":{"displayName":"Artem Vachev","userId":"08691019652572164792"}}},"id":"_WN9p7MMo_Dd","execution_count":50,"outputs":[]},{"cell_type":"code","source":["# Using torch.utils.data.random_split, we generate the training and validation sets along with the respective data loaders\n","\n","full_dataset = BanknoteAuthenDataset(X_features_std, y_labels)\n","\n","train_set, val_set = torch.utils.data.random_split(full_dataset, [train_size, val_size])"],"metadata":{"id":"Z5li8eiSo-8Q","executionInfo":{"status":"ok","timestamp":1684963512775,"user_tz":-120,"elapsed":3,"user":{"displayName":"Artem Vachev","userId":"08691019652572164792"}}},"id":"Z5li8eiSo-8Q","execution_count":51,"outputs":[]},{"cell_type":"code","source":["train_loader = DataLoader(\n","    dataset=train_set,\n","    batch_size=10,\n","    shuffle=True,\n",")\n","\n","val_loader = DataLoader(\n","    dataset=val_set,\n","    batch_size=10,\n","    shuffle=False,\n",")"],"metadata":{"id":"EeS1rubwo-1g","executionInfo":{"status":"ok","timestamp":1684963520134,"user_tz":-120,"elapsed":348,"user":{"displayName":"Artem Vachev","userId":"08691019652572164792"}}},"id":"EeS1rubwo-1g","execution_count":52,"outputs":[]},{"cell_type":"markdown","id":"db50db02-3696-4f86-b149-74baabeec6c4","metadata":{"id":"db50db02-3696-4f86-b149-74baabeec6c4"},"source":["## Implementing the model"]},{"cell_type":"code","execution_count":53,"id":"3da86d9a-7cd5-467c-bf65-3388fe272bd5","metadata":{"id":"3da86d9a-7cd5-467c-bf65-3388fe272bd5","executionInfo":{"status":"ok","timestamp":1684963559814,"user_tz":-120,"elapsed":307,"user":{"displayName":"Artem Vachev","userId":"08691019652572164792"}}},"outputs":[],"source":["class LogisticRegression(torch.nn.Module):    \n","    def __init__(self, num_features):\n","        super().__init__()\n","        self.linear = torch.nn.Linear(num_features, 1)\n","    \n","    def forward(self, x):\n","        logits = self.linear(x)\n","        probas = torch.sigmoid(logits)\n","        return probas"]},{"cell_type":"code","source":["model = LogisticRegression(num_features=4)"],"metadata":{"id":"ebqM4I_op9m-","executionInfo":{"status":"ok","timestamp":1684963595157,"user_tz":-120,"elapsed":385,"user":{"displayName":"Artem Vachev","userId":"08691019652572164792"}}},"id":"ebqM4I_op9m-","execution_count":54,"outputs":[]},{"cell_type":"code","execution_count":55,"id":"f2923ae0-cf24-4252-bd19-cc326a39bc72","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f2923ae0-cf24-4252-bd19-cc326a39bc72","executionInfo":{"status":"ok","timestamp":1684963627463,"user_tz":-120,"elapsed":9,"user":{"displayName":"Artem Vachev","userId":"08691019652572164792"}},"outputId":"34dcd904-720b-4eed-cd09-19509661eea9"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([0.9005])\n"]}],"source":["x = torch.tensor([1.1, 2.1, 1.4, 2.3])\n","\n","with torch.no_grad():\n","    proba = model(x)\n","    \n","print(proba)"]},{"cell_type":"markdown","id":"46bc16a0-ec59-4c54-a209-0a5e22406287","metadata":{"id":"46bc16a0-ec59-4c54-a209-0a5e22406287"},"source":["## The training loop"]},{"cell_type":"code","source":["optimizer = torch.optim.SGD(model.parameters(), \n","                            lr=0.1)"],"metadata":{"id":"amAW7HdFqTFf","executionInfo":{"status":"ok","timestamp":1684963672602,"user_tz":-120,"elapsed":321,"user":{"displayName":"Artem Vachev","userId":"08691019652572164792"}}},"id":"amAW7HdFqTFf","execution_count":56,"outputs":[]},{"cell_type":"code","source":["num_epochs = 25"],"metadata":{"id":"q48uB28YqT_o","executionInfo":{"status":"ok","timestamp":1684963726267,"user_tz":-120,"elapsed":412,"user":{"displayName":"Artem Vachev","userId":"08691019652572164792"}}},"id":"q48uB28YqT_o","execution_count":58,"outputs":[]},{"cell_type":"code","execution_count":59,"id":"3dcaa2b1-4019-4128-9ff5-6a966c3abdf2","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3dcaa2b1-4019-4128-9ff5-6a966c3abdf2","executionInfo":{"status":"ok","timestamp":1684963730205,"user_tz":-120,"elapsed":3537,"user":{"displayName":"Artem Vachev","userId":"08691019652572164792"}},"outputId":"ae3a59e1-6fea-444e-8ddb-a9cd2b7530d2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 001/025 | Batch 000/110 | Loss: 0.95\n","Epoch: 001/025 | Batch 001/110 | Loss: 1.01\n","Epoch: 001/025 | Batch 002/110 | Loss: 0.96\n","Epoch: 001/025 | Batch 003/110 | Loss: 0.88\n","Epoch: 001/025 | Batch 004/110 | Loss: 0.85\n","Epoch: 001/025 | Batch 005/110 | Loss: 0.89\n","Epoch: 001/025 | Batch 006/110 | Loss: 0.76\n","Epoch: 001/025 | Batch 007/110 | Loss: 0.84\n","Epoch: 001/025 | Batch 008/110 | Loss: 0.77\n","Epoch: 001/025 | Batch 009/110 | Loss: 0.79\n","Epoch: 001/025 | Batch 010/110 | Loss: 0.74\n","Epoch: 001/025 | Batch 011/110 | Loss: 0.73\n","Epoch: 001/025 | Batch 012/110 | Loss: 0.68\n","Epoch: 001/025 | Batch 013/110 | Loss: 0.72\n","Epoch: 001/025 | Batch 014/110 | Loss: 0.75\n","Epoch: 001/025 | Batch 015/110 | Loss: 0.65\n","Epoch: 001/025 | Batch 016/110 | Loss: 0.70\n","Epoch: 001/025 | Batch 017/110 | Loss: 0.65\n","Epoch: 001/025 | Batch 018/110 | Loss: 0.76\n","Epoch: 001/025 | Batch 019/110 | Loss: 0.65\n","Epoch: 001/025 | Batch 020/110 | Loss: 0.62\n","Epoch: 001/025 | Batch 021/110 | Loss: 0.71\n","Epoch: 001/025 | Batch 022/110 | Loss: 0.49\n","Epoch: 001/025 | Batch 023/110 | Loss: 0.54\n","Epoch: 001/025 | Batch 024/110 | Loss: 0.59\n","Epoch: 001/025 | Batch 025/110 | Loss: 0.59\n","Epoch: 001/025 | Batch 026/110 | Loss: 0.49\n","Epoch: 001/025 | Batch 027/110 | Loss: 0.46\n","Epoch: 001/025 | Batch 028/110 | Loss: 0.55\n","Epoch: 001/025 | Batch 029/110 | Loss: 0.59\n","Epoch: 001/025 | Batch 030/110 | Loss: 0.57\n","Epoch: 001/025 | Batch 031/110 | Loss: 0.52\n","Epoch: 001/025 | Batch 032/110 | Loss: 0.47\n","Epoch: 001/025 | Batch 033/110 | Loss: 0.51\n","Epoch: 001/025 | Batch 034/110 | Loss: 0.58\n","Epoch: 001/025 | Batch 035/110 | Loss: 0.50\n","Epoch: 001/025 | Batch 036/110 | Loss: 0.48\n","Epoch: 001/025 | Batch 037/110 | Loss: 0.46\n","Epoch: 001/025 | Batch 038/110 | Loss: 0.55\n","Epoch: 001/025 | Batch 039/110 | Loss: 0.38\n","Epoch: 001/025 | Batch 040/110 | Loss: 0.60\n","Epoch: 001/025 | Batch 041/110 | Loss: 0.41\n","Epoch: 001/025 | Batch 042/110 | Loss: 0.61\n","Epoch: 001/025 | Batch 043/110 | Loss: 0.51\n","Epoch: 001/025 | Batch 044/110 | Loss: 0.64\n","Epoch: 001/025 | Batch 045/110 | Loss: 0.50\n","Epoch: 001/025 | Batch 046/110 | Loss: 0.33\n","Epoch: 001/025 | Batch 047/110 | Loss: 0.48\n","Epoch: 001/025 | Batch 048/110 | Loss: 0.35\n","Epoch: 001/025 | Batch 049/110 | Loss: 0.38\n","Epoch: 001/025 | Batch 050/110 | Loss: 0.50\n","Epoch: 001/025 | Batch 051/110 | Loss: 0.33\n","Epoch: 001/025 | Batch 052/110 | Loss: 0.61\n","Epoch: 001/025 | Batch 053/110 | Loss: 0.47\n","Epoch: 001/025 | Batch 054/110 | Loss: 0.54\n","Epoch: 001/025 | Batch 055/110 | Loss: 0.42\n","Epoch: 001/025 | Batch 056/110 | Loss: 0.29\n","Epoch: 001/025 | Batch 057/110 | Loss: 0.60\n","Epoch: 001/025 | Batch 058/110 | Loss: 0.50\n","Epoch: 001/025 | Batch 059/110 | Loss: 0.33\n","Epoch: 001/025 | Batch 060/110 | Loss: 0.49\n","Epoch: 001/025 | Batch 061/110 | Loss: 0.36\n","Epoch: 001/025 | Batch 062/110 | Loss: 0.49\n","Epoch: 001/025 | Batch 063/110 | Loss: 0.49\n","Epoch: 001/025 | Batch 064/110 | Loss: 0.42\n","Epoch: 001/025 | Batch 065/110 | Loss: 0.35\n","Epoch: 001/025 | Batch 066/110 | Loss: 0.50\n","Epoch: 001/025 | Batch 067/110 | Loss: 0.29\n","Epoch: 001/025 | Batch 068/110 | Loss: 0.50\n","Epoch: 001/025 | Batch 069/110 | Loss: 0.38\n","Epoch: 001/025 | Batch 070/110 | Loss: 0.46\n","Epoch: 001/025 | Batch 071/110 | Loss: 0.36\n","Epoch: 001/025 | Batch 072/110 | Loss: 0.35\n","Epoch: 001/025 | Batch 073/110 | Loss: 0.19\n","Epoch: 001/025 | Batch 074/110 | Loss: 0.22\n","Epoch: 001/025 | Batch 075/110 | Loss: 0.44\n","Epoch: 001/025 | Batch 076/110 | Loss: 0.29\n","Epoch: 001/025 | Batch 077/110 | Loss: 0.27\n","Epoch: 001/025 | Batch 078/110 | Loss: 0.44\n","Epoch: 001/025 | Batch 079/110 | Loss: 0.28\n","Epoch: 001/025 | Batch 080/110 | Loss: 0.28\n","Epoch: 001/025 | Batch 081/110 | Loss: 0.50\n","Epoch: 001/025 | Batch 082/110 | Loss: 0.33\n","Epoch: 001/025 | Batch 083/110 | Loss: 0.48\n","Epoch: 001/025 | Batch 084/110 | Loss: 0.26\n","Epoch: 001/025 | Batch 085/110 | Loss: 0.28\n","Epoch: 001/025 | Batch 086/110 | Loss: 0.33\n","Epoch: 001/025 | Batch 087/110 | Loss: 0.34\n","Epoch: 001/025 | Batch 088/110 | Loss: 0.28\n","Epoch: 001/025 | Batch 089/110 | Loss: 0.28\n","Epoch: 001/025 | Batch 090/110 | Loss: 0.34\n","Epoch: 001/025 | Batch 091/110 | Loss: 0.24\n","Epoch: 001/025 | Batch 092/110 | Loss: 0.33\n","Epoch: 001/025 | Batch 093/110 | Loss: 0.36\n","Epoch: 001/025 | Batch 094/110 | Loss: 0.22\n","Epoch: 001/025 | Batch 095/110 | Loss: 0.38\n","Epoch: 001/025 | Batch 096/110 | Loss: 0.40\n","Epoch: 001/025 | Batch 097/110 | Loss: 0.25\n","Epoch: 001/025 | Batch 098/110 | Loss: 0.32\n","Epoch: 001/025 | Batch 099/110 | Loss: 0.37\n","Epoch: 001/025 | Batch 100/110 | Loss: 0.12\n","Epoch: 001/025 | Batch 101/110 | Loss: 0.27\n","Epoch: 001/025 | Batch 102/110 | Loss: 0.45\n","Epoch: 001/025 | Batch 103/110 | Loss: 0.39\n","Epoch: 001/025 | Batch 104/110 | Loss: 0.26\n","Epoch: 001/025 | Batch 105/110 | Loss: 0.33\n","Epoch: 001/025 | Batch 106/110 | Loss: 0.44\n","Epoch: 001/025 | Batch 107/110 | Loss: 0.28\n","Epoch: 001/025 | Batch 108/110 | Loss: 0.41\n","Epoch: 001/025 | Batch 109/110 | Loss: 0.29\n","Epoch: 002/025 | Batch 000/110 | Loss: 0.21\n","Epoch: 002/025 | Batch 001/110 | Loss: 0.23\n","Epoch: 002/025 | Batch 002/110 | Loss: 0.30\n","Epoch: 002/025 | Batch 003/110 | Loss: 0.34\n","Epoch: 002/025 | Batch 004/110 | Loss: 0.30\n","Epoch: 002/025 | Batch 005/110 | Loss: 0.27\n","Epoch: 002/025 | Batch 006/110 | Loss: 0.35\n","Epoch: 002/025 | Batch 007/110 | Loss: 0.36\n","Epoch: 002/025 | Batch 008/110 | Loss: 0.26\n","Epoch: 002/025 | Batch 009/110 | Loss: 0.41\n","Epoch: 002/025 | Batch 010/110 | Loss: 0.29\n","Epoch: 002/025 | Batch 011/110 | Loss: 0.23\n","Epoch: 002/025 | Batch 012/110 | Loss: 0.26\n","Epoch: 002/025 | Batch 013/110 | Loss: 0.23\n","Epoch: 002/025 | Batch 014/110 | Loss: 0.17\n","Epoch: 002/025 | Batch 015/110 | Loss: 0.39\n","Epoch: 002/025 | Batch 016/110 | Loss: 0.41\n","Epoch: 002/025 | Batch 017/110 | Loss: 0.40\n","Epoch: 002/025 | Batch 018/110 | Loss: 0.26\n","Epoch: 002/025 | Batch 019/110 | Loss: 0.35\n","Epoch: 002/025 | Batch 020/110 | Loss: 0.25\n","Epoch: 002/025 | Batch 021/110 | Loss: 0.25\n","Epoch: 002/025 | Batch 022/110 | Loss: 0.25\n","Epoch: 002/025 | Batch 023/110 | Loss: 0.30\n","Epoch: 002/025 | Batch 024/110 | Loss: 0.24\n","Epoch: 002/025 | Batch 025/110 | Loss: 0.13\n","Epoch: 002/025 | Batch 026/110 | Loss: 0.15\n","Epoch: 002/025 | Batch 027/110 | Loss: 0.25\n","Epoch: 002/025 | Batch 028/110 | Loss: 0.35\n","Epoch: 002/025 | Batch 029/110 | Loss: 0.15\n","Epoch: 002/025 | Batch 030/110 | Loss: 0.33\n","Epoch: 002/025 | Batch 031/110 | Loss: 0.31\n","Epoch: 002/025 | Batch 032/110 | Loss: 0.32\n","Epoch: 002/025 | Batch 033/110 | Loss: 0.28\n","Epoch: 002/025 | Batch 034/110 | Loss: 0.23\n","Epoch: 002/025 | Batch 035/110 | Loss: 0.26\n","Epoch: 002/025 | Batch 036/110 | Loss: 0.32\n","Epoch: 002/025 | Batch 037/110 | Loss: 0.27\n","Epoch: 002/025 | Batch 038/110 | Loss: 0.29\n","Epoch: 002/025 | Batch 039/110 | Loss: 0.32\n","Epoch: 002/025 | Batch 040/110 | Loss: 0.32\n","Epoch: 002/025 | Batch 041/110 | Loss: 0.25\n","Epoch: 002/025 | Batch 042/110 | Loss: 0.33\n","Epoch: 002/025 | Batch 043/110 | Loss: 0.20\n","Epoch: 002/025 | Batch 044/110 | Loss: 0.19\n","Epoch: 002/025 | Batch 045/110 | Loss: 0.25\n","Epoch: 002/025 | Batch 046/110 | Loss: 0.24\n","Epoch: 002/025 | Batch 047/110 | Loss: 0.24\n","Epoch: 002/025 | Batch 048/110 | Loss: 0.23\n","Epoch: 002/025 | Batch 049/110 | Loss: 0.16\n","Epoch: 002/025 | Batch 050/110 | Loss: 0.47\n","Epoch: 002/025 | Batch 051/110 | Loss: 0.20\n","Epoch: 002/025 | Batch 052/110 | Loss: 0.24\n","Epoch: 002/025 | Batch 053/110 | Loss: 0.20\n","Epoch: 002/025 | Batch 054/110 | Loss: 0.26\n","Epoch: 002/025 | Batch 055/110 | Loss: 0.18\n","Epoch: 002/025 | Batch 056/110 | Loss: 0.38\n","Epoch: 002/025 | Batch 057/110 | Loss: 0.17\n","Epoch: 002/025 | Batch 058/110 | Loss: 0.24\n","Epoch: 002/025 | Batch 059/110 | Loss: 0.27\n","Epoch: 002/025 | Batch 060/110 | Loss: 0.20\n","Epoch: 002/025 | Batch 061/110 | Loss: 0.56\n","Epoch: 002/025 | Batch 062/110 | Loss: 0.26\n","Epoch: 002/025 | Batch 063/110 | Loss: 0.24\n","Epoch: 002/025 | Batch 064/110 | Loss: 0.16\n","Epoch: 002/025 | Batch 065/110 | Loss: 0.21\n","Epoch: 002/025 | Batch 066/110 | Loss: 0.22\n","Epoch: 002/025 | Batch 067/110 | Loss: 0.15\n","Epoch: 002/025 | Batch 068/110 | Loss: 0.31\n","Epoch: 002/025 | Batch 069/110 | Loss: 0.29\n","Epoch: 002/025 | Batch 070/110 | Loss: 0.13\n","Epoch: 002/025 | Batch 071/110 | Loss: 0.30\n","Epoch: 002/025 | Batch 072/110 | Loss: 0.22\n","Epoch: 002/025 | Batch 073/110 | Loss: 0.23\n","Epoch: 002/025 | Batch 074/110 | Loss: 0.23\n","Epoch: 002/025 | Batch 075/110 | Loss: 0.24\n","Epoch: 002/025 | Batch 076/110 | Loss: 0.20\n","Epoch: 002/025 | Batch 077/110 | Loss: 0.19\n","Epoch: 002/025 | Batch 078/110 | Loss: 0.15\n","Epoch: 002/025 | Batch 079/110 | Loss: 0.22\n","Epoch: 002/025 | Batch 080/110 | Loss: 0.19\n","Epoch: 002/025 | Batch 081/110 | Loss: 0.24\n","Epoch: 002/025 | Batch 082/110 | Loss: 0.23\n","Epoch: 002/025 | Batch 083/110 | Loss: 0.28\n","Epoch: 002/025 | Batch 084/110 | Loss: 0.23\n","Epoch: 002/025 | Batch 085/110 | Loss: 0.13\n","Epoch: 002/025 | Batch 086/110 | Loss: 0.31\n","Epoch: 002/025 | Batch 087/110 | Loss: 0.29\n","Epoch: 002/025 | Batch 088/110 | Loss: 0.13\n","Epoch: 002/025 | Batch 089/110 | Loss: 0.26\n","Epoch: 002/025 | Batch 090/110 | Loss: 0.27\n","Epoch: 002/025 | Batch 091/110 | Loss: 0.22\n","Epoch: 002/025 | Batch 092/110 | Loss: 0.21\n","Epoch: 002/025 | Batch 093/110 | Loss: 0.21\n","Epoch: 002/025 | Batch 094/110 | Loss: 0.17\n","Epoch: 002/025 | Batch 095/110 | Loss: 0.22\n","Epoch: 002/025 | Batch 096/110 | Loss: 0.15\n","Epoch: 002/025 | Batch 097/110 | Loss: 0.35\n","Epoch: 002/025 | Batch 098/110 | Loss: 0.25\n","Epoch: 002/025 | Batch 099/110 | Loss: 0.12\n","Epoch: 002/025 | Batch 100/110 | Loss: 0.19\n","Epoch: 002/025 | Batch 101/110 | Loss: 0.10\n","Epoch: 002/025 | Batch 102/110 | Loss: 0.21\n","Epoch: 002/025 | Batch 103/110 | Loss: 0.22\n","Epoch: 002/025 | Batch 104/110 | Loss: 0.28\n","Epoch: 002/025 | Batch 105/110 | Loss: 0.27\n","Epoch: 002/025 | Batch 106/110 | Loss: 0.17\n","Epoch: 002/025 | Batch 107/110 | Loss: 0.17\n","Epoch: 002/025 | Batch 108/110 | Loss: 0.16\n","Epoch: 002/025 | Batch 109/110 | Loss: 0.16\n","Epoch: 003/025 | Batch 000/110 | Loss: 0.18\n","Epoch: 003/025 | Batch 001/110 | Loss: 0.23\n","Epoch: 003/025 | Batch 002/110 | Loss: 0.27\n","Epoch: 003/025 | Batch 003/110 | Loss: 0.10\n","Epoch: 003/025 | Batch 004/110 | Loss: 0.21\n","Epoch: 003/025 | Batch 005/110 | Loss: 0.25\n","Epoch: 003/025 | Batch 006/110 | Loss: 0.18\n","Epoch: 003/025 | Batch 007/110 | Loss: 0.16\n","Epoch: 003/025 | Batch 008/110 | Loss: 0.15\n","Epoch: 003/025 | Batch 009/110 | Loss: 0.20\n","Epoch: 003/025 | Batch 010/110 | Loss: 0.14\n","Epoch: 003/025 | Batch 011/110 | Loss: 0.08\n","Epoch: 003/025 | Batch 012/110 | Loss: 0.16\n","Epoch: 003/025 | Batch 013/110 | Loss: 0.23\n","Epoch: 003/025 | Batch 014/110 | Loss: 0.28\n","Epoch: 003/025 | Batch 015/110 | Loss: 0.21\n","Epoch: 003/025 | Batch 016/110 | Loss: 0.22\n","Epoch: 003/025 | Batch 017/110 | Loss: 0.20\n","Epoch: 003/025 | Batch 018/110 | Loss: 0.22\n","Epoch: 003/025 | Batch 019/110 | Loss: 0.15\n","Epoch: 003/025 | Batch 020/110 | Loss: 0.15\n","Epoch: 003/025 | Batch 021/110 | Loss: 0.22\n","Epoch: 003/025 | Batch 022/110 | Loss: 0.13\n","Epoch: 003/025 | Batch 023/110 | Loss: 0.11\n","Epoch: 003/025 | Batch 024/110 | Loss: 0.12\n","Epoch: 003/025 | Batch 025/110 | Loss: 0.15\n","Epoch: 003/025 | Batch 026/110 | Loss: 0.28\n","Epoch: 003/025 | Batch 027/110 | Loss: 0.21\n","Epoch: 003/025 | Batch 028/110 | Loss: 0.13\n","Epoch: 003/025 | Batch 029/110 | Loss: 0.18\n","Epoch: 003/025 | Batch 030/110 | Loss: 0.17\n","Epoch: 003/025 | Batch 031/110 | Loss: 0.36\n","Epoch: 003/025 | Batch 032/110 | Loss: 0.11\n","Epoch: 003/025 | Batch 033/110 | Loss: 0.22\n","Epoch: 003/025 | Batch 034/110 | Loss: 0.25\n","Epoch: 003/025 | Batch 035/110 | Loss: 0.10\n","Epoch: 003/025 | Batch 036/110 | Loss: 0.32\n","Epoch: 003/025 | Batch 037/110 | Loss: 0.20\n","Epoch: 003/025 | Batch 038/110 | Loss: 0.18\n","Epoch: 003/025 | Batch 039/110 | Loss: 0.25\n","Epoch: 003/025 | Batch 040/110 | Loss: 0.11\n","Epoch: 003/025 | Batch 041/110 | Loss: 0.14\n","Epoch: 003/025 | Batch 042/110 | Loss: 0.15\n","Epoch: 003/025 | Batch 043/110 | Loss: 0.13\n","Epoch: 003/025 | Batch 044/110 | Loss: 0.27\n","Epoch: 003/025 | Batch 045/110 | Loss: 0.21\n","Epoch: 003/025 | Batch 046/110 | Loss: 0.17\n","Epoch: 003/025 | Batch 047/110 | Loss: 0.15\n","Epoch: 003/025 | Batch 048/110 | Loss: 0.15\n","Epoch: 003/025 | Batch 049/110 | Loss: 0.12\n","Epoch: 003/025 | Batch 050/110 | Loss: 0.15\n","Epoch: 003/025 | Batch 051/110 | Loss: 0.13\n","Epoch: 003/025 | Batch 052/110 | Loss: 0.23\n","Epoch: 003/025 | Batch 053/110 | Loss: 0.09\n","Epoch: 003/025 | Batch 054/110 | Loss: 0.24\n","Epoch: 003/025 | Batch 055/110 | Loss: 0.24\n","Epoch: 003/025 | Batch 056/110 | Loss: 0.13\n","Epoch: 003/025 | Batch 057/110 | Loss: 0.11\n","Epoch: 003/025 | Batch 058/110 | Loss: 0.21\n","Epoch: 003/025 | Batch 059/110 | Loss: 0.15\n","Epoch: 003/025 | Batch 060/110 | Loss: 0.09\n","Epoch: 003/025 | Batch 061/110 | Loss: 0.16\n","Epoch: 003/025 | Batch 062/110 | Loss: 0.14\n","Epoch: 003/025 | Batch 063/110 | Loss: 0.17\n","Epoch: 003/025 | Batch 064/110 | Loss: 0.11\n","Epoch: 003/025 | Batch 065/110 | Loss: 0.18\n","Epoch: 003/025 | Batch 066/110 | Loss: 0.16\n","Epoch: 003/025 | Batch 067/110 | Loss: 0.23\n","Epoch: 003/025 | Batch 068/110 | Loss: 0.13\n","Epoch: 003/025 | Batch 069/110 | Loss: 0.15\n","Epoch: 003/025 | Batch 070/110 | Loss: 0.22\n","Epoch: 003/025 | Batch 071/110 | Loss: 0.16\n","Epoch: 003/025 | Batch 072/110 | Loss: 0.21\n","Epoch: 003/025 | Batch 073/110 | Loss: 0.21\n","Epoch: 003/025 | Batch 074/110 | Loss: 0.15\n","Epoch: 003/025 | Batch 075/110 | Loss: 0.28\n","Epoch: 003/025 | Batch 076/110 | Loss: 0.27\n","Epoch: 003/025 | Batch 077/110 | Loss: 0.14\n","Epoch: 003/025 | Batch 078/110 | Loss: 0.16\n","Epoch: 003/025 | Batch 079/110 | Loss: 0.10\n","Epoch: 003/025 | Batch 080/110 | Loss: 0.29\n","Epoch: 003/025 | Batch 081/110 | Loss: 0.11\n","Epoch: 003/025 | Batch 082/110 | Loss: 0.16\n","Epoch: 003/025 | Batch 083/110 | Loss: 0.28\n","Epoch: 003/025 | Batch 084/110 | Loss: 0.14\n","Epoch: 003/025 | Batch 085/110 | Loss: 0.27\n","Epoch: 003/025 | Batch 086/110 | Loss: 0.15\n","Epoch: 003/025 | Batch 087/110 | Loss: 0.20\n","Epoch: 003/025 | Batch 088/110 | Loss: 0.29\n","Epoch: 003/025 | Batch 089/110 | Loss: 0.21\n","Epoch: 003/025 | Batch 090/110 | Loss: 0.12\n","Epoch: 003/025 | Batch 091/110 | Loss: 0.20\n","Epoch: 003/025 | Batch 092/110 | Loss: 0.11\n","Epoch: 003/025 | Batch 093/110 | Loss: 0.17\n","Epoch: 003/025 | Batch 094/110 | Loss: 0.10\n","Epoch: 003/025 | Batch 095/110 | Loss: 0.13\n","Epoch: 003/025 | Batch 096/110 | Loss: 0.17\n","Epoch: 003/025 | Batch 097/110 | Loss: 0.29\n","Epoch: 003/025 | Batch 098/110 | Loss: 0.13\n","Epoch: 003/025 | Batch 099/110 | Loss: 0.24\n","Epoch: 003/025 | Batch 100/110 | Loss: 0.24\n","Epoch: 003/025 | Batch 101/110 | Loss: 0.21\n","Epoch: 003/025 | Batch 102/110 | Loss: 0.11\n","Epoch: 003/025 | Batch 103/110 | Loss: 0.12\n","Epoch: 003/025 | Batch 104/110 | Loss: 0.17\n","Epoch: 003/025 | Batch 105/110 | Loss: 0.14\n","Epoch: 003/025 | Batch 106/110 | Loss: 0.07\n","Epoch: 003/025 | Batch 107/110 | Loss: 0.17\n","Epoch: 003/025 | Batch 108/110 | Loss: 0.20\n","Epoch: 003/025 | Batch 109/110 | Loss: 0.22\n","Epoch: 004/025 | Batch 000/110 | Loss: 0.07\n","Epoch: 004/025 | Batch 001/110 | Loss: 0.22\n","Epoch: 004/025 | Batch 002/110 | Loss: 0.07\n","Epoch: 004/025 | Batch 003/110 | Loss: 0.13\n","Epoch: 004/025 | Batch 004/110 | Loss: 0.12\n","Epoch: 004/025 | Batch 005/110 | Loss: 0.27\n","Epoch: 004/025 | Batch 006/110 | Loss: 0.19\n","Epoch: 004/025 | Batch 007/110 | Loss: 0.10\n","Epoch: 004/025 | Batch 008/110 | Loss: 0.26\n","Epoch: 004/025 | Batch 009/110 | Loss: 0.20\n","Epoch: 004/025 | Batch 010/110 | Loss: 0.10\n","Epoch: 004/025 | Batch 011/110 | Loss: 0.24\n","Epoch: 004/025 | Batch 012/110 | Loss: 0.17\n","Epoch: 004/025 | Batch 013/110 | Loss: 0.14\n","Epoch: 004/025 | Batch 014/110 | Loss: 0.16\n","Epoch: 004/025 | Batch 015/110 | Loss: 0.12\n","Epoch: 004/025 | Batch 016/110 | Loss: 0.18\n","Epoch: 004/025 | Batch 017/110 | Loss: 0.10\n","Epoch: 004/025 | Batch 018/110 | Loss: 0.07\n","Epoch: 004/025 | Batch 019/110 | Loss: 0.15\n","Epoch: 004/025 | Batch 020/110 | Loss: 0.14\n","Epoch: 004/025 | Batch 021/110 | Loss: 0.17\n","Epoch: 004/025 | Batch 022/110 | Loss: 0.06\n","Epoch: 004/025 | Batch 023/110 | Loss: 0.21\n","Epoch: 004/025 | Batch 024/110 | Loss: 0.17\n","Epoch: 004/025 | Batch 025/110 | Loss: 0.13\n","Epoch: 004/025 | Batch 026/110 | Loss: 0.22\n","Epoch: 004/025 | Batch 027/110 | Loss: 0.18\n","Epoch: 004/025 | Batch 028/110 | Loss: 0.19\n","Epoch: 004/025 | Batch 029/110 | Loss: 0.24\n","Epoch: 004/025 | Batch 030/110 | Loss: 0.19\n","Epoch: 004/025 | Batch 031/110 | Loss: 0.16\n","Epoch: 004/025 | Batch 032/110 | Loss: 0.11\n","Epoch: 004/025 | Batch 033/110 | Loss: 0.16\n","Epoch: 004/025 | Batch 034/110 | Loss: 0.12\n","Epoch: 004/025 | Batch 035/110 | Loss: 0.18\n","Epoch: 004/025 | Batch 036/110 | Loss: 0.20\n","Epoch: 004/025 | Batch 037/110 | Loss: 0.15\n","Epoch: 004/025 | Batch 038/110 | Loss: 0.15\n","Epoch: 004/025 | Batch 039/110 | Loss: 0.14\n","Epoch: 004/025 | Batch 040/110 | Loss: 0.11\n","Epoch: 004/025 | Batch 041/110 | Loss: 0.08\n","Epoch: 004/025 | Batch 042/110 | Loss: 0.32\n","Epoch: 004/025 | Batch 043/110 | Loss: 0.08\n","Epoch: 004/025 | Batch 044/110 | Loss: 0.15\n","Epoch: 004/025 | Batch 045/110 | Loss: 0.10\n","Epoch: 004/025 | Batch 046/110 | Loss: 0.29\n","Epoch: 004/025 | Batch 047/110 | Loss: 0.15\n","Epoch: 004/025 | Batch 048/110 | Loss: 0.07\n","Epoch: 004/025 | Batch 049/110 | Loss: 0.11\n","Epoch: 004/025 | Batch 050/110 | Loss: 0.15\n","Epoch: 004/025 | Batch 051/110 | Loss: 0.13\n","Epoch: 004/025 | Batch 052/110 | Loss: 0.11\n","Epoch: 004/025 | Batch 053/110 | Loss: 0.12\n","Epoch: 004/025 | Batch 054/110 | Loss: 0.10\n","Epoch: 004/025 | Batch 055/110 | Loss: 0.32\n","Epoch: 004/025 | Batch 056/110 | Loss: 0.09\n","Epoch: 004/025 | Batch 057/110 | Loss: 0.08\n","Epoch: 004/025 | Batch 058/110 | Loss: 0.10\n","Epoch: 004/025 | Batch 059/110 | Loss: 0.18\n","Epoch: 004/025 | Batch 060/110 | Loss: 0.11\n","Epoch: 004/025 | Batch 061/110 | Loss: 0.16\n","Epoch: 004/025 | Batch 062/110 | Loss: 0.18\n","Epoch: 004/025 | Batch 063/110 | Loss: 0.12\n","Epoch: 004/025 | Batch 064/110 | Loss: 0.15\n","Epoch: 004/025 | Batch 065/110 | Loss: 0.14\n","Epoch: 004/025 | Batch 066/110 | Loss: 0.08\n","Epoch: 004/025 | Batch 067/110 | Loss: 0.16\n","Epoch: 004/025 | Batch 068/110 | Loss: 0.15\n","Epoch: 004/025 | Batch 069/110 | Loss: 0.14\n","Epoch: 004/025 | Batch 070/110 | Loss: 0.08\n","Epoch: 004/025 | Batch 071/110 | Loss: 0.10\n","Epoch: 004/025 | Batch 072/110 | Loss: 0.16\n","Epoch: 004/025 | Batch 073/110 | Loss: 0.09\n","Epoch: 004/025 | Batch 074/110 | Loss: 0.20\n","Epoch: 004/025 | Batch 075/110 | Loss: 0.12\n","Epoch: 004/025 | Batch 076/110 | Loss: 0.14\n","Epoch: 004/025 | Batch 077/110 | Loss: 0.14\n","Epoch: 004/025 | Batch 078/110 | Loss: 0.06\n","Epoch: 004/025 | Batch 079/110 | Loss: 0.06\n","Epoch: 004/025 | Batch 080/110 | Loss: 0.23\n","Epoch: 004/025 | Batch 081/110 | Loss: 0.17\n","Epoch: 004/025 | Batch 082/110 | Loss: 0.08\n","Epoch: 004/025 | Batch 083/110 | Loss: 0.13\n","Epoch: 004/025 | Batch 084/110 | Loss: 0.06\n","Epoch: 004/025 | Batch 085/110 | Loss: 0.13\n","Epoch: 004/025 | Batch 086/110 | Loss: 0.06\n","Epoch: 004/025 | Batch 087/110 | Loss: 0.11\n","Epoch: 004/025 | Batch 088/110 | Loss: 0.05\n","Epoch: 004/025 | Batch 089/110 | Loss: 0.15\n","Epoch: 004/025 | Batch 090/110 | Loss: 0.06\n","Epoch: 004/025 | Batch 091/110 | Loss: 0.11\n","Epoch: 004/025 | Batch 092/110 | Loss: 0.15\n","Epoch: 004/025 | Batch 093/110 | Loss: 0.10\n","Epoch: 004/025 | Batch 094/110 | Loss: 0.13\n","Epoch: 004/025 | Batch 095/110 | Loss: 0.28\n","Epoch: 004/025 | Batch 096/110 | Loss: 0.08\n","Epoch: 004/025 | Batch 097/110 | Loss: 0.18\n","Epoch: 004/025 | Batch 098/110 | Loss: 0.25\n","Epoch: 004/025 | Batch 099/110 | Loss: 0.29\n","Epoch: 004/025 | Batch 100/110 | Loss: 0.12\n","Epoch: 004/025 | Batch 101/110 | Loss: 0.08\n","Epoch: 004/025 | Batch 102/110 | Loss: 0.21\n","Epoch: 004/025 | Batch 103/110 | Loss: 0.08\n","Epoch: 004/025 | Batch 104/110 | Loss: 0.23\n","Epoch: 004/025 | Batch 105/110 | Loss: 0.10\n","Epoch: 004/025 | Batch 106/110 | Loss: 0.16\n","Epoch: 004/025 | Batch 107/110 | Loss: 0.11\n","Epoch: 004/025 | Batch 108/110 | Loss: 0.12\n","Epoch: 004/025 | Batch 109/110 | Loss: 0.12\n","Epoch: 005/025 | Batch 000/110 | Loss: 0.09\n","Epoch: 005/025 | Batch 001/110 | Loss: 0.23\n","Epoch: 005/025 | Batch 002/110 | Loss: 0.19\n","Epoch: 005/025 | Batch 003/110 | Loss: 0.10\n","Epoch: 005/025 | Batch 004/110 | Loss: 0.10\n","Epoch: 005/025 | Batch 005/110 | Loss: 0.09\n","Epoch: 005/025 | Batch 006/110 | Loss: 0.18\n","Epoch: 005/025 | Batch 007/110 | Loss: 0.16\n","Epoch: 005/025 | Batch 008/110 | Loss: 0.12\n","Epoch: 005/025 | Batch 009/110 | Loss: 0.10\n","Epoch: 005/025 | Batch 010/110 | Loss: 0.09\n","Epoch: 005/025 | Batch 011/110 | Loss: 0.13\n","Epoch: 005/025 | Batch 012/110 | Loss: 0.11\n","Epoch: 005/025 | Batch 013/110 | Loss: 0.12\n","Epoch: 005/025 | Batch 014/110 | Loss: 0.16\n","Epoch: 005/025 | Batch 015/110 | Loss: 0.26\n","Epoch: 005/025 | Batch 016/110 | Loss: 0.15\n","Epoch: 005/025 | Batch 017/110 | Loss: 0.19\n","Epoch: 005/025 | Batch 018/110 | Loss: 0.15\n","Epoch: 005/025 | Batch 019/110 | Loss: 0.21\n","Epoch: 005/025 | Batch 020/110 | Loss: 0.14\n","Epoch: 005/025 | Batch 021/110 | Loss: 0.19\n","Epoch: 005/025 | Batch 022/110 | Loss: 0.10\n","Epoch: 005/025 | Batch 023/110 | Loss: 0.05\n","Epoch: 005/025 | Batch 024/110 | Loss: 0.10\n","Epoch: 005/025 | Batch 025/110 | Loss: 0.15\n","Epoch: 005/025 | Batch 026/110 | Loss: 0.18\n","Epoch: 005/025 | Batch 027/110 | Loss: 0.10\n","Epoch: 005/025 | Batch 028/110 | Loss: 0.15\n","Epoch: 005/025 | Batch 029/110 | Loss: 0.12\n","Epoch: 005/025 | Batch 030/110 | Loss: 0.08\n","Epoch: 005/025 | Batch 031/110 | Loss: 0.18\n","Epoch: 005/025 | Batch 032/110 | Loss: 0.12\n","Epoch: 005/025 | Batch 033/110 | Loss: 0.06\n","Epoch: 005/025 | Batch 034/110 | Loss: 0.13\n","Epoch: 005/025 | Batch 035/110 | Loss: 0.13\n","Epoch: 005/025 | Batch 036/110 | Loss: 0.09\n","Epoch: 005/025 | Batch 037/110 | Loss: 0.09\n","Epoch: 005/025 | Batch 038/110 | Loss: 0.14\n","Epoch: 005/025 | Batch 039/110 | Loss: 0.07\n","Epoch: 005/025 | Batch 040/110 | Loss: 0.11\n","Epoch: 005/025 | Batch 041/110 | Loss: 0.09\n","Epoch: 005/025 | Batch 042/110 | Loss: 0.06\n","Epoch: 005/025 | Batch 043/110 | Loss: 0.07\n","Epoch: 005/025 | Batch 044/110 | Loss: 0.22\n","Epoch: 005/025 | Batch 045/110 | Loss: 0.11\n","Epoch: 005/025 | Batch 046/110 | Loss: 0.11\n","Epoch: 005/025 | Batch 047/110 | Loss: 0.07\n","Epoch: 005/025 | Batch 048/110 | Loss: 0.06\n","Epoch: 005/025 | Batch 049/110 | Loss: 0.14\n","Epoch: 005/025 | Batch 050/110 | Loss: 0.06\n","Epoch: 005/025 | Batch 051/110 | Loss: 0.12\n","Epoch: 005/025 | Batch 052/110 | Loss: 0.06\n","Epoch: 005/025 | Batch 053/110 | Loss: 0.04\n","Epoch: 005/025 | Batch 054/110 | Loss: 0.21\n","Epoch: 005/025 | Batch 055/110 | Loss: 0.13\n","Epoch: 005/025 | Batch 056/110 | Loss: 0.07\n","Epoch: 005/025 | Batch 057/110 | Loss: 0.10\n","Epoch: 005/025 | Batch 058/110 | Loss: 0.21\n","Epoch: 005/025 | Batch 059/110 | Loss: 0.07\n","Epoch: 005/025 | Batch 060/110 | Loss: 0.07\n","Epoch: 005/025 | Batch 061/110 | Loss: 0.07\n","Epoch: 005/025 | Batch 062/110 | Loss: 0.09\n","Epoch: 005/025 | Batch 063/110 | Loss: 0.10\n","Epoch: 005/025 | Batch 064/110 | Loss: 0.12\n","Epoch: 005/025 | Batch 065/110 | Loss: 0.04\n","Epoch: 005/025 | Batch 066/110 | Loss: 0.18\n","Epoch: 005/025 | Batch 067/110 | Loss: 0.14\n","Epoch: 005/025 | Batch 068/110 | Loss: 0.17\n","Epoch: 005/025 | Batch 069/110 | Loss: 0.10\n","Epoch: 005/025 | Batch 070/110 | Loss: 0.10\n","Epoch: 005/025 | Batch 071/110 | Loss: 0.10\n","Epoch: 005/025 | Batch 072/110 | Loss: 0.10\n","Epoch: 005/025 | Batch 073/110 | Loss: 0.08\n","Epoch: 005/025 | Batch 074/110 | Loss: 0.08\n","Epoch: 005/025 | Batch 075/110 | Loss: 0.08\n","Epoch: 005/025 | Batch 076/110 | Loss: 0.18\n","Epoch: 005/025 | Batch 077/110 | Loss: 0.15\n","Epoch: 005/025 | Batch 078/110 | Loss: 0.13\n","Epoch: 005/025 | Batch 079/110 | Loss: 0.27\n","Epoch: 005/025 | Batch 080/110 | Loss: 0.15\n","Epoch: 005/025 | Batch 081/110 | Loss: 0.07\n","Epoch: 005/025 | Batch 082/110 | Loss: 0.14\n","Epoch: 005/025 | Batch 083/110 | Loss: 0.12\n","Epoch: 005/025 | Batch 084/110 | Loss: 0.10\n","Epoch: 005/025 | Batch 085/110 | Loss: 0.09\n","Epoch: 005/025 | Batch 086/110 | Loss: 0.21\n","Epoch: 005/025 | Batch 087/110 | Loss: 0.22\n","Epoch: 005/025 | Batch 088/110 | Loss: 0.13\n","Epoch: 005/025 | Batch 089/110 | Loss: 0.05\n","Epoch: 005/025 | Batch 090/110 | Loss: 0.14\n","Epoch: 005/025 | Batch 091/110 | Loss: 0.07\n","Epoch: 005/025 | Batch 092/110 | Loss: 0.31\n","Epoch: 005/025 | Batch 093/110 | Loss: 0.08\n","Epoch: 005/025 | Batch 094/110 | Loss: 0.11\n","Epoch: 005/025 | Batch 095/110 | Loss: 0.06\n","Epoch: 005/025 | Batch 096/110 | Loss: 0.19\n","Epoch: 005/025 | Batch 097/110 | Loss: 0.10\n","Epoch: 005/025 | Batch 098/110 | Loss: 0.19\n","Epoch: 005/025 | Batch 099/110 | Loss: 0.11\n","Epoch: 005/025 | Batch 100/110 | Loss: 0.16\n","Epoch: 005/025 | Batch 101/110 | Loss: 0.11\n","Epoch: 005/025 | Batch 102/110 | Loss: 0.18\n","Epoch: 005/025 | Batch 103/110 | Loss: 0.11\n","Epoch: 005/025 | Batch 104/110 | Loss: 0.10\n","Epoch: 005/025 | Batch 105/110 | Loss: 0.11\n","Epoch: 005/025 | Batch 106/110 | Loss: 0.14\n","Epoch: 005/025 | Batch 107/110 | Loss: 0.08\n","Epoch: 005/025 | Batch 108/110 | Loss: 0.05\n","Epoch: 005/025 | Batch 109/110 | Loss: 0.08\n","Epoch: 006/025 | Batch 000/110 | Loss: 0.08\n","Epoch: 006/025 | Batch 001/110 | Loss: 0.15\n","Epoch: 006/025 | Batch 002/110 | Loss: 0.07\n","Epoch: 006/025 | Batch 003/110 | Loss: 0.10\n","Epoch: 006/025 | Batch 004/110 | Loss: 0.12\n","Epoch: 006/025 | Batch 005/110 | Loss: 0.15\n","Epoch: 006/025 | Batch 006/110 | Loss: 0.19\n","Epoch: 006/025 | Batch 007/110 | Loss: 0.06\n","Epoch: 006/025 | Batch 008/110 | Loss: 0.03\n","Epoch: 006/025 | Batch 009/110 | Loss: 0.11\n","Epoch: 006/025 | Batch 010/110 | Loss: 0.11\n","Epoch: 006/025 | Batch 011/110 | Loss: 0.12\n","Epoch: 006/025 | Batch 012/110 | Loss: 0.04\n","Epoch: 006/025 | Batch 013/110 | Loss: 0.11\n","Epoch: 006/025 | Batch 014/110 | Loss: 0.28\n","Epoch: 006/025 | Batch 015/110 | Loss: 0.13\n","Epoch: 006/025 | Batch 016/110 | Loss: 0.04\n","Epoch: 006/025 | Batch 017/110 | Loss: 0.04\n","Epoch: 006/025 | Batch 018/110 | Loss: 0.16\n","Epoch: 006/025 | Batch 019/110 | Loss: 0.17\n","Epoch: 006/025 | Batch 020/110 | Loss: 0.10\n","Epoch: 006/025 | Batch 021/110 | Loss: 0.08\n","Epoch: 006/025 | Batch 022/110 | Loss: 0.07\n","Epoch: 006/025 | Batch 023/110 | Loss: 0.06\n","Epoch: 006/025 | Batch 024/110 | Loss: 0.11\n","Epoch: 006/025 | Batch 025/110 | Loss: 0.14\n","Epoch: 006/025 | Batch 026/110 | Loss: 0.07\n","Epoch: 006/025 | Batch 027/110 | Loss: 0.07\n","Epoch: 006/025 | Batch 028/110 | Loss: 0.17\n","Epoch: 006/025 | Batch 029/110 | Loss: 0.19\n","Epoch: 006/025 | Batch 030/110 | Loss: 0.10\n","Epoch: 006/025 | Batch 031/110 | Loss: 0.08\n","Epoch: 006/025 | Batch 032/110 | Loss: 0.22\n","Epoch: 006/025 | Batch 033/110 | Loss: 0.19\n","Epoch: 006/025 | Batch 034/110 | Loss: 0.10\n","Epoch: 006/025 | Batch 035/110 | Loss: 0.18\n","Epoch: 006/025 | Batch 036/110 | Loss: 0.07\n","Epoch: 006/025 | Batch 037/110 | Loss: 0.06\n","Epoch: 006/025 | Batch 038/110 | Loss: 0.10\n","Epoch: 006/025 | Batch 039/110 | Loss: 0.09\n","Epoch: 006/025 | Batch 040/110 | Loss: 0.09\n","Epoch: 006/025 | Batch 041/110 | Loss: 0.08\n","Epoch: 006/025 | Batch 042/110 | Loss: 0.14\n","Epoch: 006/025 | Batch 043/110 | Loss: 0.17\n","Epoch: 006/025 | Batch 044/110 | Loss: 0.09\n","Epoch: 006/025 | Batch 045/110 | Loss: 0.07\n","Epoch: 006/025 | Batch 046/110 | Loss: 0.07\n","Epoch: 006/025 | Batch 047/110 | Loss: 0.12\n","Epoch: 006/025 | Batch 048/110 | Loss: 0.08\n","Epoch: 006/025 | Batch 049/110 | Loss: 0.15\n","Epoch: 006/025 | Batch 050/110 | Loss: 0.16\n","Epoch: 006/025 | Batch 051/110 | Loss: 0.04\n","Epoch: 006/025 | Batch 052/110 | Loss: 0.22\n","Epoch: 006/025 | Batch 053/110 | Loss: 0.05\n","Epoch: 006/025 | Batch 054/110 | Loss: 0.19\n","Epoch: 006/025 | Batch 055/110 | Loss: 0.15\n","Epoch: 006/025 | Batch 056/110 | Loss: 0.12\n","Epoch: 006/025 | Batch 057/110 | Loss: 0.07\n","Epoch: 006/025 | Batch 058/110 | Loss: 0.05\n","Epoch: 006/025 | Batch 059/110 | Loss: 0.05\n","Epoch: 006/025 | Batch 060/110 | Loss: 0.13\n","Epoch: 006/025 | Batch 061/110 | Loss: 0.21\n","Epoch: 006/025 | Batch 062/110 | Loss: 0.07\n","Epoch: 006/025 | Batch 063/110 | Loss: 0.08\n","Epoch: 006/025 | Batch 064/110 | Loss: 0.07\n","Epoch: 006/025 | Batch 065/110 | Loss: 0.13\n","Epoch: 006/025 | Batch 066/110 | Loss: 0.04\n","Epoch: 006/025 | Batch 067/110 | Loss: 0.14\n","Epoch: 006/025 | Batch 068/110 | Loss: 0.11\n","Epoch: 006/025 | Batch 069/110 | Loss: 0.09\n","Epoch: 006/025 | Batch 070/110 | Loss: 0.10\n","Epoch: 006/025 | Batch 071/110 | Loss: 0.20\n","Epoch: 006/025 | Batch 072/110 | Loss: 0.10\n","Epoch: 006/025 | Batch 073/110 | Loss: 0.09\n","Epoch: 006/025 | Batch 074/110 | Loss: 0.13\n","Epoch: 006/025 | Batch 075/110 | Loss: 0.08\n","Epoch: 006/025 | Batch 076/110 | Loss: 0.14\n","Epoch: 006/025 | Batch 077/110 | Loss: 0.13\n","Epoch: 006/025 | Batch 078/110 | Loss: 0.04\n","Epoch: 006/025 | Batch 079/110 | Loss: 0.12\n","Epoch: 006/025 | Batch 080/110 | Loss: 0.08\n","Epoch: 006/025 | Batch 081/110 | Loss: 0.07\n","Epoch: 006/025 | Batch 082/110 | Loss: 0.15\n","Epoch: 006/025 | Batch 083/110 | Loss: 0.05\n","Epoch: 006/025 | Batch 084/110 | Loss: 0.06\n","Epoch: 006/025 | Batch 085/110 | Loss: 0.25\n","Epoch: 006/025 | Batch 086/110 | Loss: 0.05\n","Epoch: 006/025 | Batch 087/110 | Loss: 0.14\n","Epoch: 006/025 | Batch 088/110 | Loss: 0.05\n","Epoch: 006/025 | Batch 089/110 | Loss: 0.06\n","Epoch: 006/025 | Batch 090/110 | Loss: 0.04\n","Epoch: 006/025 | Batch 091/110 | Loss: 0.12\n","Epoch: 006/025 | Batch 092/110 | Loss: 0.07\n","Epoch: 006/025 | Batch 093/110 | Loss: 0.14\n","Epoch: 006/025 | Batch 094/110 | Loss: 0.04\n","Epoch: 006/025 | Batch 095/110 | Loss: 0.06\n","Epoch: 006/025 | Batch 096/110 | Loss: 0.18\n","Epoch: 006/025 | Batch 097/110 | Loss: 0.05\n","Epoch: 006/025 | Batch 098/110 | Loss: 0.12\n","Epoch: 006/025 | Batch 099/110 | Loss: 0.09\n","Epoch: 006/025 | Batch 100/110 | Loss: 0.10\n","Epoch: 006/025 | Batch 101/110 | Loss: 0.11\n","Epoch: 006/025 | Batch 102/110 | Loss: 0.18\n","Epoch: 006/025 | Batch 103/110 | Loss: 0.13\n","Epoch: 006/025 | Batch 104/110 | Loss: 0.21\n","Epoch: 006/025 | Batch 105/110 | Loss: 0.06\n","Epoch: 006/025 | Batch 106/110 | Loss: 0.04\n","Epoch: 006/025 | Batch 107/110 | Loss: 0.17\n","Epoch: 006/025 | Batch 108/110 | Loss: 0.16\n","Epoch: 006/025 | Batch 109/110 | Loss: 0.08\n","Epoch: 007/025 | Batch 000/110 | Loss: 0.05\n","Epoch: 007/025 | Batch 001/110 | Loss: 0.14\n","Epoch: 007/025 | Batch 002/110 | Loss: 0.09\n","Epoch: 007/025 | Batch 003/110 | Loss: 0.08\n","Epoch: 007/025 | Batch 004/110 | Loss: 0.11\n","Epoch: 007/025 | Batch 005/110 | Loss: 0.12\n","Epoch: 007/025 | Batch 006/110 | Loss: 0.12\n","Epoch: 007/025 | Batch 007/110 | Loss: 0.07\n","Epoch: 007/025 | Batch 008/110 | Loss: 0.07\n","Epoch: 007/025 | Batch 009/110 | Loss: 0.15\n","Epoch: 007/025 | Batch 010/110 | Loss: 0.10\n","Epoch: 007/025 | Batch 011/110 | Loss: 0.06\n","Epoch: 007/025 | Batch 012/110 | Loss: 0.18\n","Epoch: 007/025 | Batch 013/110 | Loss: 0.09\n","Epoch: 007/025 | Batch 014/110 | Loss: 0.09\n","Epoch: 007/025 | Batch 015/110 | Loss: 0.05\n","Epoch: 007/025 | Batch 016/110 | Loss: 0.19\n","Epoch: 007/025 | Batch 017/110 | Loss: 0.09\n","Epoch: 007/025 | Batch 018/110 | Loss: 0.14\n","Epoch: 007/025 | Batch 019/110 | Loss: 0.09\n","Epoch: 007/025 | Batch 020/110 | Loss: 0.09\n","Epoch: 007/025 | Batch 021/110 | Loss: 0.06\n","Epoch: 007/025 | Batch 022/110 | Loss: 0.10\n","Epoch: 007/025 | Batch 023/110 | Loss: 0.04\n","Epoch: 007/025 | Batch 024/110 | Loss: 0.12\n","Epoch: 007/025 | Batch 025/110 | Loss: 0.10\n","Epoch: 007/025 | Batch 026/110 | Loss: 0.05\n","Epoch: 007/025 | Batch 027/110 | Loss: 0.11\n","Epoch: 007/025 | Batch 028/110 | Loss: 0.05\n","Epoch: 007/025 | Batch 029/110 | Loss: 0.09\n","Epoch: 007/025 | Batch 030/110 | Loss: 0.12\n","Epoch: 007/025 | Batch 031/110 | Loss: 0.05\n","Epoch: 007/025 | Batch 032/110 | Loss: 0.12\n","Epoch: 007/025 | Batch 033/110 | Loss: 0.10\n","Epoch: 007/025 | Batch 034/110 | Loss: 0.21\n","Epoch: 007/025 | Batch 035/110 | Loss: 0.03\n","Epoch: 007/025 | Batch 036/110 | Loss: 0.12\n","Epoch: 007/025 | Batch 037/110 | Loss: 0.16\n","Epoch: 007/025 | Batch 038/110 | Loss: 0.05\n","Epoch: 007/025 | Batch 039/110 | Loss: 0.08\n","Epoch: 007/025 | Batch 040/110 | Loss: 0.05\n","Epoch: 007/025 | Batch 041/110 | Loss: 0.12\n","Epoch: 007/025 | Batch 042/110 | Loss: 0.18\n","Epoch: 007/025 | Batch 043/110 | Loss: 0.08\n","Epoch: 007/025 | Batch 044/110 | Loss: 0.04\n","Epoch: 007/025 | Batch 045/110 | Loss: 0.25\n","Epoch: 007/025 | Batch 046/110 | Loss: 0.07\n","Epoch: 007/025 | Batch 047/110 | Loss: 0.12\n","Epoch: 007/025 | Batch 048/110 | Loss: 0.07\n","Epoch: 007/025 | Batch 049/110 | Loss: 0.08\n","Epoch: 007/025 | Batch 050/110 | Loss: 0.14\n","Epoch: 007/025 | Batch 051/110 | Loss: 0.15\n","Epoch: 007/025 | Batch 052/110 | Loss: 0.10\n","Epoch: 007/025 | Batch 053/110 | Loss: 0.06\n","Epoch: 007/025 | Batch 054/110 | Loss: 0.03\n","Epoch: 007/025 | Batch 055/110 | Loss: 0.09\n","Epoch: 007/025 | Batch 056/110 | Loss: 0.04\n","Epoch: 007/025 | Batch 057/110 | Loss: 0.14\n","Epoch: 007/025 | Batch 058/110 | Loss: 0.27\n","Epoch: 007/025 | Batch 059/110 | Loss: 0.13\n","Epoch: 007/025 | Batch 060/110 | Loss: 0.12\n","Epoch: 007/025 | Batch 061/110 | Loss: 0.06\n","Epoch: 007/025 | Batch 062/110 | Loss: 0.07\n","Epoch: 007/025 | Batch 063/110 | Loss: 0.17\n","Epoch: 007/025 | Batch 064/110 | Loss: 0.06\n","Epoch: 007/025 | Batch 065/110 | Loss: 0.09\n","Epoch: 007/025 | Batch 066/110 | Loss: 0.10\n","Epoch: 007/025 | Batch 067/110 | Loss: 0.08\n","Epoch: 007/025 | Batch 068/110 | Loss: 0.10\n","Epoch: 007/025 | Batch 069/110 | Loss: 0.06\n","Epoch: 007/025 | Batch 070/110 | Loss: 0.07\n","Epoch: 007/025 | Batch 071/110 | Loss: 0.07\n","Epoch: 007/025 | Batch 072/110 | Loss: 0.07\n","Epoch: 007/025 | Batch 073/110 | Loss: 0.04\n","Epoch: 007/025 | Batch 074/110 | Loss: 0.04\n","Epoch: 007/025 | Batch 075/110 | Loss: 0.13\n","Epoch: 007/025 | Batch 076/110 | Loss: 0.08\n","Epoch: 007/025 | Batch 077/110 | Loss: 0.06\n","Epoch: 007/025 | Batch 078/110 | Loss: 0.04\n","Epoch: 007/025 | Batch 079/110 | Loss: 0.07\n","Epoch: 007/025 | Batch 080/110 | Loss: 0.08\n","Epoch: 007/025 | Batch 081/110 | Loss: 0.18\n","Epoch: 007/025 | Batch 082/110 | Loss: 0.05\n","Epoch: 007/025 | Batch 083/110 | Loss: 0.05\n","Epoch: 007/025 | Batch 084/110 | Loss: 0.13\n","Epoch: 007/025 | Batch 085/110 | Loss: 0.08\n","Epoch: 007/025 | Batch 086/110 | Loss: 0.13\n","Epoch: 007/025 | Batch 087/110 | Loss: 0.04\n","Epoch: 007/025 | Batch 088/110 | Loss: 0.10\n","Epoch: 007/025 | Batch 089/110 | Loss: 0.10\n","Epoch: 007/025 | Batch 090/110 | Loss: 0.25\n","Epoch: 007/025 | Batch 091/110 | Loss: 0.07\n","Epoch: 007/025 | Batch 092/110 | Loss: 0.06\n","Epoch: 007/025 | Batch 093/110 | Loss: 0.09\n","Epoch: 007/025 | Batch 094/110 | Loss: 0.20\n","Epoch: 007/025 | Batch 095/110 | Loss: 0.05\n","Epoch: 007/025 | Batch 096/110 | Loss: 0.12\n","Epoch: 007/025 | Batch 097/110 | Loss: 0.06\n","Epoch: 007/025 | Batch 098/110 | Loss: 0.22\n","Epoch: 007/025 | Batch 099/110 | Loss: 0.06\n","Epoch: 007/025 | Batch 100/110 | Loss: 0.16\n","Epoch: 007/025 | Batch 101/110 | Loss: 0.07\n","Epoch: 007/025 | Batch 102/110 | Loss: 0.24\n","Epoch: 007/025 | Batch 103/110 | Loss: 0.04\n","Epoch: 007/025 | Batch 104/110 | Loss: 0.04\n","Epoch: 007/025 | Batch 105/110 | Loss: 0.11\n","Epoch: 007/025 | Batch 106/110 | Loss: 0.16\n","Epoch: 007/025 | Batch 107/110 | Loss: 0.07\n","Epoch: 007/025 | Batch 108/110 | Loss: 0.09\n","Epoch: 007/025 | Batch 109/110 | Loss: 0.04\n","Epoch: 008/025 | Batch 000/110 | Loss: 0.06\n","Epoch: 008/025 | Batch 001/110 | Loss: 0.15\n","Epoch: 008/025 | Batch 002/110 | Loss: 0.04\n","Epoch: 008/025 | Batch 003/110 | Loss: 0.29\n","Epoch: 008/025 | Batch 004/110 | Loss: 0.07\n","Epoch: 008/025 | Batch 005/110 | Loss: 0.04\n","Epoch: 008/025 | Batch 006/110 | Loss: 0.07\n","Epoch: 008/025 | Batch 007/110 | Loss: 0.06\n","Epoch: 008/025 | Batch 008/110 | Loss: 0.13\n","Epoch: 008/025 | Batch 009/110 | Loss: 0.06\n","Epoch: 008/025 | Batch 010/110 | Loss: 0.05\n","Epoch: 008/025 | Batch 011/110 | Loss: 0.13\n","Epoch: 008/025 | Batch 012/110 | Loss: 0.13\n","Epoch: 008/025 | Batch 013/110 | Loss: 0.06\n","Epoch: 008/025 | Batch 014/110 | Loss: 0.09\n","Epoch: 008/025 | Batch 015/110 | Loss: 0.09\n","Epoch: 008/025 | Batch 016/110 | Loss: 0.03\n","Epoch: 008/025 | Batch 017/110 | Loss: 0.09\n","Epoch: 008/025 | Batch 018/110 | Loss: 0.05\n","Epoch: 008/025 | Batch 019/110 | Loss: 0.23\n","Epoch: 008/025 | Batch 020/110 | Loss: 0.13\n","Epoch: 008/025 | Batch 021/110 | Loss: 0.05\n","Epoch: 008/025 | Batch 022/110 | Loss: 0.16\n","Epoch: 008/025 | Batch 023/110 | Loss: 0.22\n","Epoch: 008/025 | Batch 024/110 | Loss: 0.14\n","Epoch: 008/025 | Batch 025/110 | Loss: 0.27\n","Epoch: 008/025 | Batch 026/110 | Loss: 0.03\n","Epoch: 008/025 | Batch 027/110 | Loss: 0.06\n","Epoch: 008/025 | Batch 028/110 | Loss: 0.04\n","Epoch: 008/025 | Batch 029/110 | Loss: 0.13\n","Epoch: 008/025 | Batch 030/110 | Loss: 0.04\n","Epoch: 008/025 | Batch 031/110 | Loss: 0.25\n","Epoch: 008/025 | Batch 032/110 | Loss: 0.09\n","Epoch: 008/025 | Batch 033/110 | Loss: 0.10\n","Epoch: 008/025 | Batch 034/110 | Loss: 0.12\n","Epoch: 008/025 | Batch 035/110 | Loss: 0.06\n","Epoch: 008/025 | Batch 036/110 | Loss: 0.08\n","Epoch: 008/025 | Batch 037/110 | Loss: 0.06\n","Epoch: 008/025 | Batch 038/110 | Loss: 0.11\n","Epoch: 008/025 | Batch 039/110 | Loss: 0.11\n","Epoch: 008/025 | Batch 040/110 | Loss: 0.06\n","Epoch: 008/025 | Batch 041/110 | Loss: 0.03\n","Epoch: 008/025 | Batch 042/110 | Loss: 0.05\n","Epoch: 008/025 | Batch 043/110 | Loss: 0.06\n","Epoch: 008/025 | Batch 044/110 | Loss: 0.14\n","Epoch: 008/025 | Batch 045/110 | Loss: 0.04\n","Epoch: 008/025 | Batch 046/110 | Loss: 0.09\n","Epoch: 008/025 | Batch 047/110 | Loss: 0.10\n","Epoch: 008/025 | Batch 048/110 | Loss: 0.04\n","Epoch: 008/025 | Batch 049/110 | Loss: 0.03\n","Epoch: 008/025 | Batch 050/110 | Loss: 0.06\n","Epoch: 008/025 | Batch 051/110 | Loss: 0.03\n","Epoch: 008/025 | Batch 052/110 | Loss: 0.24\n","Epoch: 008/025 | Batch 053/110 | Loss: 0.04\n","Epoch: 008/025 | Batch 054/110 | Loss: 0.04\n","Epoch: 008/025 | Batch 055/110 | Loss: 0.12\n","Epoch: 008/025 | Batch 056/110 | Loss: 0.05\n","Epoch: 008/025 | Batch 057/110 | Loss: 0.11\n","Epoch: 008/025 | Batch 058/110 | Loss: 0.05\n","Epoch: 008/025 | Batch 059/110 | Loss: 0.10\n","Epoch: 008/025 | Batch 060/110 | Loss: 0.06\n","Epoch: 008/025 | Batch 061/110 | Loss: 0.11\n","Epoch: 008/025 | Batch 062/110 | Loss: 0.03\n","Epoch: 008/025 | Batch 063/110 | Loss: 0.08\n","Epoch: 008/025 | Batch 064/110 | Loss: 0.07\n","Epoch: 008/025 | Batch 065/110 | Loss: 0.05\n","Epoch: 008/025 | Batch 066/110 | Loss: 0.14\n","Epoch: 008/025 | Batch 067/110 | Loss: 0.06\n","Epoch: 008/025 | Batch 068/110 | Loss: 0.06\n","Epoch: 008/025 | Batch 069/110 | Loss: 0.13\n","Epoch: 008/025 | Batch 070/110 | Loss: 0.10\n","Epoch: 008/025 | Batch 071/110 | Loss: 0.10\n","Epoch: 008/025 | Batch 072/110 | Loss: 0.05\n","Epoch: 008/025 | Batch 073/110 | Loss: 0.12\n","Epoch: 008/025 | Batch 074/110 | Loss: 0.05\n","Epoch: 008/025 | Batch 075/110 | Loss: 0.12\n","Epoch: 008/025 | Batch 076/110 | Loss: 0.08\n","Epoch: 008/025 | Batch 077/110 | Loss: 0.07\n","Epoch: 008/025 | Batch 078/110 | Loss: 0.10\n","Epoch: 008/025 | Batch 079/110 | Loss: 0.08\n","Epoch: 008/025 | Batch 080/110 | Loss: 0.07\n","Epoch: 008/025 | Batch 081/110 | Loss: 0.08\n","Epoch: 008/025 | Batch 082/110 | Loss: 0.16\n","Epoch: 008/025 | Batch 083/110 | Loss: 0.09\n","Epoch: 008/025 | Batch 084/110 | Loss: 0.06\n","Epoch: 008/025 | Batch 085/110 | Loss: 0.06\n","Epoch: 008/025 | Batch 086/110 | Loss: 0.11\n","Epoch: 008/025 | Batch 087/110 | Loss: 0.06\n","Epoch: 008/025 | Batch 088/110 | Loss: 0.09\n","Epoch: 008/025 | Batch 089/110 | Loss: 0.07\n","Epoch: 008/025 | Batch 090/110 | Loss: 0.06\n","Epoch: 008/025 | Batch 091/110 | Loss: 0.06\n","Epoch: 008/025 | Batch 092/110 | Loss: 0.08\n","Epoch: 008/025 | Batch 093/110 | Loss: 0.12\n","Epoch: 008/025 | Batch 094/110 | Loss: 0.05\n","Epoch: 008/025 | Batch 095/110 | Loss: 0.06\n","Epoch: 008/025 | Batch 096/110 | Loss: 0.09\n","Epoch: 008/025 | Batch 097/110 | Loss: 0.07\n","Epoch: 008/025 | Batch 098/110 | Loss: 0.08\n","Epoch: 008/025 | Batch 099/110 | Loss: 0.24\n","Epoch: 008/025 | Batch 100/110 | Loss: 0.05\n","Epoch: 008/025 | Batch 101/110 | Loss: 0.03\n","Epoch: 008/025 | Batch 102/110 | Loss: 0.16\n","Epoch: 008/025 | Batch 103/110 | Loss: 0.11\n","Epoch: 008/025 | Batch 104/110 | Loss: 0.08\n","Epoch: 008/025 | Batch 105/110 | Loss: 0.18\n","Epoch: 008/025 | Batch 106/110 | Loss: 0.06\n","Epoch: 008/025 | Batch 107/110 | Loss: 0.16\n","Epoch: 008/025 | Batch 108/110 | Loss: 0.08\n","Epoch: 008/025 | Batch 109/110 | Loss: 0.06\n","Epoch: 009/025 | Batch 000/110 | Loss: 0.18\n","Epoch: 009/025 | Batch 001/110 | Loss: 0.17\n","Epoch: 009/025 | Batch 002/110 | Loss: 0.10\n","Epoch: 009/025 | Batch 003/110 | Loss: 0.08\n","Epoch: 009/025 | Batch 004/110 | Loss: 0.07\n","Epoch: 009/025 | Batch 005/110 | Loss: 0.11\n","Epoch: 009/025 | Batch 006/110 | Loss: 0.04\n","Epoch: 009/025 | Batch 007/110 | Loss: 0.05\n","Epoch: 009/025 | Batch 008/110 | Loss: 0.03\n","Epoch: 009/025 | Batch 009/110 | Loss: 0.05\n","Epoch: 009/025 | Batch 010/110 | Loss: 0.05\n","Epoch: 009/025 | Batch 011/110 | Loss: 0.04\n","Epoch: 009/025 | Batch 012/110 | Loss: 0.06\n","Epoch: 009/025 | Batch 013/110 | Loss: 0.03\n","Epoch: 009/025 | Batch 014/110 | Loss: 0.09\n","Epoch: 009/025 | Batch 015/110 | Loss: 0.09\n","Epoch: 009/025 | Batch 016/110 | Loss: 0.08\n","Epoch: 009/025 | Batch 017/110 | Loss: 0.03\n","Epoch: 009/025 | Batch 018/110 | Loss: 0.03\n","Epoch: 009/025 | Batch 019/110 | Loss: 0.08\n","Epoch: 009/025 | Batch 020/110 | Loss: 0.10\n","Epoch: 009/025 | Batch 021/110 | Loss: 0.16\n","Epoch: 009/025 | Batch 022/110 | Loss: 0.04\n","Epoch: 009/025 | Batch 023/110 | Loss: 0.05\n","Epoch: 009/025 | Batch 024/110 | Loss: 0.20\n","Epoch: 009/025 | Batch 025/110 | Loss: 0.02\n","Epoch: 009/025 | Batch 026/110 | Loss: 0.06\n","Epoch: 009/025 | Batch 027/110 | Loss: 0.07\n","Epoch: 009/025 | Batch 028/110 | Loss: 0.19\n","Epoch: 009/025 | Batch 029/110 | Loss: 0.14\n","Epoch: 009/025 | Batch 030/110 | Loss: 0.12\n","Epoch: 009/025 | Batch 031/110 | Loss: 0.21\n","Epoch: 009/025 | Batch 032/110 | Loss: 0.05\n","Epoch: 009/025 | Batch 033/110 | Loss: 0.17\n","Epoch: 009/025 | Batch 034/110 | Loss: 0.07\n","Epoch: 009/025 | Batch 035/110 | Loss: 0.07\n","Epoch: 009/025 | Batch 036/110 | Loss: 0.03\n","Epoch: 009/025 | Batch 037/110 | Loss: 0.18\n","Epoch: 009/025 | Batch 038/110 | Loss: 0.13\n","Epoch: 009/025 | Batch 039/110 | Loss: 0.23\n","Epoch: 009/025 | Batch 040/110 | Loss: 0.04\n","Epoch: 009/025 | Batch 041/110 | Loss: 0.04\n","Epoch: 009/025 | Batch 042/110 | Loss: 0.05\n","Epoch: 009/025 | Batch 043/110 | Loss: 0.08\n","Epoch: 009/025 | Batch 044/110 | Loss: 0.12\n","Epoch: 009/025 | Batch 045/110 | Loss: 0.04\n","Epoch: 009/025 | Batch 046/110 | Loss: 0.05\n","Epoch: 009/025 | Batch 047/110 | Loss: 0.06\n","Epoch: 009/025 | Batch 048/110 | Loss: 0.14\n","Epoch: 009/025 | Batch 049/110 | Loss: 0.04\n","Epoch: 009/025 | Batch 050/110 | Loss: 0.03\n","Epoch: 009/025 | Batch 051/110 | Loss: 0.05\n","Epoch: 009/025 | Batch 052/110 | Loss: 0.09\n","Epoch: 009/025 | Batch 053/110 | Loss: 0.03\n","Epoch: 009/025 | Batch 054/110 | Loss: 0.18\n","Epoch: 009/025 | Batch 055/110 | Loss: 0.06\n","Epoch: 009/025 | Batch 056/110 | Loss: 0.02\n","Epoch: 009/025 | Batch 057/110 | Loss: 0.06\n","Epoch: 009/025 | Batch 058/110 | Loss: 0.06\n","Epoch: 009/025 | Batch 059/110 | Loss: 0.07\n","Epoch: 009/025 | Batch 060/110 | Loss: 0.02\n","Epoch: 009/025 | Batch 061/110 | Loss: 0.05\n","Epoch: 009/025 | Batch 062/110 | Loss: 0.09\n","Epoch: 009/025 | Batch 063/110 | Loss: 0.06\n","Epoch: 009/025 | Batch 064/110 | Loss: 0.15\n","Epoch: 009/025 | Batch 065/110 | Loss: 0.03\n","Epoch: 009/025 | Batch 066/110 | Loss: 0.05\n","Epoch: 009/025 | Batch 067/110 | Loss: 0.06\n","Epoch: 009/025 | Batch 068/110 | Loss: 0.09\n","Epoch: 009/025 | Batch 069/110 | Loss: 0.05\n","Epoch: 009/025 | Batch 070/110 | Loss: 0.13\n","Epoch: 009/025 | Batch 071/110 | Loss: 0.14\n","Epoch: 009/025 | Batch 072/110 | Loss: 0.20\n","Epoch: 009/025 | Batch 073/110 | Loss: 0.04\n","Epoch: 009/025 | Batch 074/110 | Loss: 0.06\n","Epoch: 009/025 | Batch 075/110 | Loss: 0.06\n","Epoch: 009/025 | Batch 076/110 | Loss: 0.06\n","Epoch: 009/025 | Batch 077/110 | Loss: 0.13\n","Epoch: 009/025 | Batch 078/110 | Loss: 0.06\n","Epoch: 009/025 | Batch 079/110 | Loss: 0.06\n","Epoch: 009/025 | Batch 080/110 | Loss: 0.09\n","Epoch: 009/025 | Batch 081/110 | Loss: 0.06\n","Epoch: 009/025 | Batch 082/110 | Loss: 0.12\n","Epoch: 009/025 | Batch 083/110 | Loss: 0.13\n","Epoch: 009/025 | Batch 084/110 | Loss: 0.16\n","Epoch: 009/025 | Batch 085/110 | Loss: 0.12\n","Epoch: 009/025 | Batch 086/110 | Loss: 0.18\n","Epoch: 009/025 | Batch 087/110 | Loss: 0.06\n","Epoch: 009/025 | Batch 088/110 | Loss: 0.05\n","Epoch: 009/025 | Batch 089/110 | Loss: 0.23\n","Epoch: 009/025 | Batch 090/110 | Loss: 0.20\n","Epoch: 009/025 | Batch 091/110 | Loss: 0.11\n","Epoch: 009/025 | Batch 092/110 | Loss: 0.03\n","Epoch: 009/025 | Batch 093/110 | Loss: 0.18\n","Epoch: 009/025 | Batch 094/110 | Loss: 0.06\n","Epoch: 009/025 | Batch 095/110 | Loss: 0.11\n","Epoch: 009/025 | Batch 096/110 | Loss: 0.05\n","Epoch: 009/025 | Batch 097/110 | Loss: 0.22\n","Epoch: 009/025 | Batch 098/110 | Loss: 0.06\n","Epoch: 009/025 | Batch 099/110 | Loss: 0.04\n","Epoch: 009/025 | Batch 100/110 | Loss: 0.06\n","Epoch: 009/025 | Batch 101/110 | Loss: 0.07\n","Epoch: 009/025 | Batch 102/110 | Loss: 0.06\n","Epoch: 009/025 | Batch 103/110 | Loss: 0.04\n","Epoch: 009/025 | Batch 104/110 | Loss: 0.03\n","Epoch: 009/025 | Batch 105/110 | Loss: 0.11\n","Epoch: 009/025 | Batch 106/110 | Loss: 0.04\n","Epoch: 009/025 | Batch 107/110 | Loss: 0.09\n","Epoch: 009/025 | Batch 108/110 | Loss: 0.04\n","Epoch: 009/025 | Batch 109/110 | Loss: 0.01\n","Epoch: 010/025 | Batch 000/110 | Loss: 0.05\n","Epoch: 010/025 | Batch 001/110 | Loss: 0.13\n","Epoch: 010/025 | Batch 002/110 | Loss: 0.13\n","Epoch: 010/025 | Batch 003/110 | Loss: 0.14\n","Epoch: 010/025 | Batch 004/110 | Loss: 0.05\n","Epoch: 010/025 | Batch 005/110 | Loss: 0.05\n","Epoch: 010/025 | Batch 006/110 | Loss: 0.17\n","Epoch: 010/025 | Batch 007/110 | Loss: 0.05\n","Epoch: 010/025 | Batch 008/110 | Loss: 0.05\n","Epoch: 010/025 | Batch 009/110 | Loss: 0.06\n","Epoch: 010/025 | Batch 010/110 | Loss: 0.07\n","Epoch: 010/025 | Batch 011/110 | Loss: 0.06\n","Epoch: 010/025 | Batch 012/110 | Loss: 0.09\n","Epoch: 010/025 | Batch 013/110 | Loss: 0.03\n","Epoch: 010/025 | Batch 014/110 | Loss: 0.03\n","Epoch: 010/025 | Batch 015/110 | Loss: 0.06\n","Epoch: 010/025 | Batch 016/110 | Loss: 0.03\n","Epoch: 010/025 | Batch 017/110 | Loss: 0.06\n","Epoch: 010/025 | Batch 018/110 | Loss: 0.06\n","Epoch: 010/025 | Batch 019/110 | Loss: 0.04\n","Epoch: 010/025 | Batch 020/110 | Loss: 0.01\n","Epoch: 010/025 | Batch 021/110 | Loss: 0.03\n","Epoch: 010/025 | Batch 022/110 | Loss: 0.27\n","Epoch: 010/025 | Batch 023/110 | Loss: 0.03\n","Epoch: 010/025 | Batch 024/110 | Loss: 0.10\n","Epoch: 010/025 | Batch 025/110 | Loss: 0.05\n","Epoch: 010/025 | Batch 026/110 | Loss: 0.02\n","Epoch: 010/025 | Batch 027/110 | Loss: 0.08\n","Epoch: 010/025 | Batch 028/110 | Loss: 0.13\n","Epoch: 010/025 | Batch 029/110 | Loss: 0.08\n","Epoch: 010/025 | Batch 030/110 | Loss: 0.05\n","Epoch: 010/025 | Batch 031/110 | Loss: 0.15\n","Epoch: 010/025 | Batch 032/110 | Loss: 0.05\n","Epoch: 010/025 | Batch 033/110 | Loss: 0.08\n","Epoch: 010/025 | Batch 034/110 | Loss: 0.09\n","Epoch: 010/025 | Batch 035/110 | Loss: 0.11\n","Epoch: 010/025 | Batch 036/110 | Loss: 0.03\n","Epoch: 010/025 | Batch 037/110 | Loss: 0.14\n","Epoch: 010/025 | Batch 038/110 | Loss: 0.23\n","Epoch: 010/025 | Batch 039/110 | Loss: 0.11\n","Epoch: 010/025 | Batch 040/110 | Loss: 0.07\n","Epoch: 010/025 | Batch 041/110 | Loss: 0.03\n","Epoch: 010/025 | Batch 042/110 | Loss: 0.10\n","Epoch: 010/025 | Batch 043/110 | Loss: 0.06\n","Epoch: 010/025 | Batch 044/110 | Loss: 0.02\n","Epoch: 010/025 | Batch 045/110 | Loss: 0.06\n","Epoch: 010/025 | Batch 046/110 | Loss: 0.33\n","Epoch: 010/025 | Batch 047/110 | Loss: 0.16\n","Epoch: 010/025 | Batch 048/110 | Loss: 0.06\n","Epoch: 010/025 | Batch 049/110 | Loss: 0.04\n","Epoch: 010/025 | Batch 050/110 | Loss: 0.20\n","Epoch: 010/025 | Batch 051/110 | Loss: 0.09\n","Epoch: 010/025 | Batch 052/110 | Loss: 0.04\n","Epoch: 010/025 | Batch 053/110 | Loss: 0.05\n","Epoch: 010/025 | Batch 054/110 | Loss: 0.17\n","Epoch: 010/025 | Batch 055/110 | Loss: 0.02\n","Epoch: 010/025 | Batch 056/110 | Loss: 0.03\n","Epoch: 010/025 | Batch 057/110 | Loss: 0.13\n","Epoch: 010/025 | Batch 058/110 | Loss: 0.15\n","Epoch: 010/025 | Batch 059/110 | Loss: 0.03\n","Epoch: 010/025 | Batch 060/110 | Loss: 0.10\n","Epoch: 010/025 | Batch 061/110 | Loss: 0.09\n","Epoch: 010/025 | Batch 062/110 | Loss: 0.23\n","Epoch: 010/025 | Batch 063/110 | Loss: 0.04\n","Epoch: 010/025 | Batch 064/110 | Loss: 0.06\n","Epoch: 010/025 | Batch 065/110 | Loss: 0.08\n","Epoch: 010/025 | Batch 066/110 | Loss: 0.07\n","Epoch: 010/025 | Batch 067/110 | Loss: 0.05\n","Epoch: 010/025 | Batch 068/110 | Loss: 0.13\n","Epoch: 010/025 | Batch 069/110 | Loss: 0.04\n","Epoch: 010/025 | Batch 070/110 | Loss: 0.07\n","Epoch: 010/025 | Batch 071/110 | Loss: 0.06\n","Epoch: 010/025 | Batch 072/110 | Loss: 0.03\n","Epoch: 010/025 | Batch 073/110 | Loss: 0.09\n","Epoch: 010/025 | Batch 074/110 | Loss: 0.04\n","Epoch: 010/025 | Batch 075/110 | Loss: 0.04\n","Epoch: 010/025 | Batch 076/110 | Loss: 0.08\n","Epoch: 010/025 | Batch 077/110 | Loss: 0.09\n","Epoch: 010/025 | Batch 078/110 | Loss: 0.12\n","Epoch: 010/025 | Batch 079/110 | Loss: 0.15\n","Epoch: 010/025 | Batch 080/110 | Loss: 0.06\n","Epoch: 010/025 | Batch 081/110 | Loss: 0.11\n","Epoch: 010/025 | Batch 082/110 | Loss: 0.06\n","Epoch: 010/025 | Batch 083/110 | Loss: 0.02\n","Epoch: 010/025 | Batch 084/110 | Loss: 0.06\n","Epoch: 010/025 | Batch 085/110 | Loss: 0.19\n","Epoch: 010/025 | Batch 086/110 | Loss: 0.07\n","Epoch: 010/025 | Batch 087/110 | Loss: 0.06\n","Epoch: 010/025 | Batch 088/110 | Loss: 0.12\n","Epoch: 010/025 | Batch 089/110 | Loss: 0.03\n","Epoch: 010/025 | Batch 090/110 | Loss: 0.08\n","Epoch: 010/025 | Batch 091/110 | Loss: 0.03\n","Epoch: 010/025 | Batch 092/110 | Loss: 0.09\n","Epoch: 010/025 | Batch 093/110 | Loss: 0.08\n","Epoch: 010/025 | Batch 094/110 | Loss: 0.03\n","Epoch: 010/025 | Batch 095/110 | Loss: 0.06\n","Epoch: 010/025 | Batch 096/110 | Loss: 0.05\n","Epoch: 010/025 | Batch 097/110 | Loss: 0.06\n","Epoch: 010/025 | Batch 098/110 | Loss: 0.05\n","Epoch: 010/025 | Batch 099/110 | Loss: 0.05\n","Epoch: 010/025 | Batch 100/110 | Loss: 0.03\n","Epoch: 010/025 | Batch 101/110 | Loss: 0.05\n","Epoch: 010/025 | Batch 102/110 | Loss: 0.12\n","Epoch: 010/025 | Batch 103/110 | Loss: 0.08\n","Epoch: 010/025 | Batch 104/110 | Loss: 0.08\n","Epoch: 010/025 | Batch 105/110 | Loss: 0.04\n","Epoch: 010/025 | Batch 106/110 | Loss: 0.03\n","Epoch: 010/025 | Batch 107/110 | Loss: 0.09\n","Epoch: 010/025 | Batch 108/110 | Loss: 0.18\n","Epoch: 010/025 | Batch 109/110 | Loss: 0.07\n","Epoch: 011/025 | Batch 000/110 | Loss: 0.03\n","Epoch: 011/025 | Batch 001/110 | Loss: 0.04\n","Epoch: 011/025 | Batch 002/110 | Loss: 0.15\n","Epoch: 011/025 | Batch 003/110 | Loss: 0.17\n","Epoch: 011/025 | Batch 004/110 | Loss: 0.06\n","Epoch: 011/025 | Batch 005/110 | Loss: 0.17\n","Epoch: 011/025 | Batch 006/110 | Loss: 0.03\n","Epoch: 011/025 | Batch 007/110 | Loss: 0.09\n","Epoch: 011/025 | Batch 008/110 | Loss: 0.11\n","Epoch: 011/025 | Batch 009/110 | Loss: 0.04\n","Epoch: 011/025 | Batch 010/110 | Loss: 0.04\n","Epoch: 011/025 | Batch 011/110 | Loss: 0.05\n","Epoch: 011/025 | Batch 012/110 | Loss: 0.11\n","Epoch: 011/025 | Batch 013/110 | Loss: 0.03\n","Epoch: 011/025 | Batch 014/110 | Loss: 0.04\n","Epoch: 011/025 | Batch 015/110 | Loss: 0.14\n","Epoch: 011/025 | Batch 016/110 | Loss: 0.10\n","Epoch: 011/025 | Batch 017/110 | Loss: 0.10\n","Epoch: 011/025 | Batch 018/110 | Loss: 0.09\n","Epoch: 011/025 | Batch 019/110 | Loss: 0.22\n","Epoch: 011/025 | Batch 020/110 | Loss: 0.06\n","Epoch: 011/025 | Batch 021/110 | Loss: 0.22\n","Epoch: 011/025 | Batch 022/110 | Loss: 0.13\n","Epoch: 011/025 | Batch 023/110 | Loss: 0.04\n","Epoch: 011/025 | Batch 024/110 | Loss: 0.12\n","Epoch: 011/025 | Batch 025/110 | Loss: 0.07\n","Epoch: 011/025 | Batch 026/110 | Loss: 0.10\n","Epoch: 011/025 | Batch 027/110 | Loss: 0.14\n","Epoch: 011/025 | Batch 028/110 | Loss: 0.03\n","Epoch: 011/025 | Batch 029/110 | Loss: 0.04\n","Epoch: 011/025 | Batch 030/110 | Loss: 0.05\n","Epoch: 011/025 | Batch 031/110 | Loss: 0.13\n","Epoch: 011/025 | Batch 032/110 | Loss: 0.13\n","Epoch: 011/025 | Batch 033/110 | Loss: 0.04\n","Epoch: 011/025 | Batch 034/110 | Loss: 0.09\n","Epoch: 011/025 | Batch 035/110 | Loss: 0.07\n","Epoch: 011/025 | Batch 036/110 | Loss: 0.03\n","Epoch: 011/025 | Batch 037/110 | Loss: 0.06\n","Epoch: 011/025 | Batch 038/110 | Loss: 0.04\n","Epoch: 011/025 | Batch 039/110 | Loss: 0.07\n","Epoch: 011/025 | Batch 040/110 | Loss: 0.07\n","Epoch: 011/025 | Batch 041/110 | Loss: 0.04\n","Epoch: 011/025 | Batch 042/110 | Loss: 0.03\n","Epoch: 011/025 | Batch 043/110 | Loss: 0.03\n","Epoch: 011/025 | Batch 044/110 | Loss: 0.09\n","Epoch: 011/025 | Batch 045/110 | Loss: 0.04\n","Epoch: 011/025 | Batch 046/110 | Loss: 0.06\n","Epoch: 011/025 | Batch 047/110 | Loss: 0.13\n","Epoch: 011/025 | Batch 048/110 | Loss: 0.03\n","Epoch: 011/025 | Batch 049/110 | Loss: 0.24\n","Epoch: 011/025 | Batch 050/110 | Loss: 0.10\n","Epoch: 011/025 | Batch 051/110 | Loss: 0.07\n","Epoch: 011/025 | Batch 052/110 | Loss: 0.16\n","Epoch: 011/025 | Batch 053/110 | Loss: 0.13\n","Epoch: 011/025 | Batch 054/110 | Loss: 0.05\n","Epoch: 011/025 | Batch 055/110 | Loss: 0.05\n","Epoch: 011/025 | Batch 056/110 | Loss: 0.05\n","Epoch: 011/025 | Batch 057/110 | Loss: 0.04\n","Epoch: 011/025 | Batch 058/110 | Loss: 0.02\n","Epoch: 011/025 | Batch 059/110 | Loss: 0.13\n","Epoch: 011/025 | Batch 060/110 | Loss: 0.05\n","Epoch: 011/025 | Batch 061/110 | Loss: 0.07\n","Epoch: 011/025 | Batch 062/110 | Loss: 0.05\n","Epoch: 011/025 | Batch 063/110 | Loss: 0.13\n","Epoch: 011/025 | Batch 064/110 | Loss: 0.05\n","Epoch: 011/025 | Batch 065/110 | Loss: 0.05\n","Epoch: 011/025 | Batch 066/110 | Loss: 0.02\n","Epoch: 011/025 | Batch 067/110 | Loss: 0.13\n","Epoch: 011/025 | Batch 068/110 | Loss: 0.06\n","Epoch: 011/025 | Batch 069/110 | Loss: 0.06\n","Epoch: 011/025 | Batch 070/110 | Loss: 0.04\n","Epoch: 011/025 | Batch 071/110 | Loss: 0.09\n","Epoch: 011/025 | Batch 072/110 | Loss: 0.18\n","Epoch: 011/025 | Batch 073/110 | Loss: 0.02\n","Epoch: 011/025 | Batch 074/110 | Loss: 0.05\n","Epoch: 011/025 | Batch 075/110 | Loss: 0.07\n","Epoch: 011/025 | Batch 076/110 | Loss: 0.13\n","Epoch: 011/025 | Batch 077/110 | Loss: 0.05\n","Epoch: 011/025 | Batch 078/110 | Loss: 0.02\n","Epoch: 011/025 | Batch 079/110 | Loss: 0.09\n","Epoch: 011/025 | Batch 080/110 | Loss: 0.07\n","Epoch: 011/025 | Batch 081/110 | Loss: 0.01\n","Epoch: 011/025 | Batch 082/110 | Loss: 0.06\n","Epoch: 011/025 | Batch 083/110 | Loss: 0.02\n","Epoch: 011/025 | Batch 084/110 | Loss: 0.03\n","Epoch: 011/025 | Batch 085/110 | Loss: 0.05\n","Epoch: 011/025 | Batch 086/110 | Loss: 0.13\n","Epoch: 011/025 | Batch 087/110 | Loss: 0.03\n","Epoch: 011/025 | Batch 088/110 | Loss: 0.13\n","Epoch: 011/025 | Batch 089/110 | Loss: 0.04\n","Epoch: 011/025 | Batch 090/110 | Loss: 0.08\n","Epoch: 011/025 | Batch 091/110 | Loss: 0.08\n","Epoch: 011/025 | Batch 092/110 | Loss: 0.04\n","Epoch: 011/025 | Batch 093/110 | Loss: 0.07\n","Epoch: 011/025 | Batch 094/110 | Loss: 0.13\n","Epoch: 011/025 | Batch 095/110 | Loss: 0.11\n","Epoch: 011/025 | Batch 096/110 | Loss: 0.03\n","Epoch: 011/025 | Batch 097/110 | Loss: 0.14\n","Epoch: 011/025 | Batch 098/110 | Loss: 0.05\n","Epoch: 011/025 | Batch 099/110 | Loss: 0.06\n","Epoch: 011/025 | Batch 100/110 | Loss: 0.03\n","Epoch: 011/025 | Batch 101/110 | Loss: 0.06\n","Epoch: 011/025 | Batch 102/110 | Loss: 0.18\n","Epoch: 011/025 | Batch 103/110 | Loss: 0.06\n","Epoch: 011/025 | Batch 104/110 | Loss: 0.06\n","Epoch: 011/025 | Batch 105/110 | Loss: 0.07\n","Epoch: 011/025 | Batch 106/110 | Loss: 0.05\n","Epoch: 011/025 | Batch 107/110 | Loss: 0.03\n","Epoch: 011/025 | Batch 108/110 | Loss: 0.05\n","Epoch: 011/025 | Batch 109/110 | Loss: 0.08\n","Epoch: 012/025 | Batch 000/110 | Loss: 0.06\n","Epoch: 012/025 | Batch 001/110 | Loss: 0.06\n","Epoch: 012/025 | Batch 002/110 | Loss: 0.06\n","Epoch: 012/025 | Batch 003/110 | Loss: 0.13\n","Epoch: 012/025 | Batch 004/110 | Loss: 0.03\n","Epoch: 012/025 | Batch 005/110 | Loss: 0.05\n","Epoch: 012/025 | Batch 006/110 | Loss: 0.11\n","Epoch: 012/025 | Batch 007/110 | Loss: 0.07\n","Epoch: 012/025 | Batch 008/110 | Loss: 0.07\n","Epoch: 012/025 | Batch 009/110 | Loss: 0.03\n","Epoch: 012/025 | Batch 010/110 | Loss: 0.19\n","Epoch: 012/025 | Batch 011/110 | Loss: 0.02\n","Epoch: 012/025 | Batch 012/110 | Loss: 0.07\n","Epoch: 012/025 | Batch 013/110 | Loss: 0.05\n","Epoch: 012/025 | Batch 014/110 | Loss: 0.07\n","Epoch: 012/025 | Batch 015/110 | Loss: 0.02\n","Epoch: 012/025 | Batch 016/110 | Loss: 0.12\n","Epoch: 012/025 | Batch 017/110 | Loss: 0.04\n","Epoch: 012/025 | Batch 018/110 | Loss: 0.08\n","Epoch: 012/025 | Batch 019/110 | Loss: 0.13\n","Epoch: 012/025 | Batch 020/110 | Loss: 0.15\n","Epoch: 012/025 | Batch 021/110 | Loss: 0.06\n","Epoch: 012/025 | Batch 022/110 | Loss: 0.10\n","Epoch: 012/025 | Batch 023/110 | Loss: 0.15\n","Epoch: 012/025 | Batch 024/110 | Loss: 0.07\n","Epoch: 012/025 | Batch 025/110 | Loss: 0.06\n","Epoch: 012/025 | Batch 026/110 | Loss: 0.03\n","Epoch: 012/025 | Batch 027/110 | Loss: 0.06\n","Epoch: 012/025 | Batch 028/110 | Loss: 0.17\n","Epoch: 012/025 | Batch 029/110 | Loss: 0.26\n","Epoch: 012/025 | Batch 030/110 | Loss: 0.13\n","Epoch: 012/025 | Batch 031/110 | Loss: 0.05\n","Epoch: 012/025 | Batch 032/110 | Loss: 0.03\n","Epoch: 012/025 | Batch 033/110 | Loss: 0.02\n","Epoch: 012/025 | Batch 034/110 | Loss: 0.03\n","Epoch: 012/025 | Batch 035/110 | Loss: 0.13\n","Epoch: 012/025 | Batch 036/110 | Loss: 0.04\n","Epoch: 012/025 | Batch 037/110 | Loss: 0.07\n","Epoch: 012/025 | Batch 038/110 | Loss: 0.06\n","Epoch: 012/025 | Batch 039/110 | Loss: 0.08\n","Epoch: 012/025 | Batch 040/110 | Loss: 0.05\n","Epoch: 012/025 | Batch 041/110 | Loss: 0.04\n","Epoch: 012/025 | Batch 042/110 | Loss: 0.03\n","Epoch: 012/025 | Batch 043/110 | Loss: 0.05\n","Epoch: 012/025 | Batch 044/110 | Loss: 0.15\n","Epoch: 012/025 | Batch 045/110 | Loss: 0.08\n","Epoch: 012/025 | Batch 046/110 | Loss: 0.16\n","Epoch: 012/025 | Batch 047/110 | Loss: 0.07\n","Epoch: 012/025 | Batch 048/110 | Loss: 0.03\n","Epoch: 012/025 | Batch 049/110 | Loss: 0.11\n","Epoch: 012/025 | Batch 050/110 | Loss: 0.06\n","Epoch: 012/025 | Batch 051/110 | Loss: 0.07\n","Epoch: 012/025 | Batch 052/110 | Loss: 0.04\n","Epoch: 012/025 | Batch 053/110 | Loss: 0.02\n","Epoch: 012/025 | Batch 054/110 | Loss: 0.04\n","Epoch: 012/025 | Batch 055/110 | Loss: 0.04\n","Epoch: 012/025 | Batch 056/110 | Loss: 0.03\n","Epoch: 012/025 | Batch 057/110 | Loss: 0.09\n","Epoch: 012/025 | Batch 058/110 | Loss: 0.07\n","Epoch: 012/025 | Batch 059/110 | Loss: 0.10\n","Epoch: 012/025 | Batch 060/110 | Loss: 0.04\n","Epoch: 012/025 | Batch 061/110 | Loss: 0.11\n","Epoch: 012/025 | Batch 062/110 | Loss: 0.03\n","Epoch: 012/025 | Batch 063/110 | Loss: 0.16\n","Epoch: 012/025 | Batch 064/110 | Loss: 0.02\n","Epoch: 012/025 | Batch 065/110 | Loss: 0.08\n","Epoch: 012/025 | Batch 066/110 | Loss: 0.06\n","Epoch: 012/025 | Batch 067/110 | Loss: 0.05\n","Epoch: 012/025 | Batch 068/110 | Loss: 0.06\n","Epoch: 012/025 | Batch 069/110 | Loss: 0.14\n","Epoch: 012/025 | Batch 070/110 | Loss: 0.13\n","Epoch: 012/025 | Batch 071/110 | Loss: 0.02\n","Epoch: 012/025 | Batch 072/110 | Loss: 0.06\n","Epoch: 012/025 | Batch 073/110 | Loss: 0.05\n","Epoch: 012/025 | Batch 074/110 | Loss: 0.04\n","Epoch: 012/025 | Batch 075/110 | Loss: 0.05\n","Epoch: 012/025 | Batch 076/110 | Loss: 0.06\n","Epoch: 012/025 | Batch 077/110 | Loss: 0.10\n","Epoch: 012/025 | Batch 078/110 | Loss: 0.05\n","Epoch: 012/025 | Batch 079/110 | Loss: 0.15\n","Epoch: 012/025 | Batch 080/110 | Loss: 0.19\n","Epoch: 012/025 | Batch 081/110 | Loss: 0.03\n","Epoch: 012/025 | Batch 082/110 | Loss: 0.10\n","Epoch: 012/025 | Batch 083/110 | Loss: 0.03\n","Epoch: 012/025 | Batch 084/110 | Loss: 0.03\n","Epoch: 012/025 | Batch 085/110 | Loss: 0.09\n","Epoch: 012/025 | Batch 086/110 | Loss: 0.04\n","Epoch: 012/025 | Batch 087/110 | Loss: 0.05\n","Epoch: 012/025 | Batch 088/110 | Loss: 0.10\n","Epoch: 012/025 | Batch 089/110 | Loss: 0.01\n","Epoch: 012/025 | Batch 090/110 | Loss: 0.15\n","Epoch: 012/025 | Batch 091/110 | Loss: 0.08\n","Epoch: 012/025 | Batch 092/110 | Loss: 0.15\n","Epoch: 012/025 | Batch 093/110 | Loss: 0.14\n","Epoch: 012/025 | Batch 094/110 | Loss: 0.05\n","Epoch: 012/025 | Batch 095/110 | Loss: 0.14\n","Epoch: 012/025 | Batch 096/110 | Loss: 0.11\n","Epoch: 012/025 | Batch 097/110 | Loss: 0.03\n","Epoch: 012/025 | Batch 098/110 | Loss: 0.04\n","Epoch: 012/025 | Batch 099/110 | Loss: 0.05\n","Epoch: 012/025 | Batch 100/110 | Loss: 0.02\n","Epoch: 012/025 | Batch 101/110 | Loss: 0.05\n","Epoch: 012/025 | Batch 102/110 | Loss: 0.02\n","Epoch: 012/025 | Batch 103/110 | Loss: 0.04\n","Epoch: 012/025 | Batch 104/110 | Loss: 0.03\n","Epoch: 012/025 | Batch 105/110 | Loss: 0.10\n","Epoch: 012/025 | Batch 106/110 | Loss: 0.07\n","Epoch: 012/025 | Batch 107/110 | Loss: 0.03\n","Epoch: 012/025 | Batch 108/110 | Loss: 0.10\n","Epoch: 012/025 | Batch 109/110 | Loss: 0.03\n","Epoch: 013/025 | Batch 000/110 | Loss: 0.02\n","Epoch: 013/025 | Batch 001/110 | Loss: 0.03\n","Epoch: 013/025 | Batch 002/110 | Loss: 0.06\n","Epoch: 013/025 | Batch 003/110 | Loss: 0.11\n","Epoch: 013/025 | Batch 004/110 | Loss: 0.03\n","Epoch: 013/025 | Batch 005/110 | Loss: 0.03\n","Epoch: 013/025 | Batch 006/110 | Loss: 0.05\n","Epoch: 013/025 | Batch 007/110 | Loss: 0.14\n","Epoch: 013/025 | Batch 008/110 | Loss: 0.04\n","Epoch: 013/025 | Batch 009/110 | Loss: 0.07\n","Epoch: 013/025 | Batch 010/110 | Loss: 0.05\n","Epoch: 013/025 | Batch 011/110 | Loss: 0.05\n","Epoch: 013/025 | Batch 012/110 | Loss: 0.03\n","Epoch: 013/025 | Batch 013/110 | Loss: 0.05\n","Epoch: 013/025 | Batch 014/110 | Loss: 0.09\n","Epoch: 013/025 | Batch 015/110 | Loss: 0.16\n","Epoch: 013/025 | Batch 016/110 | Loss: 0.09\n","Epoch: 013/025 | Batch 017/110 | Loss: 0.17\n","Epoch: 013/025 | Batch 018/110 | Loss: 0.18\n","Epoch: 013/025 | Batch 019/110 | Loss: 0.02\n","Epoch: 013/025 | Batch 020/110 | Loss: 0.08\n","Epoch: 013/025 | Batch 021/110 | Loss: 0.03\n","Epoch: 013/025 | Batch 022/110 | Loss: 0.07\n","Epoch: 013/025 | Batch 023/110 | Loss: 0.05\n","Epoch: 013/025 | Batch 024/110 | Loss: 0.12\n","Epoch: 013/025 | Batch 025/110 | Loss: 0.03\n","Epoch: 013/025 | Batch 026/110 | Loss: 0.08\n","Epoch: 013/025 | Batch 027/110 | Loss: 0.18\n","Epoch: 013/025 | Batch 028/110 | Loss: 0.04\n","Epoch: 013/025 | Batch 029/110 | Loss: 0.19\n","Epoch: 013/025 | Batch 030/110 | Loss: 0.01\n","Epoch: 013/025 | Batch 031/110 | Loss: 0.15\n","Epoch: 013/025 | Batch 032/110 | Loss: 0.19\n","Epoch: 013/025 | Batch 033/110 | Loss: 0.07\n","Epoch: 013/025 | Batch 034/110 | Loss: 0.05\n","Epoch: 013/025 | Batch 035/110 | Loss: 0.10\n","Epoch: 013/025 | Batch 036/110 | Loss: 0.03\n","Epoch: 013/025 | Batch 037/110 | Loss: 0.05\n","Epoch: 013/025 | Batch 038/110 | Loss: 0.11\n","Epoch: 013/025 | Batch 039/110 | Loss: 0.02\n","Epoch: 013/025 | Batch 040/110 | Loss: 0.04\n","Epoch: 013/025 | Batch 041/110 | Loss: 0.02\n","Epoch: 013/025 | Batch 042/110 | Loss: 0.05\n","Epoch: 013/025 | Batch 043/110 | Loss: 0.05\n","Epoch: 013/025 | Batch 044/110 | Loss: 0.02\n","Epoch: 013/025 | Batch 045/110 | Loss: 0.02\n","Epoch: 013/025 | Batch 046/110 | Loss: 0.06\n","Epoch: 013/025 | Batch 047/110 | Loss: 0.08\n","Epoch: 013/025 | Batch 048/110 | Loss: 0.05\n","Epoch: 013/025 | Batch 049/110 | Loss: 0.06\n","Epoch: 013/025 | Batch 050/110 | Loss: 0.21\n","Epoch: 013/025 | Batch 051/110 | Loss: 0.21\n","Epoch: 013/025 | Batch 052/110 | Loss: 0.02\n","Epoch: 013/025 | Batch 053/110 | Loss: 0.02\n","Epoch: 013/025 | Batch 054/110 | Loss: 0.02\n","Epoch: 013/025 | Batch 055/110 | Loss: 0.02\n","Epoch: 013/025 | Batch 056/110 | Loss: 0.04\n","Epoch: 013/025 | Batch 057/110 | Loss: 0.09\n","Epoch: 013/025 | Batch 058/110 | Loss: 0.04\n","Epoch: 013/025 | Batch 059/110 | Loss: 0.05\n","Epoch: 013/025 | Batch 060/110 | Loss: 0.10\n","Epoch: 013/025 | Batch 061/110 | Loss: 0.12\n","Epoch: 013/025 | Batch 062/110 | Loss: 0.01\n","Epoch: 013/025 | Batch 063/110 | Loss: 0.03\n","Epoch: 013/025 | Batch 064/110 | Loss: 0.03\n","Epoch: 013/025 | Batch 065/110 | Loss: 0.06\n","Epoch: 013/025 | Batch 066/110 | Loss: 0.02\n","Epoch: 013/025 | Batch 067/110 | Loss: 0.11\n","Epoch: 013/025 | Batch 068/110 | Loss: 0.04\n","Epoch: 013/025 | Batch 069/110 | Loss: 0.02\n","Epoch: 013/025 | Batch 070/110 | Loss: 0.03\n","Epoch: 013/025 | Batch 071/110 | Loss: 0.05\n","Epoch: 013/025 | Batch 072/110 | Loss: 0.04\n","Epoch: 013/025 | Batch 073/110 | Loss: 0.03\n","Epoch: 013/025 | Batch 074/110 | Loss: 0.05\n","Epoch: 013/025 | Batch 075/110 | Loss: 0.07\n","Epoch: 013/025 | Batch 076/110 | Loss: 0.12\n","Epoch: 013/025 | Batch 077/110 | Loss: 0.02\n","Epoch: 013/025 | Batch 078/110 | Loss: 0.11\n","Epoch: 013/025 | Batch 079/110 | Loss: 0.04\n","Epoch: 013/025 | Batch 080/110 | Loss: 0.15\n","Epoch: 013/025 | Batch 081/110 | Loss: 0.02\n","Epoch: 013/025 | Batch 082/110 | Loss: 0.16\n","Epoch: 013/025 | Batch 083/110 | Loss: 0.10\n","Epoch: 013/025 | Batch 084/110 | Loss: 0.02\n","Epoch: 013/025 | Batch 085/110 | Loss: 0.04\n","Epoch: 013/025 | Batch 086/110 | Loss: 0.06\n","Epoch: 013/025 | Batch 087/110 | Loss: 0.07\n","Epoch: 013/025 | Batch 088/110 | Loss: 0.03\n","Epoch: 013/025 | Batch 089/110 | Loss: 0.03\n","Epoch: 013/025 | Batch 090/110 | Loss: 0.12\n","Epoch: 013/025 | Batch 091/110 | Loss: 0.02\n","Epoch: 013/025 | Batch 092/110 | Loss: 0.06\n","Epoch: 013/025 | Batch 093/110 | Loss: 0.13\n","Epoch: 013/025 | Batch 094/110 | Loss: 0.06\n","Epoch: 013/025 | Batch 095/110 | Loss: 0.08\n","Epoch: 013/025 | Batch 096/110 | Loss: 0.05\n","Epoch: 013/025 | Batch 097/110 | Loss: 0.17\n","Epoch: 013/025 | Batch 098/110 | Loss: 0.14\n","Epoch: 013/025 | Batch 099/110 | Loss: 0.06\n","Epoch: 013/025 | Batch 100/110 | Loss: 0.03\n","Epoch: 013/025 | Batch 101/110 | Loss: 0.04\n","Epoch: 013/025 | Batch 102/110 | Loss: 0.03\n","Epoch: 013/025 | Batch 103/110 | Loss: 0.14\n","Epoch: 013/025 | Batch 104/110 | Loss: 0.18\n","Epoch: 013/025 | Batch 105/110 | Loss: 0.08\n","Epoch: 013/025 | Batch 106/110 | Loss: 0.02\n","Epoch: 013/025 | Batch 107/110 | Loss: 0.17\n","Epoch: 013/025 | Batch 108/110 | Loss: 0.03\n","Epoch: 013/025 | Batch 109/110 | Loss: 0.13\n","Epoch: 014/025 | Batch 000/110 | Loss: 0.04\n","Epoch: 014/025 | Batch 001/110 | Loss: 0.07\n","Epoch: 014/025 | Batch 002/110 | Loss: 0.07\n","Epoch: 014/025 | Batch 003/110 | Loss: 0.07\n","Epoch: 014/025 | Batch 004/110 | Loss: 0.05\n","Epoch: 014/025 | Batch 005/110 | Loss: 0.03\n","Epoch: 014/025 | Batch 006/110 | Loss: 0.03\n","Epoch: 014/025 | Batch 007/110 | Loss: 0.03\n","Epoch: 014/025 | Batch 008/110 | Loss: 0.02\n","Epoch: 014/025 | Batch 009/110 | Loss: 0.10\n","Epoch: 014/025 | Batch 010/110 | Loss: 0.05\n","Epoch: 014/025 | Batch 011/110 | Loss: 0.03\n","Epoch: 014/025 | Batch 012/110 | Loss: 0.14\n","Epoch: 014/025 | Batch 013/110 | Loss: 0.15\n","Epoch: 014/025 | Batch 014/110 | Loss: 0.03\n","Epoch: 014/025 | Batch 015/110 | Loss: 0.04\n","Epoch: 014/025 | Batch 016/110 | Loss: 0.05\n","Epoch: 014/025 | Batch 017/110 | Loss: 0.03\n","Epoch: 014/025 | Batch 018/110 | Loss: 0.02\n","Epoch: 014/025 | Batch 019/110 | Loss: 0.02\n","Epoch: 014/025 | Batch 020/110 | Loss: 0.07\n","Epoch: 014/025 | Batch 021/110 | Loss: 0.07\n","Epoch: 014/025 | Batch 022/110 | Loss: 0.04\n","Epoch: 014/025 | Batch 023/110 | Loss: 0.04\n","Epoch: 014/025 | Batch 024/110 | Loss: 0.09\n","Epoch: 014/025 | Batch 025/110 | Loss: 0.02\n","Epoch: 014/025 | Batch 026/110 | Loss: 0.05\n","Epoch: 014/025 | Batch 027/110 | Loss: 0.17\n","Epoch: 014/025 | Batch 028/110 | Loss: 0.07\n","Epoch: 014/025 | Batch 029/110 | Loss: 0.04\n","Epoch: 014/025 | Batch 030/110 | Loss: 0.03\n","Epoch: 014/025 | Batch 031/110 | Loss: 0.12\n","Epoch: 014/025 | Batch 032/110 | Loss: 0.02\n","Epoch: 014/025 | Batch 033/110 | Loss: 0.16\n","Epoch: 014/025 | Batch 034/110 | Loss: 0.07\n","Epoch: 014/025 | Batch 035/110 | Loss: 0.10\n","Epoch: 014/025 | Batch 036/110 | Loss: 0.06\n","Epoch: 014/025 | Batch 037/110 | Loss: 0.03\n","Epoch: 014/025 | Batch 038/110 | Loss: 0.09\n","Epoch: 014/025 | Batch 039/110 | Loss: 0.13\n","Epoch: 014/025 | Batch 040/110 | Loss: 0.08\n","Epoch: 014/025 | Batch 041/110 | Loss: 0.09\n","Epoch: 014/025 | Batch 042/110 | Loss: 0.05\n","Epoch: 014/025 | Batch 043/110 | Loss: 0.08\n","Epoch: 014/025 | Batch 044/110 | Loss: 0.06\n","Epoch: 014/025 | Batch 045/110 | Loss: 0.08\n","Epoch: 014/025 | Batch 046/110 | Loss: 0.30\n","Epoch: 014/025 | Batch 047/110 | Loss: 0.05\n","Epoch: 014/025 | Batch 048/110 | Loss: 0.08\n","Epoch: 014/025 | Batch 049/110 | Loss: 0.13\n","Epoch: 014/025 | Batch 050/110 | Loss: 0.08\n","Epoch: 014/025 | Batch 051/110 | Loss: 0.02\n","Epoch: 014/025 | Batch 052/110 | Loss: 0.16\n","Epoch: 014/025 | Batch 053/110 | Loss: 0.04\n","Epoch: 014/025 | Batch 054/110 | Loss: 0.03\n","Epoch: 014/025 | Batch 055/110 | Loss: 0.13\n","Epoch: 014/025 | Batch 056/110 | Loss: 0.04\n","Epoch: 014/025 | Batch 057/110 | Loss: 0.07\n","Epoch: 014/025 | Batch 058/110 | Loss: 0.04\n","Epoch: 014/025 | Batch 059/110 | Loss: 0.05\n","Epoch: 014/025 | Batch 060/110 | Loss: 0.02\n","Epoch: 014/025 | Batch 061/110 | Loss: 0.02\n","Epoch: 014/025 | Batch 062/110 | Loss: 0.18\n","Epoch: 014/025 | Batch 063/110 | Loss: 0.10\n","Epoch: 014/025 | Batch 064/110 | Loss: 0.06\n","Epoch: 014/025 | Batch 065/110 | Loss: 0.19\n","Epoch: 014/025 | Batch 066/110 | Loss: 0.04\n","Epoch: 014/025 | Batch 067/110 | Loss: 0.11\n","Epoch: 014/025 | Batch 068/110 | Loss: 0.11\n","Epoch: 014/025 | Batch 069/110 | Loss: 0.03\n","Epoch: 014/025 | Batch 070/110 | Loss: 0.01\n","Epoch: 014/025 | Batch 071/110 | Loss: 0.04\n","Epoch: 014/025 | Batch 072/110 | Loss: 0.03\n","Epoch: 014/025 | Batch 073/110 | Loss: 0.02\n","Epoch: 014/025 | Batch 074/110 | Loss: 0.02\n","Epoch: 014/025 | Batch 075/110 | Loss: 0.04\n","Epoch: 014/025 | Batch 076/110 | Loss: 0.06\n","Epoch: 014/025 | Batch 077/110 | Loss: 0.01\n","Epoch: 014/025 | Batch 078/110 | Loss: 0.17\n","Epoch: 014/025 | Batch 079/110 | Loss: 0.03\n","Epoch: 014/025 | Batch 080/110 | Loss: 0.27\n","Epoch: 014/025 | Batch 081/110 | Loss: 0.01\n","Epoch: 014/025 | Batch 082/110 | Loss: 0.04\n","Epoch: 014/025 | Batch 083/110 | Loss: 0.06\n","Epoch: 014/025 | Batch 084/110 | Loss: 0.04\n","Epoch: 014/025 | Batch 085/110 | Loss: 0.01\n","Epoch: 014/025 | Batch 086/110 | Loss: 0.02\n","Epoch: 014/025 | Batch 087/110 | Loss: 0.03\n","Epoch: 014/025 | Batch 088/110 | Loss: 0.03\n","Epoch: 014/025 | Batch 089/110 | Loss: 0.05\n","Epoch: 014/025 | Batch 090/110 | Loss: 0.11\n","Epoch: 014/025 | Batch 091/110 | Loss: 0.01\n","Epoch: 014/025 | Batch 092/110 | Loss: 0.04\n","Epoch: 014/025 | Batch 093/110 | Loss: 0.03\n","Epoch: 014/025 | Batch 094/110 | Loss: 0.10\n","Epoch: 014/025 | Batch 095/110 | Loss: 0.03\n","Epoch: 014/025 | Batch 096/110 | Loss: 0.08\n","Epoch: 014/025 | Batch 097/110 | Loss: 0.03\n","Epoch: 014/025 | Batch 098/110 | Loss: 0.06\n","Epoch: 014/025 | Batch 099/110 | Loss: 0.02\n","Epoch: 014/025 | Batch 100/110 | Loss: 0.03\n","Epoch: 014/025 | Batch 101/110 | Loss: 0.06\n","Epoch: 014/025 | Batch 102/110 | Loss: 0.13\n","Epoch: 014/025 | Batch 103/110 | Loss: 0.03\n","Epoch: 014/025 | Batch 104/110 | Loss: 0.24\n","Epoch: 014/025 | Batch 105/110 | Loss: 0.11\n","Epoch: 014/025 | Batch 106/110 | Loss: 0.05\n","Epoch: 014/025 | Batch 107/110 | Loss: 0.02\n","Epoch: 014/025 | Batch 108/110 | Loss: 0.05\n","Epoch: 014/025 | Batch 109/110 | Loss: 0.28\n","Epoch: 015/025 | Batch 000/110 | Loss: 0.03\n","Epoch: 015/025 | Batch 001/110 | Loss: 0.07\n","Epoch: 015/025 | Batch 002/110 | Loss: 0.08\n","Epoch: 015/025 | Batch 003/110 | Loss: 0.06\n","Epoch: 015/025 | Batch 004/110 | Loss: 0.02\n","Epoch: 015/025 | Batch 005/110 | Loss: 0.07\n","Epoch: 015/025 | Batch 006/110 | Loss: 0.03\n","Epoch: 015/025 | Batch 007/110 | Loss: 0.05\n","Epoch: 015/025 | Batch 008/110 | Loss: 0.04\n","Epoch: 015/025 | Batch 009/110 | Loss: 0.07\n","Epoch: 015/025 | Batch 010/110 | Loss: 0.05\n","Epoch: 015/025 | Batch 011/110 | Loss: 0.03\n","Epoch: 015/025 | Batch 012/110 | Loss: 0.11\n","Epoch: 015/025 | Batch 013/110 | Loss: 0.12\n","Epoch: 015/025 | Batch 014/110 | Loss: 0.12\n","Epoch: 015/025 | Batch 015/110 | Loss: 0.15\n","Epoch: 015/025 | Batch 016/110 | Loss: 0.05\n","Epoch: 015/025 | Batch 017/110 | Loss: 0.16\n","Epoch: 015/025 | Batch 018/110 | Loss: 0.03\n","Epoch: 015/025 | Batch 019/110 | Loss: 0.10\n","Epoch: 015/025 | Batch 020/110 | Loss: 0.10\n","Epoch: 015/025 | Batch 021/110 | Loss: 0.06\n","Epoch: 015/025 | Batch 022/110 | Loss: 0.04\n","Epoch: 015/025 | Batch 023/110 | Loss: 0.04\n","Epoch: 015/025 | Batch 024/110 | Loss: 0.04\n","Epoch: 015/025 | Batch 025/110 | Loss: 0.08\n","Epoch: 015/025 | Batch 026/110 | Loss: 0.10\n","Epoch: 015/025 | Batch 027/110 | Loss: 0.01\n","Epoch: 015/025 | Batch 028/110 | Loss: 0.03\n","Epoch: 015/025 | Batch 029/110 | Loss: 0.04\n","Epoch: 015/025 | Batch 030/110 | Loss: 0.03\n","Epoch: 015/025 | Batch 031/110 | Loss: 0.02\n","Epoch: 015/025 | Batch 032/110 | Loss: 0.13\n","Epoch: 015/025 | Batch 033/110 | Loss: 0.10\n","Epoch: 015/025 | Batch 034/110 | Loss: 0.05\n","Epoch: 015/025 | Batch 035/110 | Loss: 0.05\n","Epoch: 015/025 | Batch 036/110 | Loss: 0.01\n","Epoch: 015/025 | Batch 037/110 | Loss: 0.03\n","Epoch: 015/025 | Batch 038/110 | Loss: 0.03\n","Epoch: 015/025 | Batch 039/110 | Loss: 0.13\n","Epoch: 015/025 | Batch 040/110 | Loss: 0.15\n","Epoch: 015/025 | Batch 041/110 | Loss: 0.06\n","Epoch: 015/025 | Batch 042/110 | Loss: 0.06\n","Epoch: 015/025 | Batch 043/110 | Loss: 0.08\n","Epoch: 015/025 | Batch 044/110 | Loss: 0.02\n","Epoch: 015/025 | Batch 045/110 | Loss: 0.13\n","Epoch: 015/025 | Batch 046/110 | Loss: 0.18\n","Epoch: 015/025 | Batch 047/110 | Loss: 0.08\n","Epoch: 015/025 | Batch 048/110 | Loss: 0.03\n","Epoch: 015/025 | Batch 049/110 | Loss: 0.01\n","Epoch: 015/025 | Batch 050/110 | Loss: 0.05\n","Epoch: 015/025 | Batch 051/110 | Loss: 0.07\n","Epoch: 015/025 | Batch 052/110 | Loss: 0.05\n","Epoch: 015/025 | Batch 053/110 | Loss: 0.12\n","Epoch: 015/025 | Batch 054/110 | Loss: 0.02\n","Epoch: 015/025 | Batch 055/110 | Loss: 0.03\n","Epoch: 015/025 | Batch 056/110 | Loss: 0.06\n","Epoch: 015/025 | Batch 057/110 | Loss: 0.16\n","Epoch: 015/025 | Batch 058/110 | Loss: 0.02\n","Epoch: 015/025 | Batch 059/110 | Loss: 0.02\n","Epoch: 015/025 | Batch 060/110 | Loss: 0.08\n","Epoch: 015/025 | Batch 061/110 | Loss: 0.04\n","Epoch: 015/025 | Batch 062/110 | Loss: 0.04\n","Epoch: 015/025 | Batch 063/110 | Loss: 0.08\n","Epoch: 015/025 | Batch 064/110 | Loss: 0.03\n","Epoch: 015/025 | Batch 065/110 | Loss: 0.03\n","Epoch: 015/025 | Batch 066/110 | Loss: 0.04\n","Epoch: 015/025 | Batch 067/110 | Loss: 0.03\n","Epoch: 015/025 | Batch 068/110 | Loss: 0.03\n","Epoch: 015/025 | Batch 069/110 | Loss: 0.03\n","Epoch: 015/025 | Batch 070/110 | Loss: 0.03\n","Epoch: 015/025 | Batch 071/110 | Loss: 0.11\n","Epoch: 015/025 | Batch 072/110 | Loss: 0.06\n","Epoch: 015/025 | Batch 073/110 | Loss: 0.03\n","Epoch: 015/025 | Batch 074/110 | Loss: 0.03\n","Epoch: 015/025 | Batch 075/110 | Loss: 0.09\n","Epoch: 015/025 | Batch 076/110 | Loss: 0.06\n","Epoch: 015/025 | Batch 077/110 | Loss: 0.03\n","Epoch: 015/025 | Batch 078/110 | Loss: 0.10\n","Epoch: 015/025 | Batch 079/110 | Loss: 0.05\n","Epoch: 015/025 | Batch 080/110 | Loss: 0.06\n","Epoch: 015/025 | Batch 081/110 | Loss: 0.05\n","Epoch: 015/025 | Batch 082/110 | Loss: 0.07\n","Epoch: 015/025 | Batch 083/110 | Loss: 0.06\n","Epoch: 015/025 | Batch 084/110 | Loss: 0.14\n","Epoch: 015/025 | Batch 085/110 | Loss: 0.07\n","Epoch: 015/025 | Batch 086/110 | Loss: 0.04\n","Epoch: 015/025 | Batch 087/110 | Loss: 0.08\n","Epoch: 015/025 | Batch 088/110 | Loss: 0.18\n","Epoch: 015/025 | Batch 089/110 | Loss: 0.10\n","Epoch: 015/025 | Batch 090/110 | Loss: 0.02\n","Epoch: 015/025 | Batch 091/110 | Loss: 0.01\n","Epoch: 015/025 | Batch 092/110 | Loss: 0.04\n","Epoch: 015/025 | Batch 093/110 | Loss: 0.03\n","Epoch: 015/025 | Batch 094/110 | Loss: 0.03\n","Epoch: 015/025 | Batch 095/110 | Loss: 0.17\n","Epoch: 015/025 | Batch 096/110 | Loss: 0.11\n","Epoch: 015/025 | Batch 097/110 | Loss: 0.05\n","Epoch: 015/025 | Batch 098/110 | Loss: 0.14\n","Epoch: 015/025 | Batch 099/110 | Loss: 0.07\n","Epoch: 015/025 | Batch 100/110 | Loss: 0.03\n","Epoch: 015/025 | Batch 101/110 | Loss: 0.17\n","Epoch: 015/025 | Batch 102/110 | Loss: 0.05\n","Epoch: 015/025 | Batch 103/110 | Loss: 0.02\n","Epoch: 015/025 | Batch 104/110 | Loss: 0.07\n","Epoch: 015/025 | Batch 105/110 | Loss: 0.12\n","Epoch: 015/025 | Batch 106/110 | Loss: 0.17\n","Epoch: 015/025 | Batch 107/110 | Loss: 0.01\n","Epoch: 015/025 | Batch 108/110 | Loss: 0.03\n","Epoch: 015/025 | Batch 109/110 | Loss: 0.13\n","Epoch: 016/025 | Batch 000/110 | Loss: 0.06\n","Epoch: 016/025 | Batch 001/110 | Loss: 0.11\n","Epoch: 016/025 | Batch 002/110 | Loss: 0.05\n","Epoch: 016/025 | Batch 003/110 | Loss: 0.14\n","Epoch: 016/025 | Batch 004/110 | Loss: 0.10\n","Epoch: 016/025 | Batch 005/110 | Loss: 0.27\n","Epoch: 016/025 | Batch 006/110 | Loss: 0.03\n","Epoch: 016/025 | Batch 007/110 | Loss: 0.08\n","Epoch: 016/025 | Batch 008/110 | Loss: 0.07\n","Epoch: 016/025 | Batch 009/110 | Loss: 0.06\n","Epoch: 016/025 | Batch 010/110 | Loss: 0.03\n","Epoch: 016/025 | Batch 011/110 | Loss: 0.17\n","Epoch: 016/025 | Batch 012/110 | Loss: 0.05\n","Epoch: 016/025 | Batch 013/110 | Loss: 0.02\n","Epoch: 016/025 | Batch 014/110 | Loss: 0.04\n","Epoch: 016/025 | Batch 015/110 | Loss: 0.06\n","Epoch: 016/025 | Batch 016/110 | Loss: 0.08\n","Epoch: 016/025 | Batch 017/110 | Loss: 0.06\n","Epoch: 016/025 | Batch 018/110 | Loss: 0.03\n","Epoch: 016/025 | Batch 019/110 | Loss: 0.01\n","Epoch: 016/025 | Batch 020/110 | Loss: 0.06\n","Epoch: 016/025 | Batch 021/110 | Loss: 0.13\n","Epoch: 016/025 | Batch 022/110 | Loss: 0.02\n","Epoch: 016/025 | Batch 023/110 | Loss: 0.09\n","Epoch: 016/025 | Batch 024/110 | Loss: 0.04\n","Epoch: 016/025 | Batch 025/110 | Loss: 0.04\n","Epoch: 016/025 | Batch 026/110 | Loss: 0.08\n","Epoch: 016/025 | Batch 027/110 | Loss: 0.03\n","Epoch: 016/025 | Batch 028/110 | Loss: 0.05\n","Epoch: 016/025 | Batch 029/110 | Loss: 0.03\n","Epoch: 016/025 | Batch 030/110 | Loss: 0.08\n","Epoch: 016/025 | Batch 031/110 | Loss: 0.09\n","Epoch: 016/025 | Batch 032/110 | Loss: 0.04\n","Epoch: 016/025 | Batch 033/110 | Loss: 0.03\n","Epoch: 016/025 | Batch 034/110 | Loss: 0.05\n","Epoch: 016/025 | Batch 035/110 | Loss: 0.14\n","Epoch: 016/025 | Batch 036/110 | Loss: 0.12\n","Epoch: 016/025 | Batch 037/110 | Loss: 0.03\n","Epoch: 016/025 | Batch 038/110 | Loss: 0.03\n","Epoch: 016/025 | Batch 039/110 | Loss: 0.02\n","Epoch: 016/025 | Batch 040/110 | Loss: 0.02\n","Epoch: 016/025 | Batch 041/110 | Loss: 0.11\n","Epoch: 016/025 | Batch 042/110 | Loss: 0.02\n","Epoch: 016/025 | Batch 043/110 | Loss: 0.12\n","Epoch: 016/025 | Batch 044/110 | Loss: 0.02\n","Epoch: 016/025 | Batch 045/110 | Loss: 0.01\n","Epoch: 016/025 | Batch 046/110 | Loss: 0.02\n","Epoch: 016/025 | Batch 047/110 | Loss: 0.02\n","Epoch: 016/025 | Batch 048/110 | Loss: 0.11\n","Epoch: 016/025 | Batch 049/110 | Loss: 0.04\n","Epoch: 016/025 | Batch 050/110 | Loss: 0.06\n","Epoch: 016/025 | Batch 051/110 | Loss: 0.03\n","Epoch: 016/025 | Batch 052/110 | Loss: 0.04\n","Epoch: 016/025 | Batch 053/110 | Loss: 0.04\n","Epoch: 016/025 | Batch 054/110 | Loss: 0.04\n","Epoch: 016/025 | Batch 055/110 | Loss: 0.14\n","Epoch: 016/025 | Batch 056/110 | Loss: 0.01\n","Epoch: 016/025 | Batch 057/110 | Loss: 0.10\n","Epoch: 016/025 | Batch 058/110 | Loss: 0.07\n","Epoch: 016/025 | Batch 059/110 | Loss: 0.04\n","Epoch: 016/025 | Batch 060/110 | Loss: 0.02\n","Epoch: 016/025 | Batch 061/110 | Loss: 0.18\n","Epoch: 016/025 | Batch 062/110 | Loss: 0.04\n","Epoch: 016/025 | Batch 063/110 | Loss: 0.21\n","Epoch: 016/025 | Batch 064/110 | Loss: 0.02\n","Epoch: 016/025 | Batch 065/110 | Loss: 0.11\n","Epoch: 016/025 | Batch 066/110 | Loss: 0.06\n","Epoch: 016/025 | Batch 067/110 | Loss: 0.04\n","Epoch: 016/025 | Batch 068/110 | Loss: 0.01\n","Epoch: 016/025 | Batch 069/110 | Loss: 0.01\n","Epoch: 016/025 | Batch 070/110 | Loss: 0.03\n","Epoch: 016/025 | Batch 071/110 | Loss: 0.05\n","Epoch: 016/025 | Batch 072/110 | Loss: 0.02\n","Epoch: 016/025 | Batch 073/110 | Loss: 0.03\n","Epoch: 016/025 | Batch 074/110 | Loss: 0.05\n","Epoch: 016/025 | Batch 075/110 | Loss: 0.05\n","Epoch: 016/025 | Batch 076/110 | Loss: 0.02\n","Epoch: 016/025 | Batch 077/110 | Loss: 0.12\n","Epoch: 016/025 | Batch 078/110 | Loss: 0.11\n","Epoch: 016/025 | Batch 079/110 | Loss: 0.02\n","Epoch: 016/025 | Batch 080/110 | Loss: 0.03\n","Epoch: 016/025 | Batch 081/110 | Loss: 0.05\n","Epoch: 016/025 | Batch 082/110 | Loss: 0.20\n","Epoch: 016/025 | Batch 083/110 | Loss: 0.04\n","Epoch: 016/025 | Batch 084/110 | Loss: 0.07\n","Epoch: 016/025 | Batch 085/110 | Loss: 0.20\n","Epoch: 016/025 | Batch 086/110 | Loss: 0.01\n","Epoch: 016/025 | Batch 087/110 | Loss: 0.03\n","Epoch: 016/025 | Batch 088/110 | Loss: 0.06\n","Epoch: 016/025 | Batch 089/110 | Loss: 0.06\n","Epoch: 016/025 | Batch 090/110 | Loss: 0.06\n","Epoch: 016/025 | Batch 091/110 | Loss: 0.04\n","Epoch: 016/025 | Batch 092/110 | Loss: 0.07\n","Epoch: 016/025 | Batch 093/110 | Loss: 0.02\n","Epoch: 016/025 | Batch 094/110 | Loss: 0.02\n","Epoch: 016/025 | Batch 095/110 | Loss: 0.14\n","Epoch: 016/025 | Batch 096/110 | Loss: 0.11\n","Epoch: 016/025 | Batch 097/110 | Loss: 0.04\n","Epoch: 016/025 | Batch 098/110 | Loss: 0.03\n","Epoch: 016/025 | Batch 099/110 | Loss: 0.03\n","Epoch: 016/025 | Batch 100/110 | Loss: 0.14\n","Epoch: 016/025 | Batch 101/110 | Loss: 0.04\n","Epoch: 016/025 | Batch 102/110 | Loss: 0.13\n","Epoch: 016/025 | Batch 103/110 | Loss: 0.01\n","Epoch: 016/025 | Batch 104/110 | Loss: 0.13\n","Epoch: 016/025 | Batch 105/110 | Loss: 0.04\n","Epoch: 016/025 | Batch 106/110 | Loss: 0.11\n","Epoch: 016/025 | Batch 107/110 | Loss: 0.02\n","Epoch: 016/025 | Batch 108/110 | Loss: 0.05\n","Epoch: 016/025 | Batch 109/110 | Loss: 0.18\n","Epoch: 017/025 | Batch 000/110 | Loss: 0.03\n","Epoch: 017/025 | Batch 001/110 | Loss: 0.09\n","Epoch: 017/025 | Batch 002/110 | Loss: 0.17\n","Epoch: 017/025 | Batch 003/110 | Loss: 0.03\n","Epoch: 017/025 | Batch 004/110 | Loss: 0.03\n","Epoch: 017/025 | Batch 005/110 | Loss: 0.08\n","Epoch: 017/025 | Batch 006/110 | Loss: 0.08\n","Epoch: 017/025 | Batch 007/110 | Loss: 0.05\n","Epoch: 017/025 | Batch 008/110 | Loss: 0.13\n","Epoch: 017/025 | Batch 009/110 | Loss: 0.04\n","Epoch: 017/025 | Batch 010/110 | Loss: 0.16\n","Epoch: 017/025 | Batch 011/110 | Loss: 0.02\n","Epoch: 017/025 | Batch 012/110 | Loss: 0.06\n","Epoch: 017/025 | Batch 013/110 | Loss: 0.15\n","Epoch: 017/025 | Batch 014/110 | Loss: 0.13\n","Epoch: 017/025 | Batch 015/110 | Loss: 0.03\n","Epoch: 017/025 | Batch 016/110 | Loss: 0.05\n","Epoch: 017/025 | Batch 017/110 | Loss: 0.10\n","Epoch: 017/025 | Batch 018/110 | Loss: 0.01\n","Epoch: 017/025 | Batch 019/110 | Loss: 0.14\n","Epoch: 017/025 | Batch 020/110 | Loss: 0.05\n","Epoch: 017/025 | Batch 021/110 | Loss: 0.04\n","Epoch: 017/025 | Batch 022/110 | Loss: 0.04\n","Epoch: 017/025 | Batch 023/110 | Loss: 0.08\n","Epoch: 017/025 | Batch 024/110 | Loss: 0.22\n","Epoch: 017/025 | Batch 025/110 | Loss: 0.04\n","Epoch: 017/025 | Batch 026/110 | Loss: 0.01\n","Epoch: 017/025 | Batch 027/110 | Loss: 0.05\n","Epoch: 017/025 | Batch 028/110 | Loss: 0.07\n","Epoch: 017/025 | Batch 029/110 | Loss: 0.12\n","Epoch: 017/025 | Batch 030/110 | Loss: 0.05\n","Epoch: 017/025 | Batch 031/110 | Loss: 0.02\n","Epoch: 017/025 | Batch 032/110 | Loss: 0.04\n","Epoch: 017/025 | Batch 033/110 | Loss: 0.02\n","Epoch: 017/025 | Batch 034/110 | Loss: 0.06\n","Epoch: 017/025 | Batch 035/110 | Loss: 0.13\n","Epoch: 017/025 | Batch 036/110 | Loss: 0.04\n","Epoch: 017/025 | Batch 037/110 | Loss: 0.03\n","Epoch: 017/025 | Batch 038/110 | Loss: 0.03\n","Epoch: 017/025 | Batch 039/110 | Loss: 0.05\n","Epoch: 017/025 | Batch 040/110 | Loss: 0.06\n","Epoch: 017/025 | Batch 041/110 | Loss: 0.02\n","Epoch: 017/025 | Batch 042/110 | Loss: 0.08\n","Epoch: 017/025 | Batch 043/110 | Loss: 0.06\n","Epoch: 017/025 | Batch 044/110 | Loss: 0.04\n","Epoch: 017/025 | Batch 045/110 | Loss: 0.03\n","Epoch: 017/025 | Batch 046/110 | Loss: 0.19\n","Epoch: 017/025 | Batch 047/110 | Loss: 0.16\n","Epoch: 017/025 | Batch 048/110 | Loss: 0.07\n","Epoch: 017/025 | Batch 049/110 | Loss: 0.03\n","Epoch: 017/025 | Batch 050/110 | Loss: 0.04\n","Epoch: 017/025 | Batch 051/110 | Loss: 0.02\n","Epoch: 017/025 | Batch 052/110 | Loss: 0.07\n","Epoch: 017/025 | Batch 053/110 | Loss: 0.14\n","Epoch: 017/025 | Batch 054/110 | Loss: 0.03\n","Epoch: 017/025 | Batch 055/110 | Loss: 0.12\n","Epoch: 017/025 | Batch 056/110 | Loss: 0.06\n","Epoch: 017/025 | Batch 057/110 | Loss: 0.13\n","Epoch: 017/025 | Batch 058/110 | Loss: 0.07\n","Epoch: 017/025 | Batch 059/110 | Loss: 0.02\n","Epoch: 017/025 | Batch 060/110 | Loss: 0.11\n","Epoch: 017/025 | Batch 061/110 | Loss: 0.06\n","Epoch: 017/025 | Batch 062/110 | Loss: 0.02\n","Epoch: 017/025 | Batch 063/110 | Loss: 0.03\n","Epoch: 017/025 | Batch 064/110 | Loss: 0.02\n","Epoch: 017/025 | Batch 065/110 | Loss: 0.05\n","Epoch: 017/025 | Batch 066/110 | Loss: 0.02\n","Epoch: 017/025 | Batch 067/110 | Loss: 0.01\n","Epoch: 017/025 | Batch 068/110 | Loss: 0.03\n","Epoch: 017/025 | Batch 069/110 | Loss: 0.04\n","Epoch: 017/025 | Batch 070/110 | Loss: 0.12\n","Epoch: 017/025 | Batch 071/110 | Loss: 0.05\n","Epoch: 017/025 | Batch 072/110 | Loss: 0.03\n","Epoch: 017/025 | Batch 073/110 | Loss: 0.02\n","Epoch: 017/025 | Batch 074/110 | Loss: 0.03\n","Epoch: 017/025 | Batch 075/110 | Loss: 0.07\n","Epoch: 017/025 | Batch 076/110 | Loss: 0.09\n","Epoch: 017/025 | Batch 077/110 | Loss: 0.08\n","Epoch: 017/025 | Batch 078/110 | Loss: 0.06\n","Epoch: 017/025 | Batch 079/110 | Loss: 0.04\n","Epoch: 017/025 | Batch 080/110 | Loss: 0.11\n","Epoch: 017/025 | Batch 081/110 | Loss: 0.13\n","Epoch: 017/025 | Batch 082/110 | Loss: 0.06\n","Epoch: 017/025 | Batch 083/110 | Loss: 0.01\n","Epoch: 017/025 | Batch 084/110 | Loss: 0.03\n","Epoch: 017/025 | Batch 085/110 | Loss: 0.14\n","Epoch: 017/025 | Batch 086/110 | Loss: 0.03\n","Epoch: 017/025 | Batch 087/110 | Loss: 0.10\n","Epoch: 017/025 | Batch 088/110 | Loss: 0.01\n","Epoch: 017/025 | Batch 089/110 | Loss: 0.05\n","Epoch: 017/025 | Batch 090/110 | Loss: 0.02\n","Epoch: 017/025 | Batch 091/110 | Loss: 0.03\n","Epoch: 017/025 | Batch 092/110 | Loss: 0.03\n","Epoch: 017/025 | Batch 093/110 | Loss: 0.08\n","Epoch: 017/025 | Batch 094/110 | Loss: 0.12\n","Epoch: 017/025 | Batch 095/110 | Loss: 0.04\n","Epoch: 017/025 | Batch 096/110 | Loss: 0.13\n","Epoch: 017/025 | Batch 097/110 | Loss: 0.06\n","Epoch: 017/025 | Batch 098/110 | Loss: 0.01\n","Epoch: 017/025 | Batch 099/110 | Loss: 0.03\n","Epoch: 017/025 | Batch 100/110 | Loss: 0.11\n","Epoch: 017/025 | Batch 101/110 | Loss: 0.02\n","Epoch: 017/025 | Batch 102/110 | Loss: 0.02\n","Epoch: 017/025 | Batch 103/110 | Loss: 0.04\n","Epoch: 017/025 | Batch 104/110 | Loss: 0.03\n","Epoch: 017/025 | Batch 105/110 | Loss: 0.02\n","Epoch: 017/025 | Batch 106/110 | Loss: 0.04\n","Epoch: 017/025 | Batch 107/110 | Loss: 0.11\n","Epoch: 017/025 | Batch 108/110 | Loss: 0.13\n","Epoch: 017/025 | Batch 109/110 | Loss: 0.02\n","Epoch: 018/025 | Batch 000/110 | Loss: 0.03\n","Epoch: 018/025 | Batch 001/110 | Loss: 0.14\n","Epoch: 018/025 | Batch 002/110 | Loss: 0.03\n","Epoch: 018/025 | Batch 003/110 | Loss: 0.05\n","Epoch: 018/025 | Batch 004/110 | Loss: 0.06\n","Epoch: 018/025 | Batch 005/110 | Loss: 0.08\n","Epoch: 018/025 | Batch 006/110 | Loss: 0.04\n","Epoch: 018/025 | Batch 007/110 | Loss: 0.02\n","Epoch: 018/025 | Batch 008/110 | Loss: 0.05\n","Epoch: 018/025 | Batch 009/110 | Loss: 0.02\n","Epoch: 018/025 | Batch 010/110 | Loss: 0.07\n","Epoch: 018/025 | Batch 011/110 | Loss: 0.02\n","Epoch: 018/025 | Batch 012/110 | Loss: 0.08\n","Epoch: 018/025 | Batch 013/110 | Loss: 0.04\n","Epoch: 018/025 | Batch 014/110 | Loss: 0.05\n","Epoch: 018/025 | Batch 015/110 | Loss: 0.02\n","Epoch: 018/025 | Batch 016/110 | Loss: 0.03\n","Epoch: 018/025 | Batch 017/110 | Loss: 0.12\n","Epoch: 018/025 | Batch 018/110 | Loss: 0.06\n","Epoch: 018/025 | Batch 019/110 | Loss: 0.06\n","Epoch: 018/025 | Batch 020/110 | Loss: 0.02\n","Epoch: 018/025 | Batch 021/110 | Loss: 0.06\n","Epoch: 018/025 | Batch 022/110 | Loss: 0.07\n","Epoch: 018/025 | Batch 023/110 | Loss: 0.08\n","Epoch: 018/025 | Batch 024/110 | Loss: 0.03\n","Epoch: 018/025 | Batch 025/110 | Loss: 0.03\n","Epoch: 018/025 | Batch 026/110 | Loss: 0.16\n","Epoch: 018/025 | Batch 027/110 | Loss: 0.04\n","Epoch: 018/025 | Batch 028/110 | Loss: 0.04\n","Epoch: 018/025 | Batch 029/110 | Loss: 0.07\n","Epoch: 018/025 | Batch 030/110 | Loss: 0.03\n","Epoch: 018/025 | Batch 031/110 | Loss: 0.05\n","Epoch: 018/025 | Batch 032/110 | Loss: 0.02\n","Epoch: 018/025 | Batch 033/110 | Loss: 0.06\n","Epoch: 018/025 | Batch 034/110 | Loss: 0.02\n","Epoch: 018/025 | Batch 035/110 | Loss: 0.04\n","Epoch: 018/025 | Batch 036/110 | Loss: 0.02\n","Epoch: 018/025 | Batch 037/110 | Loss: 0.01\n","Epoch: 018/025 | Batch 038/110 | Loss: 0.03\n","Epoch: 018/025 | Batch 039/110 | Loss: 0.02\n","Epoch: 018/025 | Batch 040/110 | Loss: 0.03\n","Epoch: 018/025 | Batch 041/110 | Loss: 0.03\n","Epoch: 018/025 | Batch 042/110 | Loss: 0.04\n","Epoch: 018/025 | Batch 043/110 | Loss: 0.03\n","Epoch: 018/025 | Batch 044/110 | Loss: 0.08\n","Epoch: 018/025 | Batch 045/110 | Loss: 0.04\n","Epoch: 018/025 | Batch 046/110 | Loss: 0.11\n","Epoch: 018/025 | Batch 047/110 | Loss: 0.03\n","Epoch: 018/025 | Batch 048/110 | Loss: 0.02\n","Epoch: 018/025 | Batch 049/110 | Loss: 0.02\n","Epoch: 018/025 | Batch 050/110 | Loss: 0.04\n","Epoch: 018/025 | Batch 051/110 | Loss: 0.16\n","Epoch: 018/025 | Batch 052/110 | Loss: 0.16\n","Epoch: 018/025 | Batch 053/110 | Loss: 0.02\n","Epoch: 018/025 | Batch 054/110 | Loss: 0.14\n","Epoch: 018/025 | Batch 055/110 | Loss: 0.01\n","Epoch: 018/025 | Batch 056/110 | Loss: 0.02\n","Epoch: 018/025 | Batch 057/110 | Loss: 0.06\n","Epoch: 018/025 | Batch 058/110 | Loss: 0.05\n","Epoch: 018/025 | Batch 059/110 | Loss: 0.13\n","Epoch: 018/025 | Batch 060/110 | Loss: 0.07\n","Epoch: 018/025 | Batch 061/110 | Loss: 0.03\n","Epoch: 018/025 | Batch 062/110 | Loss: 0.03\n","Epoch: 018/025 | Batch 063/110 | Loss: 0.03\n","Epoch: 018/025 | Batch 064/110 | Loss: 0.05\n","Epoch: 018/025 | Batch 065/110 | Loss: 0.06\n","Epoch: 018/025 | Batch 066/110 | Loss: 0.10\n","Epoch: 018/025 | Batch 067/110 | Loss: 0.04\n","Epoch: 018/025 | Batch 068/110 | Loss: 0.05\n","Epoch: 018/025 | Batch 069/110 | Loss: 0.02\n","Epoch: 018/025 | Batch 070/110 | Loss: 0.01\n","Epoch: 018/025 | Batch 071/110 | Loss: 0.13\n","Epoch: 018/025 | Batch 072/110 | Loss: 0.14\n","Epoch: 018/025 | Batch 073/110 | Loss: 0.16\n","Epoch: 018/025 | Batch 074/110 | Loss: 0.14\n","Epoch: 018/025 | Batch 075/110 | Loss: 0.08\n","Epoch: 018/025 | Batch 076/110 | Loss: 0.05\n","Epoch: 018/025 | Batch 077/110 | Loss: 0.04\n","Epoch: 018/025 | Batch 078/110 | Loss: 0.02\n","Epoch: 018/025 | Batch 079/110 | Loss: 0.19\n","Epoch: 018/025 | Batch 080/110 | Loss: 0.02\n","Epoch: 018/025 | Batch 081/110 | Loss: 0.03\n","Epoch: 018/025 | Batch 082/110 | Loss: 0.08\n","Epoch: 018/025 | Batch 083/110 | Loss: 0.03\n","Epoch: 018/025 | Batch 084/110 | Loss: 0.02\n","Epoch: 018/025 | Batch 085/110 | Loss: 0.06\n","Epoch: 018/025 | Batch 086/110 | Loss: 0.03\n","Epoch: 018/025 | Batch 087/110 | Loss: 0.22\n","Epoch: 018/025 | Batch 088/110 | Loss: 0.08\n","Epoch: 018/025 | Batch 089/110 | Loss: 0.15\n","Epoch: 018/025 | Batch 090/110 | Loss: 0.08\n","Epoch: 018/025 | Batch 091/110 | Loss: 0.13\n","Epoch: 018/025 | Batch 092/110 | Loss: 0.01\n","Epoch: 018/025 | Batch 093/110 | Loss: 0.02\n","Epoch: 018/025 | Batch 094/110 | Loss: 0.03\n","Epoch: 018/025 | Batch 095/110 | Loss: 0.01\n","Epoch: 018/025 | Batch 096/110 | Loss: 0.03\n","Epoch: 018/025 | Batch 097/110 | Loss: 0.10\n","Epoch: 018/025 | Batch 098/110 | Loss: 0.01\n","Epoch: 018/025 | Batch 099/110 | Loss: 0.08\n","Epoch: 018/025 | Batch 100/110 | Loss: 0.15\n","Epoch: 018/025 | Batch 101/110 | Loss: 0.05\n","Epoch: 018/025 | Batch 102/110 | Loss: 0.08\n","Epoch: 018/025 | Batch 103/110 | Loss: 0.12\n","Epoch: 018/025 | Batch 104/110 | Loss: 0.15\n","Epoch: 018/025 | Batch 105/110 | Loss: 0.05\n","Epoch: 018/025 | Batch 106/110 | Loss: 0.05\n","Epoch: 018/025 | Batch 107/110 | Loss: 0.16\n","Epoch: 018/025 | Batch 108/110 | Loss: 0.04\n","Epoch: 018/025 | Batch 109/110 | Loss: 0.09\n","Epoch: 019/025 | Batch 000/110 | Loss: 0.04\n","Epoch: 019/025 | Batch 001/110 | Loss: 0.12\n","Epoch: 019/025 | Batch 002/110 | Loss: 0.08\n","Epoch: 019/025 | Batch 003/110 | Loss: 0.06\n","Epoch: 019/025 | Batch 004/110 | Loss: 0.02\n","Epoch: 019/025 | Batch 005/110 | Loss: 0.04\n","Epoch: 019/025 | Batch 006/110 | Loss: 0.02\n","Epoch: 019/025 | Batch 007/110 | Loss: 0.07\n","Epoch: 019/025 | Batch 008/110 | Loss: 0.18\n","Epoch: 019/025 | Batch 009/110 | Loss: 0.01\n","Epoch: 019/025 | Batch 010/110 | Loss: 0.04\n","Epoch: 019/025 | Batch 011/110 | Loss: 0.03\n","Epoch: 019/025 | Batch 012/110 | Loss: 0.01\n","Epoch: 019/025 | Batch 013/110 | Loss: 0.06\n","Epoch: 019/025 | Batch 014/110 | Loss: 0.03\n","Epoch: 019/025 | Batch 015/110 | Loss: 0.10\n","Epoch: 019/025 | Batch 016/110 | Loss: 0.04\n","Epoch: 019/025 | Batch 017/110 | Loss: 0.05\n","Epoch: 019/025 | Batch 018/110 | Loss: 0.13\n","Epoch: 019/025 | Batch 019/110 | Loss: 0.03\n","Epoch: 019/025 | Batch 020/110 | Loss: 0.02\n","Epoch: 019/025 | Batch 021/110 | Loss: 0.04\n","Epoch: 019/025 | Batch 022/110 | Loss: 0.02\n","Epoch: 019/025 | Batch 023/110 | Loss: 0.02\n","Epoch: 019/025 | Batch 024/110 | Loss: 0.02\n","Epoch: 019/025 | Batch 025/110 | Loss: 0.03\n","Epoch: 019/025 | Batch 026/110 | Loss: 0.03\n","Epoch: 019/025 | Batch 027/110 | Loss: 0.14\n","Epoch: 019/025 | Batch 028/110 | Loss: 0.05\n","Epoch: 019/025 | Batch 029/110 | Loss: 0.04\n","Epoch: 019/025 | Batch 030/110 | Loss: 0.08\n","Epoch: 019/025 | Batch 031/110 | Loss: 0.01\n","Epoch: 019/025 | Batch 032/110 | Loss: 0.04\n","Epoch: 019/025 | Batch 033/110 | Loss: 0.03\n","Epoch: 019/025 | Batch 034/110 | Loss: 0.10\n","Epoch: 019/025 | Batch 035/110 | Loss: 0.01\n","Epoch: 019/025 | Batch 036/110 | Loss: 0.05\n","Epoch: 019/025 | Batch 037/110 | Loss: 0.07\n","Epoch: 019/025 | Batch 038/110 | Loss: 0.07\n","Epoch: 019/025 | Batch 039/110 | Loss: 0.13\n","Epoch: 019/025 | Batch 040/110 | Loss: 0.01\n","Epoch: 019/025 | Batch 041/110 | Loss: 0.15\n","Epoch: 019/025 | Batch 042/110 | Loss: 0.04\n","Epoch: 019/025 | Batch 043/110 | Loss: 0.02\n","Epoch: 019/025 | Batch 044/110 | Loss: 0.13\n","Epoch: 019/025 | Batch 045/110 | Loss: 0.04\n","Epoch: 019/025 | Batch 046/110 | Loss: 0.04\n","Epoch: 019/025 | Batch 047/110 | Loss: 0.03\n","Epoch: 019/025 | Batch 048/110 | Loss: 0.02\n","Epoch: 019/025 | Batch 049/110 | Loss: 0.17\n","Epoch: 019/025 | Batch 050/110 | Loss: 0.03\n","Epoch: 019/025 | Batch 051/110 | Loss: 0.02\n","Epoch: 019/025 | Batch 052/110 | Loss: 0.15\n","Epoch: 019/025 | Batch 053/110 | Loss: 0.12\n","Epoch: 019/025 | Batch 054/110 | Loss: 0.03\n","Epoch: 019/025 | Batch 055/110 | Loss: 0.06\n","Epoch: 019/025 | Batch 056/110 | Loss: 0.13\n","Epoch: 019/025 | Batch 057/110 | Loss: 0.03\n","Epoch: 019/025 | Batch 058/110 | Loss: 0.03\n","Epoch: 019/025 | Batch 059/110 | Loss: 0.05\n","Epoch: 019/025 | Batch 060/110 | Loss: 0.04\n","Epoch: 019/025 | Batch 061/110 | Loss: 0.01\n","Epoch: 019/025 | Batch 062/110 | Loss: 0.19\n","Epoch: 019/025 | Batch 063/110 | Loss: 0.03\n","Epoch: 019/025 | Batch 064/110 | Loss: 0.02\n","Epoch: 019/025 | Batch 065/110 | Loss: 0.15\n","Epoch: 019/025 | Batch 066/110 | Loss: 0.03\n","Epoch: 019/025 | Batch 067/110 | Loss: 0.02\n","Epoch: 019/025 | Batch 068/110 | Loss: 0.05\n","Epoch: 019/025 | Batch 069/110 | Loss: 0.03\n","Epoch: 019/025 | Batch 070/110 | Loss: 0.02\n","Epoch: 019/025 | Batch 071/110 | Loss: 0.02\n","Epoch: 019/025 | Batch 072/110 | Loss: 0.02\n","Epoch: 019/025 | Batch 073/110 | Loss: 0.04\n","Epoch: 019/025 | Batch 074/110 | Loss: 0.06\n","Epoch: 019/025 | Batch 075/110 | Loss: 0.07\n","Epoch: 019/025 | Batch 076/110 | Loss: 0.04\n","Epoch: 019/025 | Batch 077/110 | Loss: 0.02\n","Epoch: 019/025 | Batch 078/110 | Loss: 0.13\n","Epoch: 019/025 | Batch 079/110 | Loss: 0.01\n","Epoch: 019/025 | Batch 080/110 | Loss: 0.02\n","Epoch: 019/025 | Batch 081/110 | Loss: 0.06\n","Epoch: 019/025 | Batch 082/110 | Loss: 0.02\n","Epoch: 019/025 | Batch 083/110 | Loss: 0.06\n","Epoch: 019/025 | Batch 084/110 | Loss: 0.05\n","Epoch: 019/025 | Batch 085/110 | Loss: 0.05\n","Epoch: 019/025 | Batch 086/110 | Loss: 0.04\n","Epoch: 019/025 | Batch 087/110 | Loss: 0.14\n","Epoch: 019/025 | Batch 088/110 | Loss: 0.11\n","Epoch: 019/025 | Batch 089/110 | Loss: 0.03\n","Epoch: 019/025 | Batch 090/110 | Loss: 0.07\n","Epoch: 019/025 | Batch 091/110 | Loss: 0.03\n","Epoch: 019/025 | Batch 092/110 | Loss: 0.05\n","Epoch: 019/025 | Batch 093/110 | Loss: 0.01\n","Epoch: 019/025 | Batch 094/110 | Loss: 0.12\n","Epoch: 019/025 | Batch 095/110 | Loss: 0.04\n","Epoch: 019/025 | Batch 096/110 | Loss: 0.03\n","Epoch: 019/025 | Batch 097/110 | Loss: 0.02\n","Epoch: 019/025 | Batch 098/110 | Loss: 0.14\n","Epoch: 019/025 | Batch 099/110 | Loss: 0.04\n","Epoch: 019/025 | Batch 100/110 | Loss: 0.03\n","Epoch: 019/025 | Batch 101/110 | Loss: 0.22\n","Epoch: 019/025 | Batch 102/110 | Loss: 0.02\n","Epoch: 019/025 | Batch 103/110 | Loss: 0.03\n","Epoch: 019/025 | Batch 104/110 | Loss: 0.21\n","Epoch: 019/025 | Batch 105/110 | Loss: 0.05\n","Epoch: 019/025 | Batch 106/110 | Loss: 0.02\n","Epoch: 019/025 | Batch 107/110 | Loss: 0.17\n","Epoch: 019/025 | Batch 108/110 | Loss: 0.23\n","Epoch: 019/025 | Batch 109/110 | Loss: 0.03\n","Epoch: 020/025 | Batch 000/110 | Loss: 0.04\n","Epoch: 020/025 | Batch 001/110 | Loss: 0.02\n","Epoch: 020/025 | Batch 002/110 | Loss: 0.02\n","Epoch: 020/025 | Batch 003/110 | Loss: 0.06\n","Epoch: 020/025 | Batch 004/110 | Loss: 0.04\n","Epoch: 020/025 | Batch 005/110 | Loss: 0.05\n","Epoch: 020/025 | Batch 006/110 | Loss: 0.02\n","Epoch: 020/025 | Batch 007/110 | Loss: 0.02\n","Epoch: 020/025 | Batch 008/110 | Loss: 0.13\n","Epoch: 020/025 | Batch 009/110 | Loss: 0.01\n","Epoch: 020/025 | Batch 010/110 | Loss: 0.02\n","Epoch: 020/025 | Batch 011/110 | Loss: 0.11\n","Epoch: 020/025 | Batch 012/110 | Loss: 0.06\n","Epoch: 020/025 | Batch 013/110 | Loss: 0.02\n","Epoch: 020/025 | Batch 014/110 | Loss: 0.08\n","Epoch: 020/025 | Batch 015/110 | Loss: 0.01\n","Epoch: 020/025 | Batch 016/110 | Loss: 0.02\n","Epoch: 020/025 | Batch 017/110 | Loss: 0.03\n","Epoch: 020/025 | Batch 018/110 | Loss: 0.08\n","Epoch: 020/025 | Batch 019/110 | Loss: 0.09\n","Epoch: 020/025 | Batch 020/110 | Loss: 0.06\n","Epoch: 020/025 | Batch 021/110 | Loss: 0.03\n","Epoch: 020/025 | Batch 022/110 | Loss: 0.16\n","Epoch: 020/025 | Batch 023/110 | Loss: 0.01\n","Epoch: 020/025 | Batch 024/110 | Loss: 0.02\n","Epoch: 020/025 | Batch 025/110 | Loss: 0.06\n","Epoch: 020/025 | Batch 026/110 | Loss: 0.03\n","Epoch: 020/025 | Batch 027/110 | Loss: 0.03\n","Epoch: 020/025 | Batch 028/110 | Loss: 0.01\n","Epoch: 020/025 | Batch 029/110 | Loss: 0.02\n","Epoch: 020/025 | Batch 030/110 | Loss: 0.05\n","Epoch: 020/025 | Batch 031/110 | Loss: 0.01\n","Epoch: 020/025 | Batch 032/110 | Loss: 0.04\n","Epoch: 020/025 | Batch 033/110 | Loss: 0.18\n","Epoch: 020/025 | Batch 034/110 | Loss: 0.16\n","Epoch: 020/025 | Batch 035/110 | Loss: 0.09\n","Epoch: 020/025 | Batch 036/110 | Loss: 0.03\n","Epoch: 020/025 | Batch 037/110 | Loss: 0.02\n","Epoch: 020/025 | Batch 038/110 | Loss: 0.02\n","Epoch: 020/025 | Batch 039/110 | Loss: 0.04\n","Epoch: 020/025 | Batch 040/110 | Loss: 0.06\n","Epoch: 020/025 | Batch 041/110 | Loss: 0.06\n","Epoch: 020/025 | Batch 042/110 | Loss: 0.12\n","Epoch: 020/025 | Batch 043/110 | Loss: 0.13\n","Epoch: 020/025 | Batch 044/110 | Loss: 0.11\n","Epoch: 020/025 | Batch 045/110 | Loss: 0.04\n","Epoch: 020/025 | Batch 046/110 | Loss: 0.04\n","Epoch: 020/025 | Batch 047/110 | Loss: 0.14\n","Epoch: 020/025 | Batch 048/110 | Loss: 0.02\n","Epoch: 020/025 | Batch 049/110 | Loss: 0.05\n","Epoch: 020/025 | Batch 050/110 | Loss: 0.05\n","Epoch: 020/025 | Batch 051/110 | Loss: 0.01\n","Epoch: 020/025 | Batch 052/110 | Loss: 0.14\n","Epoch: 020/025 | Batch 053/110 | Loss: 0.13\n","Epoch: 020/025 | Batch 054/110 | Loss: 0.17\n","Epoch: 020/025 | Batch 055/110 | Loss: 0.14\n","Epoch: 020/025 | Batch 056/110 | Loss: 0.02\n","Epoch: 020/025 | Batch 057/110 | Loss: 0.08\n","Epoch: 020/025 | Batch 058/110 | Loss: 0.04\n","Epoch: 020/025 | Batch 059/110 | Loss: 0.02\n","Epoch: 020/025 | Batch 060/110 | Loss: 0.08\n","Epoch: 020/025 | Batch 061/110 | Loss: 0.05\n","Epoch: 020/025 | Batch 062/110 | Loss: 0.16\n","Epoch: 020/025 | Batch 063/110 | Loss: 0.11\n","Epoch: 020/025 | Batch 064/110 | Loss: 0.10\n","Epoch: 020/025 | Batch 065/110 | Loss: 0.02\n","Epoch: 020/025 | Batch 066/110 | Loss: 0.06\n","Epoch: 020/025 | Batch 067/110 | Loss: 0.02\n","Epoch: 020/025 | Batch 068/110 | Loss: 0.05\n","Epoch: 020/025 | Batch 069/110 | Loss: 0.21\n","Epoch: 020/025 | Batch 070/110 | Loss: 0.03\n","Epoch: 020/025 | Batch 071/110 | Loss: 0.03\n","Epoch: 020/025 | Batch 072/110 | Loss: 0.04\n","Epoch: 020/025 | Batch 073/110 | Loss: 0.05\n","Epoch: 020/025 | Batch 074/110 | Loss: 0.03\n","Epoch: 020/025 | Batch 075/110 | Loss: 0.03\n","Epoch: 020/025 | Batch 076/110 | Loss: 0.05\n","Epoch: 020/025 | Batch 077/110 | Loss: 0.02\n","Epoch: 020/025 | Batch 078/110 | Loss: 0.06\n","Epoch: 020/025 | Batch 079/110 | Loss: 0.10\n","Epoch: 020/025 | Batch 080/110 | Loss: 0.03\n","Epoch: 020/025 | Batch 081/110 | Loss: 0.12\n","Epoch: 020/025 | Batch 082/110 | Loss: 0.03\n","Epoch: 020/025 | Batch 083/110 | Loss: 0.06\n","Epoch: 020/025 | Batch 084/110 | Loss: 0.02\n","Epoch: 020/025 | Batch 085/110 | Loss: 0.04\n","Epoch: 020/025 | Batch 086/110 | Loss: 0.18\n","Epoch: 020/025 | Batch 087/110 | Loss: 0.08\n","Epoch: 020/025 | Batch 088/110 | Loss: 0.02\n","Epoch: 020/025 | Batch 089/110 | Loss: 0.09\n","Epoch: 020/025 | Batch 090/110 | Loss: 0.03\n","Epoch: 020/025 | Batch 091/110 | Loss: 0.01\n","Epoch: 020/025 | Batch 092/110 | Loss: 0.02\n","Epoch: 020/025 | Batch 093/110 | Loss: 0.01\n","Epoch: 020/025 | Batch 094/110 | Loss: 0.01\n","Epoch: 020/025 | Batch 095/110 | Loss: 0.13\n","Epoch: 020/025 | Batch 096/110 | Loss: 0.02\n","Epoch: 020/025 | Batch 097/110 | Loss: 0.13\n","Epoch: 020/025 | Batch 098/110 | Loss: 0.02\n","Epoch: 020/025 | Batch 099/110 | Loss: 0.03\n","Epoch: 020/025 | Batch 100/110 | Loss: 0.01\n","Epoch: 020/025 | Batch 101/110 | Loss: 0.01\n","Epoch: 020/025 | Batch 102/110 | Loss: 0.04\n","Epoch: 020/025 | Batch 103/110 | Loss: 0.01\n","Epoch: 020/025 | Batch 104/110 | Loss: 0.12\n","Epoch: 020/025 | Batch 105/110 | Loss: 0.07\n","Epoch: 020/025 | Batch 106/110 | Loss: 0.01\n","Epoch: 020/025 | Batch 107/110 | Loss: 0.06\n","Epoch: 020/025 | Batch 108/110 | Loss: 0.06\n","Epoch: 020/025 | Batch 109/110 | Loss: 0.12\n","Epoch: 021/025 | Batch 000/110 | Loss: 0.02\n","Epoch: 021/025 | Batch 001/110 | Loss: 0.01\n","Epoch: 021/025 | Batch 002/110 | Loss: 0.15\n","Epoch: 021/025 | Batch 003/110 | Loss: 0.07\n","Epoch: 021/025 | Batch 004/110 | Loss: 0.02\n","Epoch: 021/025 | Batch 005/110 | Loss: 0.03\n","Epoch: 021/025 | Batch 006/110 | Loss: 0.20\n","Epoch: 021/025 | Batch 007/110 | Loss: 0.11\n","Epoch: 021/025 | Batch 008/110 | Loss: 0.04\n","Epoch: 021/025 | Batch 009/110 | Loss: 0.05\n","Epoch: 021/025 | Batch 010/110 | Loss: 0.05\n","Epoch: 021/025 | Batch 011/110 | Loss: 0.16\n","Epoch: 021/025 | Batch 012/110 | Loss: 0.04\n","Epoch: 021/025 | Batch 013/110 | Loss: 0.06\n","Epoch: 021/025 | Batch 014/110 | Loss: 0.04\n","Epoch: 021/025 | Batch 015/110 | Loss: 0.04\n","Epoch: 021/025 | Batch 016/110 | Loss: 0.05\n","Epoch: 021/025 | Batch 017/110 | Loss: 0.05\n","Epoch: 021/025 | Batch 018/110 | Loss: 0.03\n","Epoch: 021/025 | Batch 019/110 | Loss: 0.05\n","Epoch: 021/025 | Batch 020/110 | Loss: 0.03\n","Epoch: 021/025 | Batch 021/110 | Loss: 0.03\n","Epoch: 021/025 | Batch 022/110 | Loss: 0.18\n","Epoch: 021/025 | Batch 023/110 | Loss: 0.07\n","Epoch: 021/025 | Batch 024/110 | Loss: 0.05\n","Epoch: 021/025 | Batch 025/110 | Loss: 0.02\n","Epoch: 021/025 | Batch 026/110 | Loss: 0.07\n","Epoch: 021/025 | Batch 027/110 | Loss: 0.22\n","Epoch: 021/025 | Batch 028/110 | Loss: 0.02\n","Epoch: 021/025 | Batch 029/110 | Loss: 0.04\n","Epoch: 021/025 | Batch 030/110 | Loss: 0.01\n","Epoch: 021/025 | Batch 031/110 | Loss: 0.09\n","Epoch: 021/025 | Batch 032/110 | Loss: 0.05\n","Epoch: 021/025 | Batch 033/110 | Loss: 0.18\n","Epoch: 021/025 | Batch 034/110 | Loss: 0.02\n","Epoch: 021/025 | Batch 035/110 | Loss: 0.05\n","Epoch: 021/025 | Batch 036/110 | Loss: 0.04\n","Epoch: 021/025 | Batch 037/110 | Loss: 0.15\n","Epoch: 021/025 | Batch 038/110 | Loss: 0.10\n","Epoch: 021/025 | Batch 039/110 | Loss: 0.01\n","Epoch: 021/025 | Batch 040/110 | Loss: 0.04\n","Epoch: 021/025 | Batch 041/110 | Loss: 0.03\n","Epoch: 021/025 | Batch 042/110 | Loss: 0.14\n","Epoch: 021/025 | Batch 043/110 | Loss: 0.02\n","Epoch: 021/025 | Batch 044/110 | Loss: 0.04\n","Epoch: 021/025 | Batch 045/110 | Loss: 0.04\n","Epoch: 021/025 | Batch 046/110 | Loss: 0.14\n","Epoch: 021/025 | Batch 047/110 | Loss: 0.03\n","Epoch: 021/025 | Batch 048/110 | Loss: 0.01\n","Epoch: 021/025 | Batch 049/110 | Loss: 0.15\n","Epoch: 021/025 | Batch 050/110 | Loss: 0.03\n","Epoch: 021/025 | Batch 051/110 | Loss: 0.06\n","Epoch: 021/025 | Batch 052/110 | Loss: 0.05\n","Epoch: 021/025 | Batch 053/110 | Loss: 0.04\n","Epoch: 021/025 | Batch 054/110 | Loss: 0.03\n","Epoch: 021/025 | Batch 055/110 | Loss: 0.04\n","Epoch: 021/025 | Batch 056/110 | Loss: 0.02\n","Epoch: 021/025 | Batch 057/110 | Loss: 0.04\n","Epoch: 021/025 | Batch 058/110 | Loss: 0.01\n","Epoch: 021/025 | Batch 059/110 | Loss: 0.04\n","Epoch: 021/025 | Batch 060/110 | Loss: 0.17\n","Epoch: 021/025 | Batch 061/110 | Loss: 0.03\n","Epoch: 021/025 | Batch 062/110 | Loss: 0.02\n","Epoch: 021/025 | Batch 063/110 | Loss: 0.03\n","Epoch: 021/025 | Batch 064/110 | Loss: 0.14\n","Epoch: 021/025 | Batch 065/110 | Loss: 0.09\n","Epoch: 021/025 | Batch 066/110 | Loss: 0.01\n","Epoch: 021/025 | Batch 067/110 | Loss: 0.09\n","Epoch: 021/025 | Batch 068/110 | Loss: 0.06\n","Epoch: 021/025 | Batch 069/110 | Loss: 0.06\n","Epoch: 021/025 | Batch 070/110 | Loss: 0.01\n","Epoch: 021/025 | Batch 071/110 | Loss: 0.01\n","Epoch: 021/025 | Batch 072/110 | Loss: 0.02\n","Epoch: 021/025 | Batch 073/110 | Loss: 0.05\n","Epoch: 021/025 | Batch 074/110 | Loss: 0.03\n","Epoch: 021/025 | Batch 075/110 | Loss: 0.03\n","Epoch: 021/025 | Batch 076/110 | Loss: 0.12\n","Epoch: 021/025 | Batch 077/110 | Loss: 0.02\n","Epoch: 021/025 | Batch 078/110 | Loss: 0.03\n","Epoch: 021/025 | Batch 079/110 | Loss: 0.05\n","Epoch: 021/025 | Batch 080/110 | Loss: 0.02\n","Epoch: 021/025 | Batch 081/110 | Loss: 0.04\n","Epoch: 021/025 | Batch 082/110 | Loss: 0.14\n","Epoch: 021/025 | Batch 083/110 | Loss: 0.03\n","Epoch: 021/025 | Batch 084/110 | Loss: 0.05\n","Epoch: 021/025 | Batch 085/110 | Loss: 0.02\n","Epoch: 021/025 | Batch 086/110 | Loss: 0.02\n","Epoch: 021/025 | Batch 087/110 | Loss: 0.06\n","Epoch: 021/025 | Batch 088/110 | Loss: 0.20\n","Epoch: 021/025 | Batch 089/110 | Loss: 0.03\n","Epoch: 021/025 | Batch 090/110 | Loss: 0.14\n","Epoch: 021/025 | Batch 091/110 | Loss: 0.02\n","Epoch: 021/025 | Batch 092/110 | Loss: 0.10\n","Epoch: 021/025 | Batch 093/110 | Loss: 0.06\n","Epoch: 021/025 | Batch 094/110 | Loss: 0.02\n","Epoch: 021/025 | Batch 095/110 | Loss: 0.05\n","Epoch: 021/025 | Batch 096/110 | Loss: 0.02\n","Epoch: 021/025 | Batch 097/110 | Loss: 0.01\n","Epoch: 021/025 | Batch 098/110 | Loss: 0.03\n","Epoch: 021/025 | Batch 099/110 | Loss: 0.01\n","Epoch: 021/025 | Batch 100/110 | Loss: 0.03\n","Epoch: 021/025 | Batch 101/110 | Loss: 0.02\n","Epoch: 021/025 | Batch 102/110 | Loss: 0.01\n","Epoch: 021/025 | Batch 103/110 | Loss: 0.02\n","Epoch: 021/025 | Batch 104/110 | Loss: 0.02\n","Epoch: 021/025 | Batch 105/110 | Loss: 0.04\n","Epoch: 021/025 | Batch 106/110 | Loss: 0.23\n","Epoch: 021/025 | Batch 107/110 | Loss: 0.03\n","Epoch: 021/025 | Batch 108/110 | Loss: 0.03\n","Epoch: 021/025 | Batch 109/110 | Loss: 0.06\n","Epoch: 022/025 | Batch 000/110 | Loss: 0.09\n","Epoch: 022/025 | Batch 001/110 | Loss: 0.03\n","Epoch: 022/025 | Batch 002/110 | Loss: 0.05\n","Epoch: 022/025 | Batch 003/110 | Loss: 0.05\n","Epoch: 022/025 | Batch 004/110 | Loss: 0.03\n","Epoch: 022/025 | Batch 005/110 | Loss: 0.02\n","Epoch: 022/025 | Batch 006/110 | Loss: 0.01\n","Epoch: 022/025 | Batch 007/110 | Loss: 0.08\n","Epoch: 022/025 | Batch 008/110 | Loss: 0.03\n","Epoch: 022/025 | Batch 009/110 | Loss: 0.10\n","Epoch: 022/025 | Batch 010/110 | Loss: 0.03\n","Epoch: 022/025 | Batch 011/110 | Loss: 0.02\n","Epoch: 022/025 | Batch 012/110 | Loss: 0.05\n","Epoch: 022/025 | Batch 013/110 | Loss: 0.10\n","Epoch: 022/025 | Batch 014/110 | Loss: 0.03\n","Epoch: 022/025 | Batch 015/110 | Loss: 0.20\n","Epoch: 022/025 | Batch 016/110 | Loss: 0.02\n","Epoch: 022/025 | Batch 017/110 | Loss: 0.03\n","Epoch: 022/025 | Batch 018/110 | Loss: 0.04\n","Epoch: 022/025 | Batch 019/110 | Loss: 0.04\n","Epoch: 022/025 | Batch 020/110 | Loss: 0.05\n","Epoch: 022/025 | Batch 021/110 | Loss: 0.10\n","Epoch: 022/025 | Batch 022/110 | Loss: 0.05\n","Epoch: 022/025 | Batch 023/110 | Loss: 0.04\n","Epoch: 022/025 | Batch 024/110 | Loss: 0.01\n","Epoch: 022/025 | Batch 025/110 | Loss: 0.07\n","Epoch: 022/025 | Batch 026/110 | Loss: 0.10\n","Epoch: 022/025 | Batch 027/110 | Loss: 0.02\n","Epoch: 022/025 | Batch 028/110 | Loss: 0.11\n","Epoch: 022/025 | Batch 029/110 | Loss: 0.04\n","Epoch: 022/025 | Batch 030/110 | Loss: 0.02\n","Epoch: 022/025 | Batch 031/110 | Loss: 0.03\n","Epoch: 022/025 | Batch 032/110 | Loss: 0.03\n","Epoch: 022/025 | Batch 033/110 | Loss: 0.04\n","Epoch: 022/025 | Batch 034/110 | Loss: 0.05\n","Epoch: 022/025 | Batch 035/110 | Loss: 0.06\n","Epoch: 022/025 | Batch 036/110 | Loss: 0.04\n","Epoch: 022/025 | Batch 037/110 | Loss: 0.02\n","Epoch: 022/025 | Batch 038/110 | Loss: 0.14\n","Epoch: 022/025 | Batch 039/110 | Loss: 0.02\n","Epoch: 022/025 | Batch 040/110 | Loss: 0.06\n","Epoch: 022/025 | Batch 041/110 | Loss: 0.14\n","Epoch: 022/025 | Batch 042/110 | Loss: 0.05\n","Epoch: 022/025 | Batch 043/110 | Loss: 0.01\n","Epoch: 022/025 | Batch 044/110 | Loss: 0.05\n","Epoch: 022/025 | Batch 045/110 | Loss: 0.04\n","Epoch: 022/025 | Batch 046/110 | Loss: 0.09\n","Epoch: 022/025 | Batch 047/110 | Loss: 0.04\n","Epoch: 022/025 | Batch 048/110 | Loss: 0.02\n","Epoch: 022/025 | Batch 049/110 | Loss: 0.13\n","Epoch: 022/025 | Batch 050/110 | Loss: 0.19\n","Epoch: 022/025 | Batch 051/110 | Loss: 0.12\n","Epoch: 022/025 | Batch 052/110 | Loss: 0.11\n","Epoch: 022/025 | Batch 053/110 | Loss: 0.01\n","Epoch: 022/025 | Batch 054/110 | Loss: 0.17\n","Epoch: 022/025 | Batch 055/110 | Loss: 0.02\n","Epoch: 022/025 | Batch 056/110 | Loss: 0.07\n","Epoch: 022/025 | Batch 057/110 | Loss: 0.02\n","Epoch: 022/025 | Batch 058/110 | Loss: 0.04\n","Epoch: 022/025 | Batch 059/110 | Loss: 0.01\n","Epoch: 022/025 | Batch 060/110 | Loss: 0.18\n","Epoch: 022/025 | Batch 061/110 | Loss: 0.03\n","Epoch: 022/025 | Batch 062/110 | Loss: 0.04\n","Epoch: 022/025 | Batch 063/110 | Loss: 0.02\n","Epoch: 022/025 | Batch 064/110 | Loss: 0.03\n","Epoch: 022/025 | Batch 065/110 | Loss: 0.03\n","Epoch: 022/025 | Batch 066/110 | Loss: 0.03\n","Epoch: 022/025 | Batch 067/110 | Loss: 0.03\n","Epoch: 022/025 | Batch 068/110 | Loss: 0.04\n","Epoch: 022/025 | Batch 069/110 | Loss: 0.07\n","Epoch: 022/025 | Batch 070/110 | Loss: 0.08\n","Epoch: 022/025 | Batch 071/110 | Loss: 0.01\n","Epoch: 022/025 | Batch 072/110 | Loss: 0.04\n","Epoch: 022/025 | Batch 073/110 | Loss: 0.03\n","Epoch: 022/025 | Batch 074/110 | Loss: 0.04\n","Epoch: 022/025 | Batch 075/110 | Loss: 0.01\n","Epoch: 022/025 | Batch 076/110 | Loss: 0.02\n","Epoch: 022/025 | Batch 077/110 | Loss: 0.01\n","Epoch: 022/025 | Batch 078/110 | Loss: 0.13\n","Epoch: 022/025 | Batch 079/110 | Loss: 0.05\n","Epoch: 022/025 | Batch 080/110 | Loss: 0.04\n","Epoch: 022/025 | Batch 081/110 | Loss: 0.17\n","Epoch: 022/025 | Batch 082/110 | Loss: 0.15\n","Epoch: 022/025 | Batch 083/110 | Loss: 0.03\n","Epoch: 022/025 | Batch 084/110 | Loss: 0.07\n","Epoch: 022/025 | Batch 085/110 | Loss: 0.07\n","Epoch: 022/025 | Batch 086/110 | Loss: 0.01\n","Epoch: 022/025 | Batch 087/110 | Loss: 0.24\n","Epoch: 022/025 | Batch 088/110 | Loss: 0.04\n","Epoch: 022/025 | Batch 089/110 | Loss: 0.01\n","Epoch: 022/025 | Batch 090/110 | Loss: 0.05\n","Epoch: 022/025 | Batch 091/110 | Loss: 0.05\n","Epoch: 022/025 | Batch 092/110 | Loss: 0.04\n","Epoch: 022/025 | Batch 093/110 | Loss: 0.06\n","Epoch: 022/025 | Batch 094/110 | Loss: 0.01\n","Epoch: 022/025 | Batch 095/110 | Loss: 0.03\n","Epoch: 022/025 | Batch 096/110 | Loss: 0.01\n","Epoch: 022/025 | Batch 097/110 | Loss: 0.06\n","Epoch: 022/025 | Batch 098/110 | Loss: 0.06\n","Epoch: 022/025 | Batch 099/110 | Loss: 0.02\n","Epoch: 022/025 | Batch 100/110 | Loss: 0.02\n","Epoch: 022/025 | Batch 101/110 | Loss: 0.22\n","Epoch: 022/025 | Batch 102/110 | Loss: 0.02\n","Epoch: 022/025 | Batch 103/110 | Loss: 0.11\n","Epoch: 022/025 | Batch 104/110 | Loss: 0.14\n","Epoch: 022/025 | Batch 105/110 | Loss: 0.01\n","Epoch: 022/025 | Batch 106/110 | Loss: 0.01\n","Epoch: 022/025 | Batch 107/110 | Loss: 0.01\n","Epoch: 022/025 | Batch 108/110 | Loss: 0.02\n","Epoch: 022/025 | Batch 109/110 | Loss: 0.01\n","Epoch: 023/025 | Batch 000/110 | Loss: 0.16\n","Epoch: 023/025 | Batch 001/110 | Loss: 0.01\n","Epoch: 023/025 | Batch 002/110 | Loss: 0.17\n","Epoch: 023/025 | Batch 003/110 | Loss: 0.02\n","Epoch: 023/025 | Batch 004/110 | Loss: 0.01\n","Epoch: 023/025 | Batch 005/110 | Loss: 0.04\n","Epoch: 023/025 | Batch 006/110 | Loss: 0.14\n","Epoch: 023/025 | Batch 007/110 | Loss: 0.04\n","Epoch: 023/025 | Batch 008/110 | Loss: 0.03\n","Epoch: 023/025 | Batch 009/110 | Loss: 0.10\n","Epoch: 023/025 | Batch 010/110 | Loss: 0.02\n","Epoch: 023/025 | Batch 011/110 | Loss: 0.03\n","Epoch: 023/025 | Batch 012/110 | Loss: 0.03\n","Epoch: 023/025 | Batch 013/110 | Loss: 0.08\n","Epoch: 023/025 | Batch 014/110 | Loss: 0.01\n","Epoch: 023/025 | Batch 015/110 | Loss: 0.03\n","Epoch: 023/025 | Batch 016/110 | Loss: 0.03\n","Epoch: 023/025 | Batch 017/110 | Loss: 0.02\n","Epoch: 023/025 | Batch 018/110 | Loss: 0.11\n","Epoch: 023/025 | Batch 019/110 | Loss: 0.02\n","Epoch: 023/025 | Batch 020/110 | Loss: 0.05\n","Epoch: 023/025 | Batch 021/110 | Loss: 0.06\n","Epoch: 023/025 | Batch 022/110 | Loss: 0.03\n","Epoch: 023/025 | Batch 023/110 | Loss: 0.01\n","Epoch: 023/025 | Batch 024/110 | Loss: 0.02\n","Epoch: 023/025 | Batch 025/110 | Loss: 0.05\n","Epoch: 023/025 | Batch 026/110 | Loss: 0.01\n","Epoch: 023/025 | Batch 027/110 | Loss: 0.06\n","Epoch: 023/025 | Batch 028/110 | Loss: 0.21\n","Epoch: 023/025 | Batch 029/110 | Loss: 0.03\n","Epoch: 023/025 | Batch 030/110 | Loss: 0.16\n","Epoch: 023/025 | Batch 031/110 | Loss: 0.01\n","Epoch: 023/025 | Batch 032/110 | Loss: 0.03\n","Epoch: 023/025 | Batch 033/110 | Loss: 0.09\n","Epoch: 023/025 | Batch 034/110 | Loss: 0.08\n","Epoch: 023/025 | Batch 035/110 | Loss: 0.20\n","Epoch: 023/025 | Batch 036/110 | Loss: 0.13\n","Epoch: 023/025 | Batch 037/110 | Loss: 0.03\n","Epoch: 023/025 | Batch 038/110 | Loss: 0.06\n","Epoch: 023/025 | Batch 039/110 | Loss: 0.06\n","Epoch: 023/025 | Batch 040/110 | Loss: 0.01\n","Epoch: 023/025 | Batch 041/110 | Loss: 0.01\n","Epoch: 023/025 | Batch 042/110 | Loss: 0.02\n","Epoch: 023/025 | Batch 043/110 | Loss: 0.04\n","Epoch: 023/025 | Batch 044/110 | Loss: 0.06\n","Epoch: 023/025 | Batch 045/110 | Loss: 0.03\n","Epoch: 023/025 | Batch 046/110 | Loss: 0.11\n","Epoch: 023/025 | Batch 047/110 | Loss: 0.05\n","Epoch: 023/025 | Batch 048/110 | Loss: 0.03\n","Epoch: 023/025 | Batch 049/110 | Loss: 0.04\n","Epoch: 023/025 | Batch 050/110 | Loss: 0.03\n","Epoch: 023/025 | Batch 051/110 | Loss: 0.07\n","Epoch: 023/025 | Batch 052/110 | Loss: 0.05\n","Epoch: 023/025 | Batch 053/110 | Loss: 0.12\n","Epoch: 023/025 | Batch 054/110 | Loss: 0.05\n","Epoch: 023/025 | Batch 055/110 | Loss: 0.08\n","Epoch: 023/025 | Batch 056/110 | Loss: 0.05\n","Epoch: 023/025 | Batch 057/110 | Loss: 0.04\n","Epoch: 023/025 | Batch 058/110 | Loss: 0.04\n","Epoch: 023/025 | Batch 059/110 | Loss: 0.12\n","Epoch: 023/025 | Batch 060/110 | Loss: 0.17\n","Epoch: 023/025 | Batch 061/110 | Loss: 0.02\n","Epoch: 023/025 | Batch 062/110 | Loss: 0.08\n","Epoch: 023/025 | Batch 063/110 | Loss: 0.04\n","Epoch: 023/025 | Batch 064/110 | Loss: 0.01\n","Epoch: 023/025 | Batch 065/110 | Loss: 0.02\n","Epoch: 023/025 | Batch 066/110 | Loss: 0.02\n","Epoch: 023/025 | Batch 067/110 | Loss: 0.18\n","Epoch: 023/025 | Batch 068/110 | Loss: 0.03\n","Epoch: 023/025 | Batch 069/110 | Loss: 0.01\n","Epoch: 023/025 | Batch 070/110 | Loss: 0.04\n","Epoch: 023/025 | Batch 071/110 | Loss: 0.03\n","Epoch: 023/025 | Batch 072/110 | Loss: 0.03\n","Epoch: 023/025 | Batch 073/110 | Loss: 0.02\n","Epoch: 023/025 | Batch 074/110 | Loss: 0.02\n","Epoch: 023/025 | Batch 075/110 | Loss: 0.06\n","Epoch: 023/025 | Batch 076/110 | Loss: 0.13\n","Epoch: 023/025 | Batch 077/110 | Loss: 0.04\n","Epoch: 023/025 | Batch 078/110 | Loss: 0.05\n","Epoch: 023/025 | Batch 079/110 | Loss: 0.06\n","Epoch: 023/025 | Batch 080/110 | Loss: 0.02\n","Epoch: 023/025 | Batch 081/110 | Loss: 0.03\n","Epoch: 023/025 | Batch 082/110 | Loss: 0.01\n","Epoch: 023/025 | Batch 083/110 | Loss: 0.13\n","Epoch: 023/025 | Batch 084/110 | Loss: 0.01\n","Epoch: 023/025 | Batch 085/110 | Loss: 0.01\n","Epoch: 023/025 | Batch 086/110 | Loss: 0.04\n","Epoch: 023/025 | Batch 087/110 | Loss: 0.04\n","Epoch: 023/025 | Batch 088/110 | Loss: 0.02\n","Epoch: 023/025 | Batch 089/110 | Loss: 0.08\n","Epoch: 023/025 | Batch 090/110 | Loss: 0.09\n","Epoch: 023/025 | Batch 091/110 | Loss: 0.03\n","Epoch: 023/025 | Batch 092/110 | Loss: 0.01\n","Epoch: 023/025 | Batch 093/110 | Loss: 0.03\n","Epoch: 023/025 | Batch 094/110 | Loss: 0.03\n","Epoch: 023/025 | Batch 095/110 | Loss: 0.02\n","Epoch: 023/025 | Batch 096/110 | Loss: 0.02\n","Epoch: 023/025 | Batch 097/110 | Loss: 0.02\n","Epoch: 023/025 | Batch 098/110 | Loss: 0.13\n","Epoch: 023/025 | Batch 099/110 | Loss: 0.13\n","Epoch: 023/025 | Batch 100/110 | Loss: 0.07\n","Epoch: 023/025 | Batch 101/110 | Loss: 0.03\n","Epoch: 023/025 | Batch 102/110 | Loss: 0.13\n","Epoch: 023/025 | Batch 103/110 | Loss: 0.05\n","Epoch: 023/025 | Batch 104/110 | Loss: 0.03\n","Epoch: 023/025 | Batch 105/110 | Loss: 0.05\n","Epoch: 023/025 | Batch 106/110 | Loss: 0.05\n","Epoch: 023/025 | Batch 107/110 | Loss: 0.04\n","Epoch: 023/025 | Batch 108/110 | Loss: 0.14\n","Epoch: 023/025 | Batch 109/110 | Loss: 0.03\n","Epoch: 024/025 | Batch 000/110 | Loss: 0.04\n","Epoch: 024/025 | Batch 001/110 | Loss: 0.02\n","Epoch: 024/025 | Batch 002/110 | Loss: 0.06\n","Epoch: 024/025 | Batch 003/110 | Loss: 0.02\n","Epoch: 024/025 | Batch 004/110 | Loss: 0.04\n","Epoch: 024/025 | Batch 005/110 | Loss: 0.04\n","Epoch: 024/025 | Batch 006/110 | Loss: 0.01\n","Epoch: 024/025 | Batch 007/110 | Loss: 0.08\n","Epoch: 024/025 | Batch 008/110 | Loss: 0.12\n","Epoch: 024/025 | Batch 009/110 | Loss: 0.02\n","Epoch: 024/025 | Batch 010/110 | Loss: 0.04\n","Epoch: 024/025 | Batch 011/110 | Loss: 0.04\n","Epoch: 024/025 | Batch 012/110 | Loss: 0.02\n","Epoch: 024/025 | Batch 013/110 | Loss: 0.22\n","Epoch: 024/025 | Batch 014/110 | Loss: 0.19\n","Epoch: 024/025 | Batch 015/110 | Loss: 0.09\n","Epoch: 024/025 | Batch 016/110 | Loss: 0.01\n","Epoch: 024/025 | Batch 017/110 | Loss: 0.06\n","Epoch: 024/025 | Batch 018/110 | Loss: 0.17\n","Epoch: 024/025 | Batch 019/110 | Loss: 0.02\n","Epoch: 024/025 | Batch 020/110 | Loss: 0.12\n","Epoch: 024/025 | Batch 021/110 | Loss: 0.06\n","Epoch: 024/025 | Batch 022/110 | Loss: 0.06\n","Epoch: 024/025 | Batch 023/110 | Loss: 0.01\n","Epoch: 024/025 | Batch 024/110 | Loss: 0.09\n","Epoch: 024/025 | Batch 025/110 | Loss: 0.01\n","Epoch: 024/025 | Batch 026/110 | Loss: 0.04\n","Epoch: 024/025 | Batch 027/110 | Loss: 0.09\n","Epoch: 024/025 | Batch 028/110 | Loss: 0.04\n","Epoch: 024/025 | Batch 029/110 | Loss: 0.06\n","Epoch: 024/025 | Batch 030/110 | Loss: 0.02\n","Epoch: 024/025 | Batch 031/110 | Loss: 0.01\n","Epoch: 024/025 | Batch 032/110 | Loss: 0.04\n","Epoch: 024/025 | Batch 033/110 | Loss: 0.02\n","Epoch: 024/025 | Batch 034/110 | Loss: 0.02\n","Epoch: 024/025 | Batch 035/110 | Loss: 0.01\n","Epoch: 024/025 | Batch 036/110 | Loss: 0.14\n","Epoch: 024/025 | Batch 037/110 | Loss: 0.04\n","Epoch: 024/025 | Batch 038/110 | Loss: 0.02\n","Epoch: 024/025 | Batch 039/110 | Loss: 0.05\n","Epoch: 024/025 | Batch 040/110 | Loss: 0.04\n","Epoch: 024/025 | Batch 041/110 | Loss: 0.04\n","Epoch: 024/025 | Batch 042/110 | Loss: 0.06\n","Epoch: 024/025 | Batch 043/110 | Loss: 0.05\n","Epoch: 024/025 | Batch 044/110 | Loss: 0.05\n","Epoch: 024/025 | Batch 045/110 | Loss: 0.04\n","Epoch: 024/025 | Batch 046/110 | Loss: 0.13\n","Epoch: 024/025 | Batch 047/110 | Loss: 0.03\n","Epoch: 024/025 | Batch 048/110 | Loss: 0.05\n","Epoch: 024/025 | Batch 049/110 | Loss: 0.06\n","Epoch: 024/025 | Batch 050/110 | Loss: 0.05\n","Epoch: 024/025 | Batch 051/110 | Loss: 0.05\n","Epoch: 024/025 | Batch 052/110 | Loss: 0.05\n","Epoch: 024/025 | Batch 053/110 | Loss: 0.01\n","Epoch: 024/025 | Batch 054/110 | Loss: 0.04\n","Epoch: 024/025 | Batch 055/110 | Loss: 0.04\n","Epoch: 024/025 | Batch 056/110 | Loss: 0.05\n","Epoch: 024/025 | Batch 057/110 | Loss: 0.03\n","Epoch: 024/025 | Batch 058/110 | Loss: 0.02\n","Epoch: 024/025 | Batch 059/110 | Loss: 0.04\n","Epoch: 024/025 | Batch 060/110 | Loss: 0.02\n","Epoch: 024/025 | Batch 061/110 | Loss: 0.11\n","Epoch: 024/025 | Batch 062/110 | Loss: 0.03\n","Epoch: 024/025 | Batch 063/110 | Loss: 0.14\n","Epoch: 024/025 | Batch 064/110 | Loss: 0.15\n","Epoch: 024/025 | Batch 065/110 | Loss: 0.13\n","Epoch: 024/025 | Batch 066/110 | Loss: 0.03\n","Epoch: 024/025 | Batch 067/110 | Loss: 0.11\n","Epoch: 024/025 | Batch 068/110 | Loss: 0.05\n","Epoch: 024/025 | Batch 069/110 | Loss: 0.01\n","Epoch: 024/025 | Batch 070/110 | Loss: 0.07\n","Epoch: 024/025 | Batch 071/110 | Loss: 0.02\n","Epoch: 024/025 | Batch 072/110 | Loss: 0.03\n","Epoch: 024/025 | Batch 073/110 | Loss: 0.08\n","Epoch: 024/025 | Batch 074/110 | Loss: 0.14\n","Epoch: 024/025 | Batch 075/110 | Loss: 0.04\n","Epoch: 024/025 | Batch 076/110 | Loss: 0.00\n","Epoch: 024/025 | Batch 077/110 | Loss: 0.02\n","Epoch: 024/025 | Batch 078/110 | Loss: 0.12\n","Epoch: 024/025 | Batch 079/110 | Loss: 0.01\n","Epoch: 024/025 | Batch 080/110 | Loss: 0.04\n","Epoch: 024/025 | Batch 081/110 | Loss: 0.01\n","Epoch: 024/025 | Batch 082/110 | Loss: 0.12\n","Epoch: 024/025 | Batch 083/110 | Loss: 0.02\n","Epoch: 024/025 | Batch 084/110 | Loss: 0.02\n","Epoch: 024/025 | Batch 085/110 | Loss: 0.10\n","Epoch: 024/025 | Batch 086/110 | Loss: 0.03\n","Epoch: 024/025 | Batch 087/110 | Loss: 0.06\n","Epoch: 024/025 | Batch 088/110 | Loss: 0.15\n","Epoch: 024/025 | Batch 089/110 | Loss: 0.16\n","Epoch: 024/025 | Batch 090/110 | Loss: 0.14\n","Epoch: 024/025 | Batch 091/110 | Loss: 0.01\n","Epoch: 024/025 | Batch 092/110 | Loss: 0.07\n","Epoch: 024/025 | Batch 093/110 | Loss: 0.03\n","Epoch: 024/025 | Batch 094/110 | Loss: 0.03\n","Epoch: 024/025 | Batch 095/110 | Loss: 0.03\n","Epoch: 024/025 | Batch 096/110 | Loss: 0.04\n","Epoch: 024/025 | Batch 097/110 | Loss: 0.03\n","Epoch: 024/025 | Batch 098/110 | Loss: 0.01\n","Epoch: 024/025 | Batch 099/110 | Loss: 0.16\n","Epoch: 024/025 | Batch 100/110 | Loss: 0.01\n","Epoch: 024/025 | Batch 101/110 | Loss: 0.02\n","Epoch: 024/025 | Batch 102/110 | Loss: 0.04\n","Epoch: 024/025 | Batch 103/110 | Loss: 0.05\n","Epoch: 024/025 | Batch 104/110 | Loss: 0.03\n","Epoch: 024/025 | Batch 105/110 | Loss: 0.02\n","Epoch: 024/025 | Batch 106/110 | Loss: 0.01\n","Epoch: 024/025 | Batch 107/110 | Loss: 0.02\n","Epoch: 024/025 | Batch 108/110 | Loss: 0.03\n","Epoch: 024/025 | Batch 109/110 | Loss: 0.01\n","Epoch: 025/025 | Batch 000/110 | Loss: 0.02\n","Epoch: 025/025 | Batch 001/110 | Loss: 0.11\n","Epoch: 025/025 | Batch 002/110 | Loss: 0.22\n","Epoch: 025/025 | Batch 003/110 | Loss: 0.07\n","Epoch: 025/025 | Batch 004/110 | Loss: 0.07\n","Epoch: 025/025 | Batch 005/110 | Loss: 0.03\n","Epoch: 025/025 | Batch 006/110 | Loss: 0.13\n","Epoch: 025/025 | Batch 007/110 | Loss: 0.06\n","Epoch: 025/025 | Batch 008/110 | Loss: 0.04\n","Epoch: 025/025 | Batch 009/110 | Loss: 0.12\n","Epoch: 025/025 | Batch 010/110 | Loss: 0.10\n","Epoch: 025/025 | Batch 011/110 | Loss: 0.02\n","Epoch: 025/025 | Batch 012/110 | Loss: 0.04\n","Epoch: 025/025 | Batch 013/110 | Loss: 0.05\n","Epoch: 025/025 | Batch 014/110 | Loss: 0.01\n","Epoch: 025/025 | Batch 015/110 | Loss: 0.08\n","Epoch: 025/025 | Batch 016/110 | Loss: 0.02\n","Epoch: 025/025 | Batch 017/110 | Loss: 0.11\n","Epoch: 025/025 | Batch 018/110 | Loss: 0.14\n","Epoch: 025/025 | Batch 019/110 | Loss: 0.07\n","Epoch: 025/025 | Batch 020/110 | Loss: 0.03\n","Epoch: 025/025 | Batch 021/110 | Loss: 0.09\n","Epoch: 025/025 | Batch 022/110 | Loss: 0.02\n","Epoch: 025/025 | Batch 023/110 | Loss: 0.12\n","Epoch: 025/025 | Batch 024/110 | Loss: 0.06\n","Epoch: 025/025 | Batch 025/110 | Loss: 0.04\n","Epoch: 025/025 | Batch 026/110 | Loss: 0.09\n","Epoch: 025/025 | Batch 027/110 | Loss: 0.14\n","Epoch: 025/025 | Batch 028/110 | Loss: 0.06\n","Epoch: 025/025 | Batch 029/110 | Loss: 0.03\n","Epoch: 025/025 | Batch 030/110 | Loss: 0.02\n","Epoch: 025/025 | Batch 031/110 | Loss: 0.15\n","Epoch: 025/025 | Batch 032/110 | Loss: 0.01\n","Epoch: 025/025 | Batch 033/110 | Loss: 0.02\n","Epoch: 025/025 | Batch 034/110 | Loss: 0.02\n","Epoch: 025/025 | Batch 035/110 | Loss: 0.01\n","Epoch: 025/025 | Batch 036/110 | Loss: 0.13\n","Epoch: 025/025 | Batch 037/110 | Loss: 0.14\n","Epoch: 025/025 | Batch 038/110 | Loss: 0.01\n","Epoch: 025/025 | Batch 039/110 | Loss: 0.04\n","Epoch: 025/025 | Batch 040/110 | Loss: 0.02\n","Epoch: 025/025 | Batch 041/110 | Loss: 0.03\n","Epoch: 025/025 | Batch 042/110 | Loss: 0.03\n","Epoch: 025/025 | Batch 043/110 | Loss: 0.10\n","Epoch: 025/025 | Batch 044/110 | Loss: 0.04\n","Epoch: 025/025 | Batch 045/110 | Loss: 0.14\n","Epoch: 025/025 | Batch 046/110 | Loss: 0.01\n","Epoch: 025/025 | Batch 047/110 | Loss: 0.02\n","Epoch: 025/025 | Batch 048/110 | Loss: 0.01\n","Epoch: 025/025 | Batch 049/110 | Loss: 0.02\n","Epoch: 025/025 | Batch 050/110 | Loss: 0.02\n","Epoch: 025/025 | Batch 051/110 | Loss: 0.01\n","Epoch: 025/025 | Batch 052/110 | Loss: 0.01\n","Epoch: 025/025 | Batch 053/110 | Loss: 0.12\n","Epoch: 025/025 | Batch 054/110 | Loss: 0.09\n","Epoch: 025/025 | Batch 055/110 | Loss: 0.03\n","Epoch: 025/025 | Batch 056/110 | Loss: 0.07\n","Epoch: 025/025 | Batch 057/110 | Loss: 0.01\n","Epoch: 025/025 | Batch 058/110 | Loss: 0.03\n","Epoch: 025/025 | Batch 059/110 | Loss: 0.03\n","Epoch: 025/025 | Batch 060/110 | Loss: 0.07\n","Epoch: 025/025 | Batch 061/110 | Loss: 0.08\n","Epoch: 025/025 | Batch 062/110 | Loss: 0.08\n","Epoch: 025/025 | Batch 063/110 | Loss: 0.01\n","Epoch: 025/025 | Batch 064/110 | Loss: 0.05\n","Epoch: 025/025 | Batch 065/110 | Loss: 0.02\n","Epoch: 025/025 | Batch 066/110 | Loss: 0.04\n","Epoch: 025/025 | Batch 067/110 | Loss: 0.05\n","Epoch: 025/025 | Batch 068/110 | Loss: 0.04\n","Epoch: 025/025 | Batch 069/110 | Loss: 0.08\n","Epoch: 025/025 | Batch 070/110 | Loss: 0.04\n","Epoch: 025/025 | Batch 071/110 | Loss: 0.10\n","Epoch: 025/025 | Batch 072/110 | Loss: 0.02\n","Epoch: 025/025 | Batch 073/110 | Loss: 0.01\n","Epoch: 025/025 | Batch 074/110 | Loss: 0.03\n","Epoch: 025/025 | Batch 075/110 | Loss: 0.23\n","Epoch: 025/025 | Batch 076/110 | Loss: 0.02\n","Epoch: 025/025 | Batch 077/110 | Loss: 0.02\n","Epoch: 025/025 | Batch 078/110 | Loss: 0.03\n","Epoch: 025/025 | Batch 079/110 | Loss: 0.04\n","Epoch: 025/025 | Batch 080/110 | Loss: 0.03\n","Epoch: 025/025 | Batch 081/110 | Loss: 0.02\n","Epoch: 025/025 | Batch 082/110 | Loss: 0.01\n","Epoch: 025/025 | Batch 083/110 | Loss: 0.06\n","Epoch: 025/025 | Batch 084/110 | Loss: 0.04\n","Epoch: 025/025 | Batch 085/110 | Loss: 0.03\n","Epoch: 025/025 | Batch 086/110 | Loss: 0.02\n","Epoch: 025/025 | Batch 087/110 | Loss: 0.04\n","Epoch: 025/025 | Batch 088/110 | Loss: 0.04\n","Epoch: 025/025 | Batch 089/110 | Loss: 0.03\n","Epoch: 025/025 | Batch 090/110 | Loss: 0.15\n","Epoch: 025/025 | Batch 091/110 | Loss: 0.04\n","Epoch: 025/025 | Batch 092/110 | Loss: 0.01\n","Epoch: 025/025 | Batch 093/110 | Loss: 0.03\n","Epoch: 025/025 | Batch 094/110 | Loss: 0.02\n","Epoch: 025/025 | Batch 095/110 | Loss: 0.04\n","Epoch: 025/025 | Batch 096/110 | Loss: 0.03\n","Epoch: 025/025 | Batch 097/110 | Loss: 0.06\n","Epoch: 025/025 | Batch 098/110 | Loss: 0.01\n","Epoch: 025/025 | Batch 099/110 | Loss: 0.16\n","Epoch: 025/025 | Batch 100/110 | Loss: 0.03\n","Epoch: 025/025 | Batch 101/110 | Loss: 0.04\n","Epoch: 025/025 | Batch 102/110 | Loss: 0.09\n","Epoch: 025/025 | Batch 103/110 | Loss: 0.03\n","Epoch: 025/025 | Batch 104/110 | Loss: 0.02\n","Epoch: 025/025 | Batch 105/110 | Loss: 0.04\n","Epoch: 025/025 | Batch 106/110 | Loss: 0.05\n","Epoch: 025/025 | Batch 107/110 | Loss: 0.02\n","Epoch: 025/025 | Batch 108/110 | Loss: 0.05\n","Epoch: 025/025 | Batch 109/110 | Loss: 0.12\n"]}],"source":["for epoch in range(num_epochs):\n","    \n","    model = model.train()\n","    for batch_idx, (features, class_labels) in enumerate(train_loader):\n","\n","        probas = model(features)\n","        \n","        loss = F.binary_cross_entropy(probas, class_labels.view(probas.shape))\n","        \n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        \n","        ### LOGGING\n","        print(f'Epoch: {epoch+1:03d}/{num_epochs:03d}'\n","               f' | Batch {batch_idx:03d}/{len(train_loader):03d}'\n","               f' | Loss: {loss:.2f}')"]},{"cell_type":"markdown","id":"bb0d5821-7c8d-46b5-9e7d-02e72cac2acc","metadata":{"id":"bb0d5821-7c8d-46b5-9e7d-02e72cac2acc"},"source":["## Evaluate the results"]},{"cell_type":"code","execution_count":60,"id":"de622a50-f406-4a36-a216-1023951502fc","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"de622a50-f406-4a36-a216-1023951502fc","executionInfo":{"status":"ok","timestamp":1684963755454,"user_tz":-120,"elapsed":344,"user":{"displayName":"Artem Vachev","userId":"08691019652572164792"}},"outputId":"26d97089-6eda-477c-adf3-ce401dff6eed"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[2.8237e-01],\n","        [1.5443e-04],\n","        [9.6026e-01],\n","        [2.8236e-01],\n","        [4.1159e-03],\n","        [8.8513e-01],\n","        [1.2630e-03]], grad_fn=<SigmoidBackward0>)"]},"metadata":{},"execution_count":60}],"source":["probas"]},{"cell_type":"code","execution_count":61,"id":"530d69a2-e428-4c8c-b9e9-a4da5238b90e","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"530d69a2-e428-4c8c-b9e9-a4da5238b90e","executionInfo":{"status":"ok","timestamp":1684963760698,"user_tz":-120,"elapsed":322,"user":{"displayName":"Artem Vachev","userId":"08691019652572164792"}},"outputId":"b45092d6-3332-4577-f259-476c27dee12c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0],\n","        [0],\n","        [1],\n","        [0],\n","        [0],\n","        [1],\n","        [0]])"]},"metadata":{},"execution_count":61}],"source":["pred = torch.where(probas > 0.5, 1, 0)\n","pred"]},{"cell_type":"code","execution_count":62,"id":"126754f3-bfed-4c95-ab14-82be9c07bbea","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"126754f3-bfed-4c95-ab14-82be9c07bbea","executionInfo":{"status":"ok","timestamp":1684963763137,"user_tz":-120,"elapsed":394,"user":{"displayName":"Artem Vachev","userId":"08691019652572164792"}},"outputId":"fa63513b-c189-4942-b35c-fd26dbb3b6c7"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0],\n","        [0],\n","        [1],\n","        [0],\n","        [0],\n","        [1],\n","        [0]])"]},"metadata":{},"execution_count":62}],"source":["class_labels.view(pred.shape).to(pred.dtype)"]},{"cell_type":"code","execution_count":63,"id":"d910ddbb-798f-47ab-8aab-e2dba4aa4005","metadata":{"id":"d910ddbb-798f-47ab-8aab-e2dba4aa4005","executionInfo":{"status":"ok","timestamp":1684963786162,"user_tz":-120,"elapsed":381,"user":{"displayName":"Artem Vachev","userId":"08691019652572164792"}}},"outputs":[],"source":["def compute_accuracy(model, dataloader):\n","\n","    model = model.eval()\n","    \n","    correct = 0.0\n","    total_examples = 0\n","    \n","    for idx, (features, class_labels) in enumerate(dataloader):\n","        \n","        with torch.no_grad():\n","            probas = model(features)\n","        \n","        pred = torch.where(probas > 0.5, 1, 0)\n","        lab = class_labels.view(pred.shape).to(pred.dtype)\n","\n","        compare = lab == pred\n","        correct += torch.sum(compare)\n","        total_examples += len(compare)\n","\n","    return correct / total_examples"]},{"cell_type":"code","execution_count":64,"id":"27538c8d-61bc-47b0-8289-b6aab4aa16ed","metadata":{"id":"27538c8d-61bc-47b0-8289-b6aab4aa16ed","executionInfo":{"status":"ok","timestamp":1684963789882,"user_tz":-120,"elapsed":482,"user":{"displayName":"Artem Vachev","userId":"08691019652572164792"}}},"outputs":[],"source":["train_acc = compute_accuracy(model, train_loader)"]},{"cell_type":"code","execution_count":65,"id":"5a4ecf35-4745-43a8-8ea8-14f71cba5b59","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5a4ecf35-4745-43a8-8ea8-14f71cba5b59","executionInfo":{"status":"ok","timestamp":1684963789883,"user_tz":-120,"elapsed":8,"user":{"displayName":"Artem Vachev","userId":"08691019652572164792"}},"outputId":"376c0851-ae2d-4f53-b2dd-84e8c392cc41"},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 98.085693359375%\n"]}],"source":["print(f\"Accuracy: {train_acc*100}%\")"]},{"cell_type":"code","source":["val_acc = compute_accuracy(model, val_loader)\n","print(f\"Accuracy: {val_acc*100:.2f}%\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xXz-SZXRqwsd","executionInfo":{"status":"ok","timestamp":1684963795299,"user_tz":-120,"elapsed":304,"user":{"displayName":"Artem Vachev","userId":"08691019652572164792"}},"outputId":"7ca17b13-095e-4d01-9de8-78742ae89f19"},"id":"xXz-SZXRqwsd","execution_count":66,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 97.82%\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"_y6zEdaVq1wb"},"id":"_y6zEdaVq1wb","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"eWw41wTyq1qk"},"id":"eWw41wTyq1qk","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"provenance":[],"toc_visible":true}},"nbformat":4,"nbformat_minor":5}