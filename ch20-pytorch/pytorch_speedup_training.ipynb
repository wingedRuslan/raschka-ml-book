{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Techniques for Speeding Up Model Training"
      ],
      "metadata": {
        "id": "atU2OCIfrJbt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to scale | Summary\n",
        "\n",
        "When we train DL models there are a two aspects we want to optimize at the same time:\n",
        "\n",
        "- Data throughput / training time\n",
        "- Model performance\n",
        "\n",
        "We have seen that each method described in the doc changes the memory usage and throughput. In general we want to **maximize the throughput** (samples/second) to minimize the training cost. This is generally achieved by utilizing the GPU as much as possible and thus filling GPU memory to its limit.\n",
        "\n",
        "For example, as mentioned earlier, we only employ gradient accumulation when we want to use a batch size beyond the size of the GPU memory. If the desired batch size fits into memory then there is no reason to apply gradient accumulation which will only slow down training.\n",
        "\n",
        "The second objective is **model performance**. Just because we can does not mean we should use a large batch size. As part of hyperparameter tuning you should determine which batch size yields the best result and then optimize the throughput accordingly.\n",
        "\n",
        "**Hardware choice**. Sometimes, even when applying all the above tweaks the throughput on a given GPU might still not be good enough. One *easy solution* is to change the type of GPU. For example switching from let's say a K80 (which you typically get on Google Colab) to a fancier GPU such as the V100 or A100. Although they are more expensive they are usually more cost effective than cheaper GPUs due to their larger memory and faster architecture. For some applications, such as pretraining, this might still not be fast enough. In this case you want to scale your experiment to several GPUs."
      ],
      "metadata": {
        "id": "mdsKzC6qJwyx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Anatomy of Model's Memory\n",
        "\n",
        "The components on GPU memory are the following:\n",
        "\n",
        "- model weights\n",
        "    - 4 bytes * number of parameters for fp32 training\n",
        "    - 6 bytes * number of parameters for mixed precision training\n",
        "\n",
        "- optimizer states\n",
        "    - 8 bytes * number of parameters for normal AdamW (maintains 2 states)\n",
        "    - 2 bytes * number of parameters for 8-bit AdamW optimizers like bitsandbytes\n",
        "    - 4 bytes * number of parameters for optimizers like SGD (maintains only 1 state)\n",
        "\n",
        "- gradients\n",
        "    - 4 bytes * number of parameters for either fp32 or mixed precision training\n",
        "\n",
        "- forward activations saved for gradient computation\n",
        "    - size depends on many factors, the key ones being sequence length, hidden size and batch size.\n",
        "\n",
        "- temporary buffers\n",
        "    - all kinds of temporary variables which get released once the calculation is done\n",
        "\n",
        "- functionality-specific memory\n",
        "    - Then your software could have special memory needs. For example, when generating text using beam search, the software needs to maintain multiple copies of inputs and outputs."
      ],
      "metadata": {
        "id": "7gJakyiQO4zE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "qZnANYsrKN4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) Accelerated Model Training via Mixed-Precision Training\n",
        "\n",
        "`Mixed-precision training` involves using a combination of different numerical precisions (typically float32 and float16 or bfloat16) during model training to improve computational efficiency and speed.\n",
        "\n",
        "Traditional training methods tend to use 32-bit floating-point numbers (float32) to represent weights, biases, activations, and gradients for neural networks. However, this can be computationally expensive and memory-intensive, particularly for large models and data sets.\n",
        "\n",
        "To address this, `mixed-precision training` employs lower-precision formats, namely 16-bit floating-point numbers (float16) and Brain Floating Point (bfloat16), in parts of the training process where higher precision is not critical.\n",
        "\n",
        "`Mixed precision training`: switching between float32 and float16 representations during training.\n",
        "\n",
        "**Impact**: Fewer bits to represent a number --> requrie less memory & faster to compute numbers with lower bit representation.\n",
        "\n",
        "The balance between speed, memory usage, and precision makes mixed-precision training an increasingly popular approach for training large-scale machine learning models.\n",
        "\n",
        "Reference:\n",
        "https://lightning.ai/pages/community/tutorial/accelerating-large-language-models-with-mixed-precision-techniques/\n",
        "\n",
        "*Example*:\n",
        "DistilBert with float32 - 21 mins with acc 92%\n",
        "DistilBert with mixed precision training - 7 mins (x3 faster) with acc 92% (identical).\n",
        "\n",
        "The only thing to change - add precision argument = '16-mixed'."
      ],
      "metadata": {
        "id": "RMpWdJzlKOH2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eeFmwA0KDzS",
        "outputId": "84132293-5747-452b-89b4-24342f3b7f28"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "finfo(resolution=1e-06, min=-3.40282e+38, max=3.40282e+38, eps=1.19209e-07, smallest_normal=1.17549e-38, tiny=1.17549e-38, dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# What are min and max values for float32 and float16\n",
        "# use torch.finfo to find out.\n",
        "# +/- 3.4 * 10^38 and +/- 65,504\n",
        "\n",
        "torch.finfo()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.set_printoptions(precision=35)"
      ],
      "metadata": {
        "id": "PNT5RGleKN15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.tensor(1/3, dtype=torch.float32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRfc-UdIKN0F",
        "outputId": "d5538c63-c076-4c54-8176-5c7978f81558"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.33333334326744079589843750000000000)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.tensor(1/3, dtype=torch.float16)\n",
        "# lower precision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kf-I6oIdKNwP",
        "outputId": "760077f6-53ab-45f0-b039-8dd6d431e281"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.33325195312500000000000000000000000, dtype=torch.float16)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lower Precision**. Since in NN we have a lot of stochastic elements and a lot of randommess anyways, we do not necessary need a very high-precision.\n",
        "\n",
        "But might suffer from **Overflow and Underflow** - representing too large / too small numbers - unexpected results."
      ],
      "metadata": {
        "id": "82dnJ7fQVd_x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.tensor(10**6, dtype=torch.float32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bia7hrGKNtz",
        "outputId": "4c3f61e2-ce10-469a-ecb5-84fcf064ee17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1000000.)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.tensor(10**6, dtype=torch.float16)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0M18nQXKNrP",
        "outputId": "d0a5734c-77ba-40a4-f8b4-cc3237186528"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(inf, dtype=torch.float16)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Automatic `mixed precision training` - network automatically switching between 32-bit and 16-bit precision representations. --> Save computation time without suffering from any accuracy loss.\n",
        "\n",
        "Under the hood:\n",
        "1) Convert weights FP32 --> FP16 weights\n",
        "2) FP16 weights --> FP16 gradients\n",
        "3) FP16 gradients --> FP32 gradients\n",
        "4) Optimizer (update weights)\n",
        "5) Repeat"
      ],
      "metadata": {
        "id": "xAOZXKtsWYP3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quantization vs Mixed precision training\n",
        "\n",
        "**Mixed precision training** - switch between 32bit representations and 16bit representations. Speedup *model training*.\n",
        "\n",
        "**Quantization** - convert 32 bit floats -> 8 bit integer representations to speed up *inference of models*."
      ],
      "metadata": {
        "id": "G8BIR8kJXOE4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) Multi-GPU Training Strategies\n",
        "\n",
        "GPU vs CPU:\n",
        "- GPUs have a much higher number of cores compared to CPUs (e.g. 10240 vs 16) -> able to carry out computations (like matrix multiplications) faster.\n",
        "- Memory Bandwidth greater (912 Gb/s vs 45 Gb/s)\n",
        "- floating-point calculations: 742 GFLOPS vs 34 TFLOPS.\n",
        "\n",
        "GPUs are well-suited for parallel computations, making them ideal for training machine learning models.\n",
        "\n",
        "`GPU training on multiple GPUs` - offers benefits and strategies for large-scale machine learning tasks.\n",
        "\n",
        "---\n",
        "Broad categories of parallelism (`Multi-GPU training strategy`) for multi-GPUs training:\n",
        "\n",
        "- data parallelism\n",
        "    - increase the training throughput\n",
        "    - split batch to train model(s) on more data in parallel\n",
        "    - e.g. create 2 copies of model, put 1 on cude:0, put another on cuda:1, train both models in parallel, average the gradients when we update the models --> go through data faster.\n",
        "    - data parallelism involves distributing different subsets of the training data across multiple GPUs and then aggregating the gradients for the model update\n",
        "\n",
        "- model parallelism\n",
        "    - used in the context of limited GPU memory\n",
        "    - put different layers on different GPUs to work around GPU memory limitations\n",
        "    - model parallelism splits the model itself across GPUs, where each GPU computes a part of the forward and backward pass.\n",
        "\n",
        "- tensor parallelism\n",
        "    - related to model parallelism, but split layers horizontally instead of vertically.\n",
        "    - deals with limited GPU memory.\n",
        "    - the computation of each layer is split across multiple GPUs.\n",
        "    - more recent approach that splits the model's tensors across multiple GPUs to handle extremely large models that don't fit into a single GPU memory.\n",
        "\n",
        "- pipeline parallelism\n",
        "    - mix between data and model parallelism where model is split across different blocks.\n",
        "    - optimized so that GPUs can work better in parallel.\n",
        "\n",
        "- sequence parallelism\n",
        "    - Specifically developed for transformers models.\n",
        "    - Splits the input sequence into smaller chunks that can be distributed across GPUs to work around memory limitations.\n",
        "\n",
        "\n",
        "**Impact**. These techniques allow for the optimization of computational resources, speed up the training process, and enable the handling of larger models and datasets, thereby making multi-GPU training a key aspect of modern machine learning infrastructure.\n",
        "\n",
        "Default choice - **DistributedDataParallel** (multiple GPUs across one to many machines).\n",
        "\n",
        "`lightning` could handle all these with just a parameter twicking - huge benefit.\n",
        "\n",
        "Reference:\n",
        "https://huggingface.co/docs/transformers/perf_train_gpu_many"
      ],
      "metadata": {
        "id": "XwXprG2KZYY0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) Data Parallelism for multi-GPU Training\n",
        "\n",
        "Concept of `data parallelism` and its extension - `distributed data parallelism`, both essential strategies for accelerating machine learning training using multiple computational resources.\n",
        "\n",
        "**Data parallelism** is a technique where the training data is divided into multiple subsets, and each subset is processed independently across multiple GPUs or computing nodes. This allows for simultaneous computation, significantly reducing the training time. The computed gradients from each subset are then aggregated to update the model parameters. However, in the context of a single machine with multiple GPUs, this method can be limited by the inter-GPU communication speed and the machine's memory capacity.\n",
        "\n",
        "To overcome the limitations of regular data parallelism in PyTorch, we discussed **distributed data parallelism**, an extension of data parallelism that spans across multiple machines, each with one or more GPUs. In distributed data parallelism, the same model is replicated on each machine, and every machine processes a different subset of the training data. This not only facilitates handling larger datasets and models but also improves the training speed, taking advantage of the collective memory and computational power of multiple machines."
      ],
      "metadata": {
        "id": "TE_0OmKrjRcj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example:\n",
        "DistilBert with mixed precision training on 1 GPU - 7 mins with acc 92%.\n",
        "\n",
        "DistilBert with mixed precision training on 4 GPUs (ddp strategy) - 2.5 mins (x3 faster) with acc 92% (identical)."
      ],
      "metadata": {
        "id": "VY6F8R5T9wp9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Data parallelism** in a nutshell:\n",
        "Suppose minibatch is too large to fit into GPU memory. *Idea* - split it into smaller batches. Put each microbatch onto a different GPU. Make a copy of the neural network for each GPU.\n",
        "\n",
        "1) Transfer model, micro batches to GPUs\n",
        "2) On each GPU - forward pass to compute logits\n",
        "3) Transfer logits to one GPU\n",
        "4) On that GPU compute loss, gradients, weight update\n",
        "5) Update each model copy on each GPU.\n",
        "\n",
        "Compute forward/backward passes on each GPU in parallel. Each GPU operates independently on the copy of the model. Average gradients, update weights, update the model copies on different GPUs.\n",
        "\n",
        "**Note** - model operates on smaller batches (gradients are computed on smaller dataset that is averaged). --> Smaller minibatches require smaller learning rates.\n",
        "\n",
        "**Linear scaling rule**: when the minibatch size is multiplied by k, multiply the learning rate by k.\n",
        "\n",
        "The learning rate is typically scaled linearly with the batch size. That means if you halve the batch size, you would also halve the learning rate. This is known as \"linear scaling rule\"."
      ],
      "metadata": {
        "id": "2XEN6dZ_n0Ud"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Distributed data parallelism** - recommended strategy in practice.\n",
        "\n",
        "Each GPU gets the full batch size. Let `batch_size = 32` each GPU get a different mini-batch but all the mini-batches have the size of 32. In contrast to data parallelism where we split the mini batch into micro batches.\n",
        "\n",
        "(+) We do not have to adjust the learning rate wrt to the batch size.\n",
        "\n",
        "1) Transfer model, mini batches to GPUs\n",
        "2) On each GPU - forward pass to compute logits\n",
        "3) On each GPU - compute the backward, loss, gradients\n",
        "4) Communication between GPUs to update the weights.\n",
        "\n",
        "To be short, fewer transfers steps in Distributed data parallelism which is faster."
      ],
      "metadata": {
        "id": "mBJbVTmXpVmy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Parallel vs Distributed Data Parallel:\n",
        "\n",
        "Data Parallel\n",
        "- older implementation (may be deprecated)\n",
        "- a single process for multiple devices\n",
        "- 1 host GPU sends data and weights and collects gradients\n",
        "\n",
        "\n",
        "DistributedDataParallel (*recommended*)\n",
        "- more modern implementation\n",
        "- multiple processes for multiple devices\n",
        "- each device handles its own data, each device performs gradient calc & update\n",
        "- faster - less data transfers involved\n",
        "- does not split the batch! (the batch size is per device and per node)"
      ],
      "metadata": {
        "id": "--0OQJgnrhCY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q. Say you have trained a model with a batch size of 64. Now you use regular data parallelism with 4 GPUs. -> You should use a smaller learning rate (learning rate scaled linearly with the batch size).\n",
        "\n",
        "Q. Say you have trained a model with a batch size of 64. Now you use distributed data parallelism with 4 GPUs. -> You should use the same learning rate. (It is likely that the same learning rate still works well because distributed data parallelism does not split the minibatches further into microbatches)."
      ],
      "metadata": {
        "id": "j3zpG3cyuN92"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4) Compiling PyTorch Models\n",
        "\n",
        "`torch.compile` - a new feature in PyTorch 2.0 that allows you to speed up your PyTorch code by JIT-compiling it into optimized kernels.\n",
        "\n",
        "`torch.compile` is a fully additive feature, which means that it does not require you to make any changes to your existing code.\n",
        "\n",
        "As we've seen, to use **torch.compile**, we can simply use the `torch.compile()` function for an exisiting PyTorch model without making any further modifications. The `torch.compile()` will then compile the model into an optimized kernel the first time it is called. Subsequent calls to the function or module will be much faster, as they will be executed directly on the GPU."
      ],
      "metadata": {
        "id": "AgQtGTn7udAJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The 3 compilation steps:\n",
        "1) Graph acquisition\n",
        "- construct the computation graph based on model definition (forward pass)\n",
        "\n",
        "2) Graph lowering\n",
        "- represent graph using simpler operations\n",
        "- *primary purpose* - Convert a PyTorch model into a form that's more amenable to optimization and execution on certain hardware.  --> This transformation is essential for optimization and efficient execution on certain hardware platforms.\n",
        "\n",
        "3) Graph compilation\n",
        "- make it run on the hardware (cpu or gpu)"
      ],
      "metadata": {
        "id": "T080xvcCu2as"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.compile(model)\n",
        "# performance boost just by using one line"
      ],
      "metadata": {
        "id": "2o-_g1GMvkly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example:\n",
        "DistilBert with mixed precision training on 1 GPU - 7 mins with acc 92%.\n",
        "\n",
        "DistilBert with mixed precision training on 1 GPU + `torch.compile` - 3.7 mins (x2 faster) with acc 92% (identical)."
      ],
      "metadata": {
        "id": "c_FP3SQ4_YrK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Rule of thumb**: the longer you train and the bigger the model - the more benefits from compilation. The initial compulation introduces some overhead."
      ],
      "metadata": {
        "id": "lIHuKZr2wQbF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have seen that PyTorch `torch.compile` can speed up the model training.\n",
        "However, what are some of the **limitations** of torch.compile:\n",
        "\n",
        "- Not all PyTorch models can be compiled with torch.compile\n",
        "- The compilation process can be time-consuming (overhead)\n",
        "- The compiled models may not be as portable as the original PyTorch models."
      ],
      "metadata": {
        "id": "cZ3UHWRAw81o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5) Increasing Batch Sizes to Increase Throughput\n",
        "\n",
        "**Increasing batch sizes** to boost throughput in machine learning model training. A larger batch size can often result in faster model convergence or better end performance.\n",
        "\n",
        "**Impact**. The batch size, or the number of training samples processed before the model is updated, plays a critical role in the efficiency and effectiveness of model training. By increasing the batch size, we can process more data simultaneously, leading to higher computational efficiency and increased throughput, particularly on hardware like GPUs which excel in parallel processing.\n",
        "\n",
        "However, in practice, throughput is not always everything, and we have to make sure to strike a careful *balance* between batch size, learning rate, computational resources, and the potential impact on model performance, which are all crucial considerations in machine learning training pipelines.\n",
        "\n",
        "**Size**. Large batch size are better but only up to a certain point (larger than 1024 not useful). Too small batch sizes can be bad when using *BatchNorm* and hurt the predictive performance.\n",
        "\n",
        "\n",
        "Q. Why might using a very large batch size in deep learning training sometimes lead to less accurate results? -> It can lead to fewer updates per epoch, which can decrease the amount of implicit regularization.\n",
        "\n",
        "More smaller updates (smalled batch size) can lead to more small noise, which can sometimes be beneficial. However, often large batch sizes work just as well if we use a learning rate scheduler."
      ],
      "metadata": {
        "id": "Slwd6aEhxGPv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Warm-starting** large batch-size training with small batch sizes at the start can help with generalization performance."
      ],
      "metadata": {
        "id": "O19lOkCRy8y1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tricks to reduce the memory footprint and speed up training for large models\n",
        "\n",
        "https://huggingface.co/docs/transformers/v4.18.0/en/performance\n",
        "\n",
        "https://huggingface.co/docs/transformers/performance"
      ],
      "metadata": {
        "id": "4fUBmzfx2Bj1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6) Gradient Accumulation\n",
        "\n",
        "- a simple trick to effectively train larger batch size"
      ],
      "metadata": {
        "id": "u4lAsaoYEwNr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **idea** behind gradient accumulation is to instead of calculating the gradients for the whole batch at once to do it in smaller steps.\n",
        "\n",
        "**How?** - The way we do that is to calculate the gradients iteratively in smaller batches by doing a forward and backward pass through the model and accumulating the gradients in the process. When enough gradients are accumulated we run the model's optimization step.\n",
        "\n",
        "**Impact** - This way we can easily increase the overall batch size to numbers that would never fit into the GPU's memory. In turn, however, the added forward and backward passes can slow down the training a bit.\n",
        "\n",
        "**Outcomes** - The memory footprint was dramatically reduced at the cost of being only slightly slower than the vanilla run. In general you would want to max out the GPU usage as much as possible. If we wanted to train with a batch size of 64 we should not use `per_device_train_batch_size=1` and `gradient_accumulation_steps=64` but instead `per_device_train_batch_size=4` and `gradient_accumulation_steps=16` which has the same effective batch size while making better use of the available GPU resources.\n",
        "\n",
        "Since **gradient accumulation** essentially is identical to having a larger batch size, just as with the larger batch size here you are likely to see a 20-30% speedup due to the optimizer running less often. *Note*: It's important to remember that using gradient accumulation you may end up with a much larger effective batch size, so you may need to adjust the learning rate, its warm up and for very short datasets it'll impact the loss as the training will end up doing less steps than normal."
      ],
      "metadata": {
        "id": "toTx9zNr2FSq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7) Gradient Checkpointing (also known as \"activation checkpointing\")\n",
        "- trick to save a little bit more GPU memory"
      ],
      "metadata": {
        "id": "ufLjlCESFihf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In order to compute the gradients during the backward pass all activations from the forward pass are normally saved. This can create a *big memory overhead*. Alternatively, one could forget all activations during the forward pass and recompute them on demand during the backward pass. This would however add a significant computational overhead and slow down training.\n",
        "\n",
        "**Gradient checkpointing** strikes a compromise between the two approaches and saves strategically selected activations throughout the computational graph so only a fraction of the activations need to be re-computed for the gradients.\n",
        "\n",
        "https://medium.com/tensorflow/fitting-larger-networks-into-memory-583e3c758ff9\n",
        "\n",
        "**Outcome**: We can see that this saved some more memory but at the same time training became a bit slower. A general rule of thumb is that gradient checkpointing slows down training by about 20%."
      ],
      "metadata": {
        "id": "hw0GVFqs34S5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8) Mixed Precision Training\n",
        "\n",
        "The **idea** of mixed precision training is that no all variables need to be stored in full (32-bit) floating point precision.\n",
        "\n",
        "If we can reduce the precision the variales and their computations are faster. The main *advantage* comes from saving the activations in half (16-bit) precision. Although the gradients are also computed in half precision they are converted back to full precision for the optimization step so no memory is saved here. Since the model is present on the GPU in both 16-bit and 32-bit precision this can use more GPU memory (1.5x the original model is on the GPU), especially for small batch sizes.\n",
        "\n",
        "Since some computations are performed in full and some in half precision this approach is also called mixed precision training.\n",
        "\n",
        "**Outcome**. We can see that with these tweaks we use about half the GPU memory as at the beginning while also being slightly faster"
      ],
      "metadata": {
        "id": "ObCT9J-S4zsY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9) Low-memory Optimizer (Adam is fat in memory)\n",
        "\n",
        "The most common optimizer used to train transformer model is Adam or AdamW (Adam with weight decay). Adam achieves good convergence by storing the rolling average of the previous gradients which, however, *adds an additional memory* footprint of the order of the number of model parameters. One remedy to this is to use an alternative optimizer such as Adafactor.\n",
        "\n",
        "### Adafactor\n",
        "\n",
        "Instead of keeping the rolling average for each element in the weight matrices Adafactor only stores aggregated information (row- and column-wise sums of the rolling averages) which reduces the footprint considerably. One downside of Adafactor is that in some instances convergence can be slower than Adam's (also the convergence of Adafactor can be worse than Adam) so some experimentation is advised here.\n",
        "\n",
        "**Impact**. We can see that this saves a few more GB on the GPU.\n",
        "\n",
        "### 8-bit Adam\n",
        "\n",
        "Instead of aggregating optimizer states like Adafactor, 8-bit Adam keeps the full state and quantizes it. **Quantization** means that it stores the state with lower precision and dequantizes it only for the optimization. This is similar to the idea behind FP16 training where using variables with lower precision saves memory.\n",
        "\n",
        "**Impact**. We can see that we get a similar memory improvement as with Adafactor while keeping the full rolling average of the gradients.\n",
        "\n",
        "We need to install the 8-bit optimizer and then pass it as a custom optimizer to the Trainer (in huggingface library)."
      ],
      "metadata": {
        "id": "XpYi_mVb5dIS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10) Multi-GPU Training\n",
        "\n",
        "If your model fits on a single GPU scaling to many GPUs can be achieved fairly easily with **data parallelism**.\n",
        "\n",
        "The *idea* is very similar to gradient accumulation with the distinction that instead of running the forward and backward passes during the accumulation in sequence on a single machine they are performed in parallel on multiple machines. So each GPU gets a small batch, runs the forward and backward passes and then the gradients from all machines are aggregated and the model is optimized."
      ],
      "metadata": {
        "id": "BCVduH6INurI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the model does not fit on a single GPU with all the mentioned tricks there are still more methods we can apply although life starts to get a bit more complicated. This usually involves some form of **pipeline or tensor parallelism** where the model itself is distributed across several GPUs.\n",
        "\n",
        "One can also make use of DeepSpeed which implements some of these parallelism strategies along with some more optimization to reduce the memory footprint such as partitioning the optimizer states.\n",
        "\n",
        "https://huggingface.co/docs/transformers/v4.18.0/en/parallelism"
      ],
      "metadata": {
        "id": "iASR6gKHN_7l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11) DP vs DDP"
      ],
      "metadata": {
        "id": "p0p7xnidQng-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DistributedDataParallel (DDP) is typically faster than DataParallel (DP), but it is not always the case:\n",
        "\n",
        "- while DP is python threads-based, DDP is multiprocess-based - and as such it has no python threads limitations, such as GIL\n",
        "- on the other hand a slow inter-connectivity between the GPU cards could lead to an actual slower outcome with DDP\n",
        "\n",
        "DDP:\n",
        "- At the start time the main process replicates the model once from gpu 0 to the rest of gpus\n",
        "- Then for each batch:\n",
        "    - each gpu consumes each own mini-batch of data directly\n",
        "    - during backward, once the local gradients are ready, they are then averaged across all processes\n",
        "\n",
        "DP:\n",
        "For each batch:\n",
        "- gpu 0 reads the batch of data and then sends a mini-batch to each gpu\n",
        "- replicates the up-to-date model from gpu 0 to each gpu\n",
        "- runs forward and sends output from each gpu to gpu 0, computes loss\n",
        "- scatters loss from gpu 0 to all gpus, runs backward\n",
        "- sends gradients from each gpu to gpu 0 and averages those\n",
        "\n",
        "**Summary**: The only communication DDP performs per batch is sending gradients, whereas DP does 5 different data exchanges per batch\n",
        "- Main differences in the inter-GPU communication overhead between the two modes.\n",
        "- Under DP gpu 0 performs a lot more work than the rest of the gpus, thus resulting in under-utilization of gpus.\n",
        "- You can use DDP across multiple machines, but this is not the case with DP.\n",
        "\n"
      ],
      "metadata": {
        "id": "9RjZcMveQp1v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 12) DataLoader\n",
        "\n",
        "One of the important requirements to reach great training speed is the ability to feed the GPU at the maximum speed it can handle. By default everything happens in the main process and it might not be able to read the data from disk fast enough, and thus create a **bottleneck**, leading to GPU under-utilization.\n",
        "\n",
        "- DataLoader(pin_memory=True, ...) which ensures that the data gets preloaded into the pinned memory on CPU and typically leads to much faster transfers from CPU to GPU memory.\n",
        "- DataLoader(num_workers=4, ...) - spawn several workers to pre-load data faster."
      ],
      "metadata": {
        "id": "FYF2oo50RlQk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VB0v0nrGX1Zl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ijqJotytTAIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H2o_An0NTACt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary Table\n",
        "\n",
        "Each method can improve speed or memory usage which is summarized in the table below:\n",
        "\n",
        "| Method | Speed | Memory |\n",
        "| -------- | -------- | -------- |\n",
        "| Gradient accumulation    | No    | Yes    |\n",
        "| Gradient checkpointing    | No    | Yes    |\n",
        "| Mixed precision training    | Yes    | (No)    |\n",
        "| Batch size    | Yes    | Yes    |\n",
        "| Optimizer choice    | Yes    | Yes    |\n",
        "| DataLoader    | Yes    | No    |\n",
        "| DeepSpeed Zero    | No    | Yes    |\n",
        "| Compile    | Yes    | No    |"
      ],
      "metadata": {
        "id": "qhMO2iA1TAku"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DeepSpeed decision tree:\n",
        "- Model fits onto a single GPU and you have enough space to fit a small batch size - you don't need to use Deepspeed as it'll only slow things down in this use case.\n",
        "- Model doesn't fit onto a single GPU or you can't fit a small batch - use DeepSpeed"
      ],
      "metadata": {
        "id": "hlCtm0s3T95q"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nNoCe2xRTCOA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}